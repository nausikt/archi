{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Archi Archi (AI Augmented Research Chat Intelligence) is a retrieval-augmented generation (RAG) framework designed to be a low-barrier, open-source, private, and customizable AI solution for research and educational support. Archi makes it easy to deploy AI assistants with a suite of tools that connect to communication platforms such as Piazza, Slack, Discourse, Mattermost, and email, and knowledge bases such as web links, files, JIRA tickets, and documentation. It is modular and extensible, allowing users to add connectors and customise pipeline behaviour for a wide range of tasks\u2014from answering simple questions to delivering detailed explanations. About Archi is an end-to-end framework developed by Prof. Paus (MIT Physics), Prof. Kraska (MIT EECS), and their students. It has already been successfully deployed as a user chatbot and technical assistant at SubMIT (the MIT Physics Department's private cluster) and as an educational assistant for several MIT courses, including 8.01 and 8.511. What sets Archi apart is that it is fully open source, configurable across foundational models and LLM libraries, and designed for private deployment. Under the hood, Archi is a highly configurable RAG system tailored for educational and scientific support. Given its success, the scope now spans additional MIT classes, CERN, Harvard, and internal deployments such as CSAIL's support staff. Educational Support Archi assists TAs, lecturers, and support staff\u2014or students directly\u2014by preparing answers based on curated class resources. In MIT course deployments, Archi leverages Piazza posts, documentation, and other class-specific materials. The Piazza integration can draft answers for staff to review or send, while the system continuously learns from revisions and new posts, improving over time. Research Resource Support Archi also serves technical support teams and end users. At SubMIT, it functions both as a user-facing chatbot and as a ticket assistant. Integration with Redmine enables Archi to prepare draft responses to support tickets that staff can review before sending. In both roles, Archi accesses the corpus of tickets and documentation, citing relevant sources in its answers.","title":"Archi"},{"location":"#archi","text":"Archi (AI Augmented Research Chat Intelligence) is a retrieval-augmented generation (RAG) framework designed to be a low-barrier, open-source, private, and customizable AI solution for research and educational support. Archi makes it easy to deploy AI assistants with a suite of tools that connect to communication platforms such as Piazza, Slack, Discourse, Mattermost, and email, and knowledge bases such as web links, files, JIRA tickets, and documentation. It is modular and extensible, allowing users to add connectors and customise pipeline behaviour for a wide range of tasks\u2014from answering simple questions to delivering detailed explanations.","title":"Archi"},{"location":"#about","text":"Archi is an end-to-end framework developed by Prof. Paus (MIT Physics), Prof. Kraska (MIT EECS), and their students. It has already been successfully deployed as a user chatbot and technical assistant at SubMIT (the MIT Physics Department's private cluster) and as an educational assistant for several MIT courses, including 8.01 and 8.511. What sets Archi apart is that it is fully open source, configurable across foundational models and LLM libraries, and designed for private deployment. Under the hood, Archi is a highly configurable RAG system tailored for educational and scientific support. Given its success, the scope now spans additional MIT classes, CERN, Harvard, and internal deployments such as CSAIL's support staff.","title":"About"},{"location":"#educational-support","text":"Archi assists TAs, lecturers, and support staff\u2014or students directly\u2014by preparing answers based on curated class resources. In MIT course deployments, Archi leverages Piazza posts, documentation, and other class-specific materials. The Piazza integration can draft answers for staff to review or send, while the system continuously learns from revisions and new posts, improving over time.","title":"Educational Support"},{"location":"#research-resource-support","text":"Archi also serves technical support teams and end users. At SubMIT, it functions both as a user-facing chatbot and as a ticket assistant. Integration with Redmine enables Archi to prepare draft responses to support tickets that staff can review before sending. In both roles, Archi accesses the corpus of tickets and documentation, citing relevant sources in its answers.","title":"Research Resource Support"},{"location":"advanced_setup_deploy/","text":"Advanced Setup & Deployment Topics related to advanced setup and deployment of Archi. Configuring Podman To ensure your Podman containers stay running for extended periods, you need to enable lingering. To do this, run: loginctl enable-linger To check or confirm the lingering status, run: loginctl user-status | grep -m1 Linger See the Red Hat documentation for additional context. Running LLMs locally on your GPUs There are a few additional system requirements for this to work: Make sure you have NVIDIA drivers installed. (Optional) For the containers where Archi will run to access the GPUs, install the NVIDIA container toolkit . Configure the container runtime to access the GPUs. For Podman Run the following command: sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml Then list the devices: nvidia-ctk cdi list You should see output similar to: INFO[0000] Found 9 CDI devices ... nvidia.com/gpu=0 nvidia.com/gpu=1 ... nvidia.com/gpu=all ... These listed \"CDI devices\" will be referenced to run Archi on the GPUs, so make sure they are present. To learn more, consult the [Podman GPU documentation](https://podman-desktop.io/docs/podman/gpu). For Docker Run the following command: sudo nvidia-ctk runtime configure --runtime=docker The remaining steps mirror the Podman flow. NOTE: this has not yet been fully tested with Docker. Refer to the [NVIDIA toolkit documentation](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#configuration) for details. Once these requirements are met, the archi create [...] --gpu-ids <gpus> option will deploy Archi across your GPUs. Helpful Notes for Production Deployments You may wish to use the CLI in order to stage production deployments. This section covers some useful notes to keep in mind. Running multiple deployments on the same machine The CLI allows multiple deployments to run on the same daemon in the case of Docker (Podman has no daemon). The container networks between all the deployments are separate, so there is very little risk of them accidentally communicating with one another. However, you need to be careful with the external ports. Suppose you're running two deployments and both of them are running the chat on external port 8000. There is no way to view both deployments at the same time from the same port, so instead you should forward the deployments to other external ports. Generally, this can be done in the configuration: services: chat_app: external_port: 7862 # default is 7861 uploader_app: external_port: 5004 # default is 5003 grafana: external_port: 3001 # default is 3000 postgres: port: 5432 # default is 5432 Persisting data between deployments Volumes persist between deployments, so if you deploy an instance and upload additional documents, you do not need to redo this every time you deploy. If you are editing any data, explicitly remove this information from the volume, or remove the volume itself with: docker/podman volume rm <volume-name> To see what volumes are currently present, run: docker/podman volume ls HTTPS Configuration for Production For production deployments, especially when using BYOK (Bring Your Own Key), HTTPS is strongly recommended to protect API keys in transit. Using a Reverse Proxy The recommended approach is to terminate TLS at a reverse proxy (nginx, Caddy, Traefik): Example nginx configuration: server { listen 443 ssl; server_name your-domain.com; ssl_certificate /path/to/cert.pem; ssl_certificate_key /path/to/key.pem; location / { proxy_pass http://localhost:7861; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; # SSE streaming support proxy_buffering off; proxy_cache off; } } Example Caddy configuration (automatic HTTPS): your-domain.com { reverse_proxy localhost:7861 } Session Cookie Security When running behind HTTPS, enable secure cookies by setting the environment variable: FLASK_SESSION_COOKIE_SECURE=true Or configure in your deployment's environment file: FLASK_SESSION_COOKIE_SECURE=true This ensures session cookies (which may contain API keys) are only sent over encrypted connections. Security Checklist [ ] TLS termination at reverse proxy or load balancer [ ] FLASK_SESSION_COOKIE_SECURE=true in production [ ] Strong FLASK_UPLOADER_APP_SECRET_KEY configured (not auto-generated) [ ] Firewall rules limiting direct access to internal ports [ ] Regular certificate renewal (use Let's Encrypt/certbot)","title":"Advanced Setup and Deployment"},{"location":"advanced_setup_deploy/#advanced-setup-deployment","text":"Topics related to advanced setup and deployment of Archi.","title":"Advanced Setup &amp; Deployment"},{"location":"advanced_setup_deploy/#configuring-podman","text":"To ensure your Podman containers stay running for extended periods, you need to enable lingering. To do this, run: loginctl enable-linger To check or confirm the lingering status, run: loginctl user-status | grep -m1 Linger See the Red Hat documentation for additional context.","title":"Configuring Podman"},{"location":"advanced_setup_deploy/#running-llms-locally-on-your-gpus","text":"There are a few additional system requirements for this to work: Make sure you have NVIDIA drivers installed. (Optional) For the containers where Archi will run to access the GPUs, install the NVIDIA container toolkit . Configure the container runtime to access the GPUs. For Podman Run the following command: sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml Then list the devices: nvidia-ctk cdi list You should see output similar to: INFO[0000] Found 9 CDI devices ... nvidia.com/gpu=0 nvidia.com/gpu=1 ... nvidia.com/gpu=all ... These listed \"CDI devices\" will be referenced to run Archi on the GPUs, so make sure they are present. To learn more, consult the [Podman GPU documentation](https://podman-desktop.io/docs/podman/gpu). For Docker Run the following command: sudo nvidia-ctk runtime configure --runtime=docker The remaining steps mirror the Podman flow. NOTE: this has not yet been fully tested with Docker. Refer to the [NVIDIA toolkit documentation](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#configuration) for details. Once these requirements are met, the archi create [...] --gpu-ids <gpus> option will deploy Archi across your GPUs.","title":"Running LLMs locally on your GPUs"},{"location":"advanced_setup_deploy/#helpful-notes-for-production-deployments","text":"You may wish to use the CLI in order to stage production deployments. This section covers some useful notes to keep in mind.","title":"Helpful Notes for Production Deployments"},{"location":"advanced_setup_deploy/#running-multiple-deployments-on-the-same-machine","text":"The CLI allows multiple deployments to run on the same daemon in the case of Docker (Podman has no daemon). The container networks between all the deployments are separate, so there is very little risk of them accidentally communicating with one another. However, you need to be careful with the external ports. Suppose you're running two deployments and both of them are running the chat on external port 8000. There is no way to view both deployments at the same time from the same port, so instead you should forward the deployments to other external ports. Generally, this can be done in the configuration: services: chat_app: external_port: 7862 # default is 7861 uploader_app: external_port: 5004 # default is 5003 grafana: external_port: 3001 # default is 3000 postgres: port: 5432 # default is 5432","title":"Running multiple deployments on the same machine"},{"location":"advanced_setup_deploy/#persisting-data-between-deployments","text":"Volumes persist between deployments, so if you deploy an instance and upload additional documents, you do not need to redo this every time you deploy. If you are editing any data, explicitly remove this information from the volume, or remove the volume itself with: docker/podman volume rm <volume-name> To see what volumes are currently present, run: docker/podman volume ls","title":"Persisting data between deployments"},{"location":"advanced_setup_deploy/#https-configuration-for-production","text":"For production deployments, especially when using BYOK (Bring Your Own Key), HTTPS is strongly recommended to protect API keys in transit.","title":"HTTPS Configuration for Production"},{"location":"advanced_setup_deploy/#using-a-reverse-proxy","text":"The recommended approach is to terminate TLS at a reverse proxy (nginx, Caddy, Traefik): Example nginx configuration: server { listen 443 ssl; server_name your-domain.com; ssl_certificate /path/to/cert.pem; ssl_certificate_key /path/to/key.pem; location / { proxy_pass http://localhost:7861; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; # SSE streaming support proxy_buffering off; proxy_cache off; } } Example Caddy configuration (automatic HTTPS): your-domain.com { reverse_proxy localhost:7861 }","title":"Using a Reverse Proxy"},{"location":"advanced_setup_deploy/#session-cookie-security","text":"When running behind HTTPS, enable secure cookies by setting the environment variable: FLASK_SESSION_COOKIE_SECURE=true Or configure in your deployment's environment file: FLASK_SESSION_COOKIE_SECURE=true This ensures session cookies (which may contain API keys) are only sent over encrypted connections.","title":"Session Cookie Security"},{"location":"advanced_setup_deploy/#security-checklist","text":"[ ] TLS termination at reverse proxy or load balancer [ ] FLASK_SESSION_COOKIE_SECURE=true in production [ ] Strong FLASK_UPLOADER_APP_SECRET_KEY configured (not auto-generated) [ ] Firewall rules limiting direct access to internal ports [ ] Regular certificate renewal (use Let's Encrypt/certbot)","title":"Security Checklist"},{"location":"api_reference/","text":"API Reference CLI The Archi CLI provides commands to create, manage, and delete Archi deployments and services. Commands 1. create Create a new Archi deployment. Usage: archi create --name <deployment_name> --config <config.yaml> --env-file <secrets.env> [OPTIONS] Options: --name, -n (str, required): Name of the deployment. --config, -c (str): Path to a YAML configuration file (repeat the flag to supply multiple files). --config-dir, -cd (str): Directory containing configuration files. --env-file, -e (str, required): Path to the secrets .env file. --services, -s (comma-separated, required): List of services to enable (e.g., chatbot,uploader ). --sources, -src (comma-separated): Additional data sources to enable (e.g., git,jira ). The links source is always available. --podman, -p : Use Podman instead of Docker. --gpu-ids : GPU configuration ( all or comma-separated IDs). --tag, -t (str): Image tag for built containers (default: 2000 ). --hostmode : Use host network mode. --verbosity, -v (int): Logging verbosity (0-4, default: 3). --force, -f : Overwrite existing deployment if it exists. --dry, --dry-run : Validate and show what would be created, but do not deploy. 2. delete Delete an existing Archi deployment. Usage: archi delete --name <deployment_name> [OPTIONS] Options: --name, -n (str): Name of the deployment to delete. --rmi : Remove container images. --rmv : Remove volumes. --keep-files : Keep deployment files (do not remove directory). --list : List all available deployments. 3. restart Restart a specific service in an existing deployment without restarting the entire stack. Usage: archi restart --name <deployment_name> --service <service_name> [OPTIONS] Options: --name, -n (str, required): Name of the existing deployment. --service, -s (str): Service to restart (default: chatbot ). --config, -c (str): Path to updated YAML configuration file(s) (can be specified multiple times). --config-dir, -cd (str): Path to directory containing configuration files. --env-file, -e (str): Path to .env file with secrets. --no-build : Restart without rebuilding the container image. --with-deps : Also restart dependent services (by default, only the specified service is restarted). --podman, -p : Use Podman instead of Docker. --verbosity, -v (int): Logging verbosity level (0-4, default: 3). Notes: Configuration changes : Restarting with --no-build will reflect changes to configuration files. If you've modified code, you must rebuild the image (omit the --no-build flag). Updating configuration : If you provide --config or --config-dir , the command will update the deployment's configuration before restarting the service. Finding services : Use archi list-deployments to see existing deployments. If you specify an invalid service name, the restart command will display the available services for that deployment. Examples: Quick config update without rebuilding: archi restart -n mybot --service chatbot --no-build Test new agent code (requires rebuild): archi restart -n mybot --service chatbot -c updated_config.yaml Restart with updated secrets: archi restart -n mybot --service chatbot -e new_secrets.env --no-build Restart data_manager to re-scrape sources: archi restart -n mybot --service data_manager 4. list-services List all available Archi services and data sources. Usage: archi list-services 5. list-deployments List all existing Archi deployments. Usage: archi list-deployments 6. evaluate Launch the benchmarking runtime to evaluate one or more configurations against a set of questions/answers. Usage: archi evaluate --name <run_name> --env-file <secrets.env> --config <file.yaml> [OPTIONS] Use --config-dir if you want to point to a directory of configs instead. Options: Supports the same flags as create ( --sources , --podman , --gpu-ids , --tag , --hostmode , --verbosity , --force ). Reads configuration from one or more YAML files that should define the services.benchmarking section. Examples Create a deployment: archi create --name mybot --config my.yaml --env-file secrets.env --services chatbot,uploader Delete a deployment and remove images/volumes: archi delete --name mybot --rmi --rmv Restart a service without rebuilding: archi restart --name mybot --service chatbot --no-build List all deployments: archi list-deployments List all services: archi list-services Configuration YAML API Reference The Archi configuration YAML file defines the deployment, services, data sources, pipelines, models, and interface settings for your Archi instance. Top-Level Fields name Type: string Description: Name of the deployment. global DATA_PATH: path for persisted data (defaults to /root/data/ ). ACCOUNTS_PATH: path for uploader/grader account data. ACCEPTED_FILES: list of extensions allowed for manual uploads. LOGGING.input_output_filename: log file that stores pipeline inputs/outputs. verbosity: default logging level for services (0-4). services Holds configuration for every containerised service. Common keys include: port / external_port: internal versus host port mapping for web apps. host / hostname: network binding and public hostname for frontends. volume/paths: template or static asset paths expected by the service. Key services: chat_app: Chat interface options ( trained_on , ports, UI toggles). uploader_app: Document uploader settings ( verify_urls , ports). grader_app: Grader-specific knobs ( num_problems , rubric paths). grafana: Port configuration for the monitoring dashboard. postgres: Database credentials ( user , database , port , host ). Also used for vector storage via pgvector. piazza , mattermost , redmine_mailbox , benchmarking , ...: Service-specific options (see user guide sections above). data_manager Controls ingestion sources and vector store behaviour. sources.links.input_lists: .list files with seed URLs. sources.links.scraper: Behaviour toggles for HTTP scraping (resetting data, URL verification, warning output). sources.links.selenium_scraper: Selenium configuration used for SSO scraping and optional link scraping. sources. .visible: Mark whether documents harvested from a source should appear in chat citations and other user-facing listings ( true by default). sources.git.enabled / sources.sso.enabled / sources.jira.enabled / sources.redmine.enabled: Toggle additional collectors when paired with --sources . sources.jira.cutoff_date: ISO-8601 date; JIRA tickets created before this are ignored. embedding_name: Embedding backend ( OpenAIEmbeddings , HuggingFaceEmbeddings , ...). embedding_class_map: Backend specific parameters (model name, device, similarity threshold). chunk_size / chunk_overlap: Text splitter parameters. reset_collection: Whether to wipe the collection before re-populating. num_documents_to_retrieve: Top-k documents returned at query time. distance_metric / use_hybrid_search / bm25_weight / semantic_weight: Retrieval tuning knobs (BM25 k1 / b are fixed when the PostgreSQL index is created). utils.anonymizer (legacy) / data_manager.utils.anonymizer : Redaction settings applied when ticket collectors anonymise content. Source configuration is persisted to PostgreSQL static_config.sources_config at deployment time and used for runtime ingestion. archi Defines pipelines and model routing. pipelines: List of pipeline names to load (e.g., QAPipeline ). pipeline_map: Per-pipeline configuration of prompts, models, token limits, and ReAct recursion limits ( recursion_limit , default 100 ). model_class_map: Definitions for each model family (base model names, provider-specific kwargs). chain_update_time: Polling interval for hot-reloading chains. utils Utility configuration for supporting components (mostly legacy fallbacks): git: Legacy toggle for Git scraping. jira / redmine: Compatibility settings for ticket integrations; prefer configuring these under data_manager.sources . Required Fields Some fields are required depending on enabled services and pipelines. For example: name data_manager.sources.links.input_lists (or other source-specific configuration) archi.pipelines and matching archi.pipeline_map entries Service-specific fields (e.g., services.piazza.network_id , services.grader_app.num_problems ) See the User Guide for more configuration examples and explanations. Example name: my_deployment global: DATA_PATH: \"/root/data/\" ACCOUNTS_PATH: \"/root/.accounts/\" ACCEPTED_FILES: [\".txt\", \".pdf\"] LOGGING: input_output_filename: \"chain_input_output.log\" verbosity: 3 data_manager: sources: links: input_lists: - examples/deployments/basic-gpu/miscellanea.list scraper: reset_data: true verify_urls: false enable_warnings: false utils: anonymizer: nlp_model: en_core_web_sm embedding_name: \"OpenAIEmbeddings\" chunk_size: 1000 chunk_overlap: 0 num_documents_to_retrieve: 5 archi: pipelines: [\"QAPipeline\"] pipeline_map: QAPipeline: max_tokens: 10000 prompts: required: condense_prompt: \"examples/deployments/basic-gpu/condense.prompt\" chat_prompt: \"examples/deployments/basic-gpu/qa.prompt\" models: required: condense_model: \"OpenAIGPT4\" chat_model: \"OpenAIGPT4\" model_class_map: OpenAIGPT4: class: OpenAIGPT4 kwargs: model_name: gpt-4 services: chat_app: trained_on: \"Course documentation\" hostname: \"example.mit.edu\" postgres: port: 5432 database: \"archi\" Tip: For a full template, see src/cli/templates/base-config.yaml in the repository. V2 API (PostgreSQL-Consolidated) The V2 API provides REST endpoints for the PostgreSQL-consolidated architecture, using PostgreSQL with pgvector for unified vector storage and metadata. Base URL All endpoints are prefixed with /api/ . Authentication POST /api/auth/login Authenticate with email and password. Request: { \"email\": \"user@example.com\", \"password\": \"secret123\" } Response: { \"success\": true, \"user\": { \"id\": \"user_abc123\", \"email\": \"user@example.com\", \"display_name\": \"John Doe\", \"is_admin\": false }, \"session_token\": \"sess_...\" } POST /api/auth/logout End the current session. Response: { \"success\": true } GET /api/auth/me Get the current authenticated user. Response (authenticated): { \"authenticated\": true, \"user\": { \"id\": \"user_abc123\", \"email\": \"user@example.com\", \"display_name\": \"John Doe\", \"is_admin\": false } } Response (anonymous): { \"authenticated\": false, \"user\": null } User Management GET /api/users/me Get or create the current user. Response: { \"id\": \"user_abc123\", \"display_name\": \"John Doe\", \"email\": \"john@example.com\", \"auth_provider\": \"basic\", \"theme\": \"dark\", \"preferred_model\": \"gpt-4o\", \"preferred_temperature\": 0.7, \"has_openrouter_key\": true, \"has_openai_key\": false, \"has_anthropic_key\": false, \"created_at\": \"2025-01-15T10:30:00Z\" } PATCH /api/users/me/preferences Update user preferences. Request: { \"theme\": \"light\", \"preferred_model\": \"claude-3-opus\", \"preferred_temperature\": 0.5 } PUT /api/users/me/api-keys/{provider} Set BYOK API key (provider: openrouter , openai , anthropic ). Request: { \"api_key\": \"sk-...\" } DELETE /api/users/me/api-keys/{provider} Delete BYOK API key. Configuration GET /api/config/static Get static (deploy-time) configuration. Response: { \"deployment_name\": \"my-archi\", \"embedding_model\": \"text-embedding-ada-002\", \"embedding_dimensions\": 1536, \"available_pipelines\": [\"QAPipeline\", \"AgentPipeline\"], \"available_models\": [\"gpt-4o\", \"claude-3-opus\"], \"auth_enabled\": true, \"prompts_path\": \"/root/archi/data/prompts/\" } GET /api/config/dynamic Get dynamic (runtime) configuration. Response: { \"active_pipeline\": \"QAPipeline\", \"active_model\": \"gpt-4o\", \"temperature\": 0.7, \"max_tokens\": 4096, \"top_p\": 0.9, \"top_k\": 50, \"active_condense_prompt\": \"default\", \"active_chat_prompt\": \"default\", \"active_system_prompt\": \"default\", \"num_documents_to_retrieve\": 10, \"verbosity\": 3 } PATCH /api/config/dynamic Update dynamic configuration. Admin only. Request: { \"active_model\": \"gpt-4o\", \"temperature\": 0.8, \"num_documents_to_retrieve\": 5 } Response: 403 Forbidden if not admin. GET /api/config/effective Get effective configuration for the current user, with user preferences applied. Response: { \"active_model\": \"claude-3-opus\", \"temperature\": 0.5, \"max_tokens\": 4096, \"num_documents_to_retrieve\": 10, \"active_condense_prompt\": \"concise\", \"active_chat_prompt\": \"technical\" } GET /api/config/audit Get configuration change audit log. Admin only. Query params: - limit : Max entries to return (default: 100) Response: { \"entries\": [ { \"id\": 1, \"user_id\": \"admin_user\", \"changed_at\": \"2025-01-20T15:30:00Z\", \"config_type\": \"dynamic\", \"field_name\": \"temperature\", \"old_value\": \"0.7\", \"new_value\": \"0.8\" } ] } Prompts GET /api/prompts List all available prompts by type. Response: { \"condense\": [\"default\", \"concise\"], \"chat\": [\"default\", \"formal\", \"technical\"], \"system\": [\"default\", \"helpful\"] } GET /api/prompts/{type} List prompts for a specific type. Response: [\"default\", \"formal\", \"technical\"] GET /api/prompts/{type}/{name} Get prompt content. Response: { \"type\": \"chat\", \"name\": \"default\", \"content\": \"You are a helpful AI assistant...\" } POST /api/prompts/reload Reload prompt cache from disk. Admin only. Response: { \"success\": true, \"message\": \"Reloaded 7 prompts\" } Document Selection (3-Tier) The document selection system uses a 3-tier precedence: 1. Conversation override (highest priority) 2. User default 3. System default (all documents enabled) GET /api/documents/selection?conversation_id={id} Get enabled documents for a conversation. PUT /api/documents/user-defaults Set user's default for a document. Request: { \"document_id\": 42, \"enabled\": false } PUT /api/documents/conversation-override Set conversation-specific override. Request: { \"conversation_id\": 123, \"document_id\": 42, \"enabled\": true } DELETE /api/documents/conversation-override Clear conversation override (fall back to user default). Analytics GET /api/analytics/model-usage Get model usage statistics. Query params: - start_date : ISO date (optional) - end_date : ISO date (optional) - service : Filter by service (optional) GET /api/analytics/ab-comparisons Get A/B comparison statistics with win rates. Query params: - model_a : Filter by model A (optional) - model_b : Filter by model B (optional) - start_date : ISO date (optional) - end_date : ISO date (optional) Data Viewer Browse and manage ingested documents. GET /api/data/documents List all ingested documents. Query params: - limit : Max documents to return (default: 100) - offset : Pagination offset (default: 0) - search : Filter by document name - source_type : Filter by source type (e.g., links , git , ticket ) Ticketing integrations normalize to source_type: ticket and record the provider in metadata.ticket_provider (e.g., jira , redmine ). Response: { \"documents\": [ { \"hash\": \"5e90ca54526f3e11\", \"file_name\": \"readme.md\", \"source_type\": \"links\", \"chunk_count\": 5, \"enabled\": true, \"ingested_at\": \"2025-01-29T10:30:00Z\" } ], \"total\": 42 } GET /api/data/documents/<hash>/content Get document content and chunks. Response: { \"hash\": \"5e90ca54526f3e11\", \"file_name\": \"readme.md\", \"content\": \"Full document text...\", \"chunks\": [ { \"id\": 1, \"content\": \"Chunk text...\", \"metadata\": {} } ] } POST /api/data/documents/<hash>/enable Enable a document for retrieval. Response: { \"success\": true, \"hash\": \"5e90ca54526f3e11\", \"enabled\": true } POST /api/data/documents/<hash>/disable Disable a document from retrieval. Response: { \"success\": true, \"hash\": \"5e90ca54526f3e11\", \"enabled\": false } POST /api/data/bulk-enable Enable multiple documents. Request: { \"hashes\": [\"5e90ca54526f3e11\", \"a1b2c3d4e5f67890\"] } POST /api/data/bulk-disable Disable multiple documents. Request: { \"hashes\": [\"5e90ca54526f3e11\", \"a1b2c3d4e5f67890\"] } GET /api/data/stats Get document statistics. Response: { \"total_documents\": 42, \"enabled_documents\": 40, \"disabled_documents\": 2, \"total_chunks\": 350, \"by_source_type\": { \"links\": 30, \"ticket\": 12 } } Health & Info GET /api/health Health check with database connectivity status. GET /api/info Get API version and available features.","title":"API Reference"},{"location":"api_reference/#api-reference","text":"","title":"API Reference"},{"location":"api_reference/#cli","text":"The Archi CLI provides commands to create, manage, and delete Archi deployments and services.","title":"CLI"},{"location":"api_reference/#commands","text":"","title":"Commands"},{"location":"api_reference/#1-create","text":"Create a new Archi deployment. Usage: archi create --name <deployment_name> --config <config.yaml> --env-file <secrets.env> [OPTIONS] Options: --name, -n (str, required): Name of the deployment. --config, -c (str): Path to a YAML configuration file (repeat the flag to supply multiple files). --config-dir, -cd (str): Directory containing configuration files. --env-file, -e (str, required): Path to the secrets .env file. --services, -s (comma-separated, required): List of services to enable (e.g., chatbot,uploader ). --sources, -src (comma-separated): Additional data sources to enable (e.g., git,jira ). The links source is always available. --podman, -p : Use Podman instead of Docker. --gpu-ids : GPU configuration ( all or comma-separated IDs). --tag, -t (str): Image tag for built containers (default: 2000 ). --hostmode : Use host network mode. --verbosity, -v (int): Logging verbosity (0-4, default: 3). --force, -f : Overwrite existing deployment if it exists. --dry, --dry-run : Validate and show what would be created, but do not deploy.","title":"1. create"},{"location":"api_reference/#2-delete","text":"Delete an existing Archi deployment. Usage: archi delete --name <deployment_name> [OPTIONS] Options: --name, -n (str): Name of the deployment to delete. --rmi : Remove container images. --rmv : Remove volumes. --keep-files : Keep deployment files (do not remove directory). --list : List all available deployments.","title":"2. delete"},{"location":"api_reference/#3-restart","text":"Restart a specific service in an existing deployment without restarting the entire stack. Usage: archi restart --name <deployment_name> --service <service_name> [OPTIONS] Options: --name, -n (str, required): Name of the existing deployment. --service, -s (str): Service to restart (default: chatbot ). --config, -c (str): Path to updated YAML configuration file(s) (can be specified multiple times). --config-dir, -cd (str): Path to directory containing configuration files. --env-file, -e (str): Path to .env file with secrets. --no-build : Restart without rebuilding the container image. --with-deps : Also restart dependent services (by default, only the specified service is restarted). --podman, -p : Use Podman instead of Docker. --verbosity, -v (int): Logging verbosity level (0-4, default: 3). Notes: Configuration changes : Restarting with --no-build will reflect changes to configuration files. If you've modified code, you must rebuild the image (omit the --no-build flag). Updating configuration : If you provide --config or --config-dir , the command will update the deployment's configuration before restarting the service. Finding services : Use archi list-deployments to see existing deployments. If you specify an invalid service name, the restart command will display the available services for that deployment. Examples: Quick config update without rebuilding: archi restart -n mybot --service chatbot --no-build Test new agent code (requires rebuild): archi restart -n mybot --service chatbot -c updated_config.yaml Restart with updated secrets: archi restart -n mybot --service chatbot -e new_secrets.env --no-build Restart data_manager to re-scrape sources: archi restart -n mybot --service data_manager","title":"3. restart"},{"location":"api_reference/#4-list-services","text":"List all available Archi services and data sources. Usage: archi list-services","title":"4. list-services"},{"location":"api_reference/#5-list-deployments","text":"List all existing Archi deployments. Usage: archi list-deployments","title":"5. list-deployments"},{"location":"api_reference/#6-evaluate","text":"Launch the benchmarking runtime to evaluate one or more configurations against a set of questions/answers. Usage: archi evaluate --name <run_name> --env-file <secrets.env> --config <file.yaml> [OPTIONS] Use --config-dir if you want to point to a directory of configs instead. Options: Supports the same flags as create ( --sources , --podman , --gpu-ids , --tag , --hostmode , --verbosity , --force ). Reads configuration from one or more YAML files that should define the services.benchmarking section.","title":"6. evaluate"},{"location":"api_reference/#examples","text":"Create a deployment: archi create --name mybot --config my.yaml --env-file secrets.env --services chatbot,uploader Delete a deployment and remove images/volumes: archi delete --name mybot --rmi --rmv Restart a service without rebuilding: archi restart --name mybot --service chatbot --no-build List all deployments: archi list-deployments List all services: archi list-services","title":"Examples"},{"location":"api_reference/#configuration-yaml-api-reference","text":"The Archi configuration YAML file defines the deployment, services, data sources, pipelines, models, and interface settings for your Archi instance.","title":"Configuration YAML API Reference"},{"location":"api_reference/#top-level-fields","text":"","title":"Top-Level Fields"},{"location":"api_reference/#name","text":"Type: string Description: Name of the deployment.","title":"name"},{"location":"api_reference/#global","text":"DATA_PATH: path for persisted data (defaults to /root/data/ ). ACCOUNTS_PATH: path for uploader/grader account data. ACCEPTED_FILES: list of extensions allowed for manual uploads. LOGGING.input_output_filename: log file that stores pipeline inputs/outputs. verbosity: default logging level for services (0-4).","title":"global"},{"location":"api_reference/#services","text":"Holds configuration for every containerised service. Common keys include: port / external_port: internal versus host port mapping for web apps. host / hostname: network binding and public hostname for frontends. volume/paths: template or static asset paths expected by the service. Key services: chat_app: Chat interface options ( trained_on , ports, UI toggles). uploader_app: Document uploader settings ( verify_urls , ports). grader_app: Grader-specific knobs ( num_problems , rubric paths). grafana: Port configuration for the monitoring dashboard. postgres: Database credentials ( user , database , port , host ). Also used for vector storage via pgvector. piazza , mattermost , redmine_mailbox , benchmarking , ...: Service-specific options (see user guide sections above).","title":"services"},{"location":"api_reference/#data_manager","text":"Controls ingestion sources and vector store behaviour. sources.links.input_lists: .list files with seed URLs. sources.links.scraper: Behaviour toggles for HTTP scraping (resetting data, URL verification, warning output). sources.links.selenium_scraper: Selenium configuration used for SSO scraping and optional link scraping. sources. .visible: Mark whether documents harvested from a source should appear in chat citations and other user-facing listings ( true by default). sources.git.enabled / sources.sso.enabled / sources.jira.enabled / sources.redmine.enabled: Toggle additional collectors when paired with --sources . sources.jira.cutoff_date: ISO-8601 date; JIRA tickets created before this are ignored. embedding_name: Embedding backend ( OpenAIEmbeddings , HuggingFaceEmbeddings , ...). embedding_class_map: Backend specific parameters (model name, device, similarity threshold). chunk_size / chunk_overlap: Text splitter parameters. reset_collection: Whether to wipe the collection before re-populating. num_documents_to_retrieve: Top-k documents returned at query time. distance_metric / use_hybrid_search / bm25_weight / semantic_weight: Retrieval tuning knobs (BM25 k1 / b are fixed when the PostgreSQL index is created). utils.anonymizer (legacy) / data_manager.utils.anonymizer : Redaction settings applied when ticket collectors anonymise content. Source configuration is persisted to PostgreSQL static_config.sources_config at deployment time and used for runtime ingestion.","title":"data_manager"},{"location":"api_reference/#archi","text":"Defines pipelines and model routing. pipelines: List of pipeline names to load (e.g., QAPipeline ). pipeline_map: Per-pipeline configuration of prompts, models, token limits, and ReAct recursion limits ( recursion_limit , default 100 ). model_class_map: Definitions for each model family (base model names, provider-specific kwargs). chain_update_time: Polling interval for hot-reloading chains.","title":"archi"},{"location":"api_reference/#utils","text":"Utility configuration for supporting components (mostly legacy fallbacks): git: Legacy toggle for Git scraping. jira / redmine: Compatibility settings for ticket integrations; prefer configuring these under data_manager.sources .","title":"utils"},{"location":"api_reference/#required-fields","text":"Some fields are required depending on enabled services and pipelines. For example: name data_manager.sources.links.input_lists (or other source-specific configuration) archi.pipelines and matching archi.pipeline_map entries Service-specific fields (e.g., services.piazza.network_id , services.grader_app.num_problems ) See the User Guide for more configuration examples and explanations.","title":"Required Fields"},{"location":"api_reference/#example","text":"name: my_deployment global: DATA_PATH: \"/root/data/\" ACCOUNTS_PATH: \"/root/.accounts/\" ACCEPTED_FILES: [\".txt\", \".pdf\"] LOGGING: input_output_filename: \"chain_input_output.log\" verbosity: 3 data_manager: sources: links: input_lists: - examples/deployments/basic-gpu/miscellanea.list scraper: reset_data: true verify_urls: false enable_warnings: false utils: anonymizer: nlp_model: en_core_web_sm embedding_name: \"OpenAIEmbeddings\" chunk_size: 1000 chunk_overlap: 0 num_documents_to_retrieve: 5 archi: pipelines: [\"QAPipeline\"] pipeline_map: QAPipeline: max_tokens: 10000 prompts: required: condense_prompt: \"examples/deployments/basic-gpu/condense.prompt\" chat_prompt: \"examples/deployments/basic-gpu/qa.prompt\" models: required: condense_model: \"OpenAIGPT4\" chat_model: \"OpenAIGPT4\" model_class_map: OpenAIGPT4: class: OpenAIGPT4 kwargs: model_name: gpt-4 services: chat_app: trained_on: \"Course documentation\" hostname: \"example.mit.edu\" postgres: port: 5432 database: \"archi\" Tip: For a full template, see src/cli/templates/base-config.yaml in the repository.","title":"Example"},{"location":"api_reference/#v2-api-postgresql-consolidated","text":"The V2 API provides REST endpoints for the PostgreSQL-consolidated architecture, using PostgreSQL with pgvector for unified vector storage and metadata.","title":"V2 API (PostgreSQL-Consolidated)"},{"location":"api_reference/#base-url","text":"All endpoints are prefixed with /api/ .","title":"Base URL"},{"location":"api_reference/#authentication","text":"","title":"Authentication"},{"location":"api_reference/#post-apiauthlogin","text":"Authenticate with email and password. Request: { \"email\": \"user@example.com\", \"password\": \"secret123\" } Response: { \"success\": true, \"user\": { \"id\": \"user_abc123\", \"email\": \"user@example.com\", \"display_name\": \"John Doe\", \"is_admin\": false }, \"session_token\": \"sess_...\" }","title":"POST /api/auth/login"},{"location":"api_reference/#post-apiauthlogout","text":"End the current session. Response: { \"success\": true }","title":"POST /api/auth/logout"},{"location":"api_reference/#get-apiauthme","text":"Get the current authenticated user. Response (authenticated): { \"authenticated\": true, \"user\": { \"id\": \"user_abc123\", \"email\": \"user@example.com\", \"display_name\": \"John Doe\", \"is_admin\": false } } Response (anonymous): { \"authenticated\": false, \"user\": null }","title":"GET /api/auth/me"},{"location":"api_reference/#user-management","text":"","title":"User Management"},{"location":"api_reference/#get-apiusersme","text":"Get or create the current user. Response: { \"id\": \"user_abc123\", \"display_name\": \"John Doe\", \"email\": \"john@example.com\", \"auth_provider\": \"basic\", \"theme\": \"dark\", \"preferred_model\": \"gpt-4o\", \"preferred_temperature\": 0.7, \"has_openrouter_key\": true, \"has_openai_key\": false, \"has_anthropic_key\": false, \"created_at\": \"2025-01-15T10:30:00Z\" }","title":"GET /api/users/me"},{"location":"api_reference/#patch-apiusersmepreferences","text":"Update user preferences. Request: { \"theme\": \"light\", \"preferred_model\": \"claude-3-opus\", \"preferred_temperature\": 0.5 }","title":"PATCH /api/users/me/preferences"},{"location":"api_reference/#put-apiusersmeapi-keysprovider","text":"Set BYOK API key (provider: openrouter , openai , anthropic ). Request: { \"api_key\": \"sk-...\" }","title":"PUT /api/users/me/api-keys/{provider}"},{"location":"api_reference/#delete-apiusersmeapi-keysprovider","text":"Delete BYOK API key.","title":"DELETE /api/users/me/api-keys/{provider}"},{"location":"api_reference/#configuration","text":"","title":"Configuration"},{"location":"api_reference/#get-apiconfigstatic","text":"Get static (deploy-time) configuration. Response: { \"deployment_name\": \"my-archi\", \"embedding_model\": \"text-embedding-ada-002\", \"embedding_dimensions\": 1536, \"available_pipelines\": [\"QAPipeline\", \"AgentPipeline\"], \"available_models\": [\"gpt-4o\", \"claude-3-opus\"], \"auth_enabled\": true, \"prompts_path\": \"/root/archi/data/prompts/\" }","title":"GET /api/config/static"},{"location":"api_reference/#get-apiconfigdynamic","text":"Get dynamic (runtime) configuration. Response: { \"active_pipeline\": \"QAPipeline\", \"active_model\": \"gpt-4o\", \"temperature\": 0.7, \"max_tokens\": 4096, \"top_p\": 0.9, \"top_k\": 50, \"active_condense_prompt\": \"default\", \"active_chat_prompt\": \"default\", \"active_system_prompt\": \"default\", \"num_documents_to_retrieve\": 10, \"verbosity\": 3 }","title":"GET /api/config/dynamic"},{"location":"api_reference/#patch-apiconfigdynamic","text":"Update dynamic configuration. Admin only. Request: { \"active_model\": \"gpt-4o\", \"temperature\": 0.8, \"num_documents_to_retrieve\": 5 } Response: 403 Forbidden if not admin.","title":"PATCH /api/config/dynamic"},{"location":"api_reference/#get-apiconfigeffective","text":"Get effective configuration for the current user, with user preferences applied. Response: { \"active_model\": \"claude-3-opus\", \"temperature\": 0.5, \"max_tokens\": 4096, \"num_documents_to_retrieve\": 10, \"active_condense_prompt\": \"concise\", \"active_chat_prompt\": \"technical\" }","title":"GET /api/config/effective"},{"location":"api_reference/#get-apiconfigaudit","text":"Get configuration change audit log. Admin only. Query params: - limit : Max entries to return (default: 100) Response: { \"entries\": [ { \"id\": 1, \"user_id\": \"admin_user\", \"changed_at\": \"2025-01-20T15:30:00Z\", \"config_type\": \"dynamic\", \"field_name\": \"temperature\", \"old_value\": \"0.7\", \"new_value\": \"0.8\" } ] }","title":"GET /api/config/audit"},{"location":"api_reference/#prompts","text":"","title":"Prompts"},{"location":"api_reference/#get-apiprompts","text":"List all available prompts by type. Response: { \"condense\": [\"default\", \"concise\"], \"chat\": [\"default\", \"formal\", \"technical\"], \"system\": [\"default\", \"helpful\"] }","title":"GET /api/prompts"},{"location":"api_reference/#get-apipromptstype","text":"List prompts for a specific type. Response: [\"default\", \"formal\", \"technical\"]","title":"GET /api/prompts/{type}"},{"location":"api_reference/#get-apipromptstypename","text":"Get prompt content. Response: { \"type\": \"chat\", \"name\": \"default\", \"content\": \"You are a helpful AI assistant...\" }","title":"GET /api/prompts/{type}/{name}"},{"location":"api_reference/#post-apipromptsreload","text":"Reload prompt cache from disk. Admin only. Response: { \"success\": true, \"message\": \"Reloaded 7 prompts\" }","title":"POST /api/prompts/reload"},{"location":"api_reference/#document-selection-3-tier","text":"The document selection system uses a 3-tier precedence: 1. Conversation override (highest priority) 2. User default 3. System default (all documents enabled)","title":"Document Selection (3-Tier)"},{"location":"api_reference/#get-apidocumentsselectionconversation_idid","text":"Get enabled documents for a conversation.","title":"GET /api/documents/selection?conversation_id={id}"},{"location":"api_reference/#put-apidocumentsuser-defaults","text":"Set user's default for a document. Request: { \"document_id\": 42, \"enabled\": false }","title":"PUT /api/documents/user-defaults"},{"location":"api_reference/#put-apidocumentsconversation-override","text":"Set conversation-specific override. Request: { \"conversation_id\": 123, \"document_id\": 42, \"enabled\": true }","title":"PUT /api/documents/conversation-override"},{"location":"api_reference/#delete-apidocumentsconversation-override","text":"Clear conversation override (fall back to user default).","title":"DELETE /api/documents/conversation-override"},{"location":"api_reference/#analytics","text":"","title":"Analytics"},{"location":"api_reference/#get-apianalyticsmodel-usage","text":"Get model usage statistics. Query params: - start_date : ISO date (optional) - end_date : ISO date (optional) - service : Filter by service (optional)","title":"GET /api/analytics/model-usage"},{"location":"api_reference/#get-apianalyticsab-comparisons","text":"Get A/B comparison statistics with win rates. Query params: - model_a : Filter by model A (optional) - model_b : Filter by model B (optional) - start_date : ISO date (optional) - end_date : ISO date (optional)","title":"GET /api/analytics/ab-comparisons"},{"location":"api_reference/#data-viewer","text":"Browse and manage ingested documents.","title":"Data Viewer"},{"location":"api_reference/#get-apidatadocuments","text":"List all ingested documents. Query params: - limit : Max documents to return (default: 100) - offset : Pagination offset (default: 0) - search : Filter by document name - source_type : Filter by source type (e.g., links , git , ticket ) Ticketing integrations normalize to source_type: ticket and record the provider in metadata.ticket_provider (e.g., jira , redmine ). Response: { \"documents\": [ { \"hash\": \"5e90ca54526f3e11\", \"file_name\": \"readme.md\", \"source_type\": \"links\", \"chunk_count\": 5, \"enabled\": true, \"ingested_at\": \"2025-01-29T10:30:00Z\" } ], \"total\": 42 }","title":"GET /api/data/documents"},{"location":"api_reference/#get-apidatadocumentshashcontent","text":"Get document content and chunks. Response: { \"hash\": \"5e90ca54526f3e11\", \"file_name\": \"readme.md\", \"content\": \"Full document text...\", \"chunks\": [ { \"id\": 1, \"content\": \"Chunk text...\", \"metadata\": {} } ] }","title":"GET /api/data/documents/&lt;hash&gt;/content"},{"location":"api_reference/#post-apidatadocumentshashenable","text":"Enable a document for retrieval. Response: { \"success\": true, \"hash\": \"5e90ca54526f3e11\", \"enabled\": true }","title":"POST /api/data/documents/&lt;hash&gt;/enable"},{"location":"api_reference/#post-apidatadocumentshashdisable","text":"Disable a document from retrieval. Response: { \"success\": true, \"hash\": \"5e90ca54526f3e11\", \"enabled\": false }","title":"POST /api/data/documents/&lt;hash&gt;/disable"},{"location":"api_reference/#post-apidatabulk-enable","text":"Enable multiple documents. Request: { \"hashes\": [\"5e90ca54526f3e11\", \"a1b2c3d4e5f67890\"] }","title":"POST /api/data/bulk-enable"},{"location":"api_reference/#post-apidatabulk-disable","text":"Disable multiple documents. Request: { \"hashes\": [\"5e90ca54526f3e11\", \"a1b2c3d4e5f67890\"] }","title":"POST /api/data/bulk-disable"},{"location":"api_reference/#get-apidatastats","text":"Get document statistics. Response: { \"total_documents\": 42, \"enabled_documents\": 40, \"disabled_documents\": 2, \"total_chunks\": 350, \"by_source_type\": { \"links\": 30, \"ticket\": 12 } }","title":"GET /api/data/stats"},{"location":"api_reference/#health-info","text":"","title":"Health &amp; Info"},{"location":"api_reference/#get-apihealth","text":"Health check with database connectivity status.","title":"GET /api/health"},{"location":"api_reference/#get-apiinfo","text":"Get API version and available features.","title":"GET /api/info"},{"location":"developer_guide/","text":"Developers Guide Below is all the information developers may need to get started contributing to the Archi project. Editing Documentation Editing documentation requires the mkdocs Python package: pip install mkdocs To edit documentation, update the .md and .yml files in the ./docs folder. To preview changes locally, run: cd docs mkdocs serve Add the -a IP:HOST argument (default is localhost:8000 ) to specify the host and port. Publish your changes with: mkdocs gh-deploy Always open a PR to merge documentation changes into main . Do not edit files directly in the gh-pages branch. Smoke Tests If you want the full CI-like smoke run (create deployment, wait for readiness, run checks, and clean up) you can use the shared runner: export Archi_DIR=~/.archi export DEPLOYMENT_NAME=local-smoke export USE_PODMAN=true export SMOKE_FORCE_CREATE=true export SMOKE_OLLAMA_MODEL=gpt-oss:latest scripts/dev/run_smoke_preview.sh \"${DEPLOYMENT_NAME}\" The shared runner performs these checks in order (ensuring the configured Ollama model is available via ollama pull before running the checks): Create a deployment from the preview config and wait for the chat app health endpoint. Wait for initial data ingestion to complete (5 minute timeout). Preflight checks: Postgres reachable, data-manager catalog searchable. Tool probes: catalog tools and vectorstore retriever (executed inside the chatbot container to match the agent runtime). ReAct agent smoke: stream response and observe at least one tool call. The combined smoke workflow alone does not start A2rchi for you. Start a deployment first, then run the checks (it validates Postgres, data-manager catalog, Ollama model availability, ReAct streaming, and direct tool probes inside the chatbot container): export Archi_CONFIG_PATH=~/.archi/archi-<deployment-name>/configs/<config-name>.yaml export Archi_CONFIG_NAME=<config-name> export Archi_PIPELINE_NAME=CMSCompOpsAgent export USE_PODMAN=true export OLLAMA_MODEL=<ollama-model-name> export PGHOST=localhost export PGPORT=<postgres-port> export PGUSER=archi export PGPASSWORD=<pg-password> export PGDATABASE=archi-db export BASE_URL=http://localhost:2786 export DM_BASE_URL=http://localhost:<data-manager-port> # from your deployment config export OLLAMA_URL=http://localhost:11434 ./tests/smoke/combined_smoke.sh <deployment-name> Optional environment variables for deterministic queries: export REACT_SMOKE_PROMPT=\"Use the search_local_files tool to find ... and summarize.\" export FILE_SEARCH_QUERY=\"first linux server installation\" export METADATA_SEARCH_QUERY=\"ppc.mit.edu\" export VECTORSTORE_QUERY=\"cms\" Postgres Usage Overview Archi relies on Postgres as the durable metadata store across services. Core usage falls into two buckets: Ingestion catalog : the resources table tracks persisted files and metadata for the data manager catalog ( CatalogService ). Conversation history : the conversation_metadata and conversations tables store chat/session metadata plus message history for interfaces like the chat app and ticketing integrations (e.g., Redmine mailer). The conversations table tracks: - model_used (string) - The model that generated the response (e.g., \"openai/gpt-4o\") - pipeline_used (string) - The pipeline that processed the request (e.g., \"QAPipeline\") Additional supporting tables store interaction telemetry and feedback: feedback captures like/dislike/comment feedback tied to conversation messages. timing tracks per-message latency milestones. agent_tool_calls stores tool call inputs/outputs extracted from agent messages. When extending an interface that writes to conversations , make sure a matching conversation_metadata row exists (create or update before inserting messages) to satisfy foreign key constraints. DockerHub Images Archi loads different base images hosted on Docker Hub. The Python base image is used when GPUs are not required; otherwise the PyTorch base image is used. The Dockerfiles for these base images live in src/cli/templates/dockerfiles/base-X-image . Images are hosted at: Python: https://hub.docker.com/r/archi/archi-python-base PyTorch: https://hub.docker.com/r/archi/archi-pytorch-base To rebuild a base image, navigate to the relevant base-xxx-image directory under src/cli/templates/dockerfiles/ . Each directory contains the Dockerfile, requirements, and license information. Regenerate the requirements files with: # Python image cat requirements/cpu-requirementsHEADER.txt requirements/requirements-base.txt > src/cli/templates/dockerfiles/base-python-image/requirements.txt # PyTorch image cat requirements/gpu-requirementsHEADER.txt requirements/requirements-base.txt > src/cli/templates/dockerfiles/base-pytorch-image/requirements.txt Build the image: podman build -t archi/<image-name>:<tag> . After verifying the image, log in to Docker Hub (ask a senior developer for credentials): podman login docker.io Push the image: podman push archi/<image-name>:<tag> Data Ingestion Architecture Archi ingests content through sources which are collected by collectors ( data_manager/collectors ). These documents are written to persistent, local files via the PersistenceService , which uses Resource objects as an abstraction for different content types, and ResourceMetadata for associated metadata. A catalog of persisted files and metadata is maintained in Postgres via CatalogService (table: resources ). Finally, the VectorStoreManager reads these files, splits them into chunks, generates embeddings, and indexes them in PostgreSQL with pgvector. Resources and BaseResource Every collected artifact from the collectors is represented as a subclass of BaseResource ( src/data_manager/collectors/resource_base.py ). Subclasses must implement: get_hash() : a stable identifier used as the key in the filesystem catalog. get_filename() : the on-disk file name (including extension). get_content() : returns the textual or binary payload that should be persisted. Resources may optionally override: get_metadata() : returns a metadata object (typically ResourceMetadata ) describing the item. Keys should be serialisable strings and are flattened into the vector store metadata. get_metadata_path() : legacy helper for .meta.yaml paths (metadata is now stored in Postgres). ResourceMetadata ( src/data_manager/collectors/utils/metadata.py ) enforces a required file_name and normalises the extra dictionary so all values become strings. Optional UI labels like display_name live in extra , alongside source-specific information such as URLs, ticket identifiers, or visibility flags. The guiding philosophy is that resources describe content , but never write to disk themselves. This separation keeps collectors simple, testable, and ensures consistent validation when persisting different resource types. Persistence Service PersistenceService ( src/data_manager/collectors/persistence.py ) centralises all filesystem writes for document content and metadata catalog updates. When persist_resource() is called it: Resolves the target path under the configured DATA_PATH . Validates and writes the resource content (rejecting empty payloads or unknown types). Normalises metadata (if provided) for storage. Upserts a row into the Postgres resources catalog with file and metadata fields. Collectors only interact with PersistenceService ; they should not touch the filesystem directly. Vector Database The vector store lives under the data_manager/vectorstore package. VectorStoreManager reads the Postgres catalog and manages embeddings in PostgreSQL: Loads the tracked files and metadata hashes from the Postgres catalog. Splits documents into chunks, optional stemming, and builds embeddings via the configured model. Adds chunks to the document_chunks table with embeddings and flattened metadata (including resource hash, filename, human-readable display fields, and any source-specific extras). Deletes stale entries when the underlying files disappear or are superseded. Because the manager defers to the catalog, any resource persisted through PersistenceService automatically becomes eligible for indexing\u2014no extra plumbing is required. Catalog Verification Checklist Confirm the Postgres resources table exists and is reachable from the service containers. Ingest or upload a new document and verify a new row appears in resources . Verify VectorStoreManager can update the collection using the Postgres catalog. Extending the stack When integrating a new source, create a collector under data_manager/collectors . Collectors should yield Resource objects. A new Resource subclass is only needed if the content type is not already represented (e.g., text, HTML, markdown, images, etc.), but it must implement the required methods described above. When integrating a new collector, ensure that any per-source configuration is encoded in the resource metadata so downstream consumers\u2014such as the chat app\u2014can honour it. When extending the embedding pipeline or storage schema, keep this flow in mind: collectors produce resources \u2192 PersistenceService writes files and updates the Postgres catalog \u2192 VectorStoreManager indexes embeddings in PostgreSQL. Keeping responsibilities narrowly scoped makes the ingestion stack easier to reason about and evolve.","title":"Developer Guide"},{"location":"developer_guide/#developers-guide","text":"Below is all the information developers may need to get started contributing to the Archi project.","title":"Developers Guide"},{"location":"developer_guide/#editing-documentation","text":"Editing documentation requires the mkdocs Python package: pip install mkdocs To edit documentation, update the .md and .yml files in the ./docs folder. To preview changes locally, run: cd docs mkdocs serve Add the -a IP:HOST argument (default is localhost:8000 ) to specify the host and port. Publish your changes with: mkdocs gh-deploy Always open a PR to merge documentation changes into main . Do not edit files directly in the gh-pages branch.","title":"Editing Documentation"},{"location":"developer_guide/#smoke-tests","text":"If you want the full CI-like smoke run (create deployment, wait for readiness, run checks, and clean up) you can use the shared runner: export Archi_DIR=~/.archi export DEPLOYMENT_NAME=local-smoke export USE_PODMAN=true export SMOKE_FORCE_CREATE=true export SMOKE_OLLAMA_MODEL=gpt-oss:latest scripts/dev/run_smoke_preview.sh \"${DEPLOYMENT_NAME}\" The shared runner performs these checks in order (ensuring the configured Ollama model is available via ollama pull before running the checks): Create a deployment from the preview config and wait for the chat app health endpoint. Wait for initial data ingestion to complete (5 minute timeout). Preflight checks: Postgres reachable, data-manager catalog searchable. Tool probes: catalog tools and vectorstore retriever (executed inside the chatbot container to match the agent runtime). ReAct agent smoke: stream response and observe at least one tool call. The combined smoke workflow alone does not start A2rchi for you. Start a deployment first, then run the checks (it validates Postgres, data-manager catalog, Ollama model availability, ReAct streaming, and direct tool probes inside the chatbot container): export Archi_CONFIG_PATH=~/.archi/archi-<deployment-name>/configs/<config-name>.yaml export Archi_CONFIG_NAME=<config-name> export Archi_PIPELINE_NAME=CMSCompOpsAgent export USE_PODMAN=true export OLLAMA_MODEL=<ollama-model-name> export PGHOST=localhost export PGPORT=<postgres-port> export PGUSER=archi export PGPASSWORD=<pg-password> export PGDATABASE=archi-db export BASE_URL=http://localhost:2786 export DM_BASE_URL=http://localhost:<data-manager-port> # from your deployment config export OLLAMA_URL=http://localhost:11434 ./tests/smoke/combined_smoke.sh <deployment-name> Optional environment variables for deterministic queries: export REACT_SMOKE_PROMPT=\"Use the search_local_files tool to find ... and summarize.\" export FILE_SEARCH_QUERY=\"first linux server installation\" export METADATA_SEARCH_QUERY=\"ppc.mit.edu\" export VECTORSTORE_QUERY=\"cms\"","title":"Smoke Tests"},{"location":"developer_guide/#postgres-usage-overview","text":"Archi relies on Postgres as the durable metadata store across services. Core usage falls into two buckets: Ingestion catalog : the resources table tracks persisted files and metadata for the data manager catalog ( CatalogService ). Conversation history : the conversation_metadata and conversations tables store chat/session metadata plus message history for interfaces like the chat app and ticketing integrations (e.g., Redmine mailer). The conversations table tracks: - model_used (string) - The model that generated the response (e.g., \"openai/gpt-4o\") - pipeline_used (string) - The pipeline that processed the request (e.g., \"QAPipeline\") Additional supporting tables store interaction telemetry and feedback: feedback captures like/dislike/comment feedback tied to conversation messages. timing tracks per-message latency milestones. agent_tool_calls stores tool call inputs/outputs extracted from agent messages. When extending an interface that writes to conversations , make sure a matching conversation_metadata row exists (create or update before inserting messages) to satisfy foreign key constraints.","title":"Postgres Usage Overview"},{"location":"developer_guide/#dockerhub-images","text":"Archi loads different base images hosted on Docker Hub. The Python base image is used when GPUs are not required; otherwise the PyTorch base image is used. The Dockerfiles for these base images live in src/cli/templates/dockerfiles/base-X-image . Images are hosted at: Python: https://hub.docker.com/r/archi/archi-python-base PyTorch: https://hub.docker.com/r/archi/archi-pytorch-base To rebuild a base image, navigate to the relevant base-xxx-image directory under src/cli/templates/dockerfiles/ . Each directory contains the Dockerfile, requirements, and license information. Regenerate the requirements files with: # Python image cat requirements/cpu-requirementsHEADER.txt requirements/requirements-base.txt > src/cli/templates/dockerfiles/base-python-image/requirements.txt # PyTorch image cat requirements/gpu-requirementsHEADER.txt requirements/requirements-base.txt > src/cli/templates/dockerfiles/base-pytorch-image/requirements.txt Build the image: podman build -t archi/<image-name>:<tag> . After verifying the image, log in to Docker Hub (ask a senior developer for credentials): podman login docker.io Push the image: podman push archi/<image-name>:<tag>","title":"DockerHub Images"},{"location":"developer_guide/#data-ingestion-architecture","text":"Archi ingests content through sources which are collected by collectors ( data_manager/collectors ). These documents are written to persistent, local files via the PersistenceService , which uses Resource objects as an abstraction for different content types, and ResourceMetadata for associated metadata. A catalog of persisted files and metadata is maintained in Postgres via CatalogService (table: resources ). Finally, the VectorStoreManager reads these files, splits them into chunks, generates embeddings, and indexes them in PostgreSQL with pgvector.","title":"Data Ingestion Architecture"},{"location":"developer_guide/#resources-and-baseresource","text":"Every collected artifact from the collectors is represented as a subclass of BaseResource ( src/data_manager/collectors/resource_base.py ). Subclasses must implement: get_hash() : a stable identifier used as the key in the filesystem catalog. get_filename() : the on-disk file name (including extension). get_content() : returns the textual or binary payload that should be persisted. Resources may optionally override: get_metadata() : returns a metadata object (typically ResourceMetadata ) describing the item. Keys should be serialisable strings and are flattened into the vector store metadata. get_metadata_path() : legacy helper for .meta.yaml paths (metadata is now stored in Postgres). ResourceMetadata ( src/data_manager/collectors/utils/metadata.py ) enforces a required file_name and normalises the extra dictionary so all values become strings. Optional UI labels like display_name live in extra , alongside source-specific information such as URLs, ticket identifiers, or visibility flags. The guiding philosophy is that resources describe content , but never write to disk themselves. This separation keeps collectors simple, testable, and ensures consistent validation when persisting different resource types.","title":"Resources and BaseResource"},{"location":"developer_guide/#persistence-service","text":"PersistenceService ( src/data_manager/collectors/persistence.py ) centralises all filesystem writes for document content and metadata catalog updates. When persist_resource() is called it: Resolves the target path under the configured DATA_PATH . Validates and writes the resource content (rejecting empty payloads or unknown types). Normalises metadata (if provided) for storage. Upserts a row into the Postgres resources catalog with file and metadata fields. Collectors only interact with PersistenceService ; they should not touch the filesystem directly.","title":"Persistence Service"},{"location":"developer_guide/#vector-database","text":"The vector store lives under the data_manager/vectorstore package. VectorStoreManager reads the Postgres catalog and manages embeddings in PostgreSQL: Loads the tracked files and metadata hashes from the Postgres catalog. Splits documents into chunks, optional stemming, and builds embeddings via the configured model. Adds chunks to the document_chunks table with embeddings and flattened metadata (including resource hash, filename, human-readable display fields, and any source-specific extras). Deletes stale entries when the underlying files disappear or are superseded. Because the manager defers to the catalog, any resource persisted through PersistenceService automatically becomes eligible for indexing\u2014no extra plumbing is required.","title":"Vector Database"},{"location":"developer_guide/#catalog-verification-checklist","text":"Confirm the Postgres resources table exists and is reachable from the service containers. Ingest or upload a new document and verify a new row appears in resources . Verify VectorStoreManager can update the collection using the Postgres catalog.","title":"Catalog Verification Checklist"},{"location":"developer_guide/#extending-the-stack","text":"When integrating a new source, create a collector under data_manager/collectors . Collectors should yield Resource objects. A new Resource subclass is only needed if the content type is not already represented (e.g., text, HTML, markdown, images, etc.), but it must implement the required methods described above. When integrating a new collector, ensure that any per-source configuration is encoded in the resource metadata so downstream consumers\u2014such as the chat app\u2014can honour it. When extending the embedding pipeline or storage schema, keep this flow in mind: collectors produce resources \u2192 PersistenceService writes files and updates the Postgres catalog \u2192 VectorStoreManager indexes embeddings in PostgreSQL. Keeping responsibilities narrowly scoped makes the ingestion stack easier to reason about and evolve.","title":"Extending the stack"},{"location":"install/","text":"Install System Requirements Archi is deployed using a Python-based CLI onto containers. It requires: docker version 24+ or podman version 5.4.0+ (for containers) python 3.10.0+ (for the CLI) Note: We support either running open-source models locally or connecting to existing APIs. If you plan to run open-source models on your machine's GPUs, see the Advanced Setup & Deployment section. Installation Clone the Archi repository: git clone https://github.com/mit-submit/archi.git Check out the latest stable tag (recommended for users; stay on main only if you're actively developing): cd archi git checkout $(git describe --tags $(git rev-list --tags --max-count=1)) Install Archi (from inside the repository): pip install -e . This installs Archi's dependencies and the CLI tool. Verify the installation with: which archi The command prints the path to the archi executable. Show Full Installation Script # Clone the repository git clone https://github.com/mit-submit/archi.git cd archi export ARCHI_DIR=$(pwd) # (Optional) Checkout the latest stable tag (recommended for users) # Skip this if you're developing and want the tip of main. git checkout $(git describe --tags $(git rev-list --tags --max-count=1)) # (Optional) Create and activate a virtual environment python3 -m venv archi_venv source archi_venv/bin/activate # Install dependencies cd \"$ARCHI_DIR\" pip install -e . # Verify installation which archi","title":"Install"},{"location":"install/#install","text":"","title":"Install"},{"location":"install/#system-requirements","text":"Archi is deployed using a Python-based CLI onto containers. It requires: docker version 24+ or podman version 5.4.0+ (for containers) python 3.10.0+ (for the CLI) Note: We support either running open-source models locally or connecting to existing APIs. If you plan to run open-source models on your machine's GPUs, see the Advanced Setup & Deployment section.","title":"System Requirements"},{"location":"install/#installation","text":"Clone the Archi repository: git clone https://github.com/mit-submit/archi.git Check out the latest stable tag (recommended for users; stay on main only if you're actively developing): cd archi git checkout $(git describe --tags $(git rev-list --tags --max-count=1)) Install Archi (from inside the repository): pip install -e . This installs Archi's dependencies and the CLI tool. Verify the installation with: which archi The command prints the path to the archi executable. Show Full Installation Script # Clone the repository git clone https://github.com/mit-submit/archi.git cd archi export ARCHI_DIR=$(pwd) # (Optional) Checkout the latest stable tag (recommended for users) # Skip this if you're developing and want the tip of main. git checkout $(git describe --tags $(git rev-list --tags --max-count=1)) # (Optional) Create and activate a virtual environment python3 -m venv archi_venv source archi_venv/bin/activate # Install dependencies cd \"$ARCHI_DIR\" pip install -e . # Verify installation which archi","title":"Installation"},{"location":"quickstart/","text":"Quickstart Deploy your first instance of Archi and walk through the important concepts. Sources and Services Archi can ingest data from a variety of sources and supports several services . List them with the CLI command below and decide which ones you want to use so that we can configure them. archi list-services Example output: Available Archi services: Application Services: chatbot Interactive chat interface for users to communicate with the AI agent grafana Monitoring dashboard for system and LLM performance metrics uploader Admin interface for uploading and managing documents grader Automated grading service for assignments with web interface Integration Services: piazza Integration service for Piazza posts and Slack notifications mattermost Integration service for Mattermost channels redmine-mailer Email processing and Cleo/Redmine ticket management Data Sources: git Git repository scraping for MkDocs-based documentation jira Jira issue tracking integration redmine Redmine ticket integration sso SSO-backed web crawling See the User Guide for detailed information about each service and source. Pipelines Archi supports several pipelines\u2014pre-defined sequences of operations that process user inputs and generate responses. Each service supports a subset of pipelines (see the User Guide for details). An example pipeline is QAPipeline , a question-answering pipeline that takes a user's question, retrieves relevant documents from the vector store, and generates an answer using a language model. You specify which pipelines should be available in the configuration file. Configuration Once you have chosen the services, sources, and pipelines you want to use, create a configuration file that specifies their settings. You can start from one of the example configuration files under examples/deployments/ , or create your own from scratch. This file sets parameters; the selected services and sources are determined at deployment time. Important: The configuration file follows the format of src/cli/templates/base-config.yaml . Any fields not specified in your configuration will be populated with the defaults from this template. Example configuration ( examples/deployments/basic-gpu/config.yaml ) for the chatbot service using QAPipeline with a local VLLM model: name: my_archi data_manager: sources: links: visible: true # include scraped pages in the chat citations input_lists: - examples/deployments/basic-gpu/miscellanea.list embedding_name: HuggingFaceEmbeddings chunk_size: 1000 archi: pipelines: - QAPipeline pipeline_map: QAPipeline: prompts: required: condense_prompt: examples/deployments/basic-gpu/condense.prompt chat_prompt: examples/deployments/basic-gpu/qa.prompt models: required: chat_model: VLLM condense_model: VLLM model_class_map: VLLM: kwargs: base_model: 'Qwen/Qwen2.5-7B-Instruct-1M' quantization: True max_model_len: 32768 tensor_parallel_size: 2 repetition_penalty: 1.0 gpu_memory_utilization: 0.5 services: chat_app: trained_on: \"My data\" hostname: \"<your-hostname>\" vectorstore: backend: postgres # Uses PostgreSQL with pgvector (default) Explanation of configuration parameters - `name`: Name of your Archi deployment. - `data_manager`: Settings related to data ingestion and the vector store. - `sources.links.input_lists`: Lists of URLs to seed the deployment. - `sources. .visible`: Controls whether content from a given source should be surfaced to end users (defaults to `true`). - `embedding_name`: Embedding model used for vectorization. - `chunk_size`: Controls how documents are split prior to embedding. - `archi`: Core pipeline settings. - `pipelines`: Pipelines to use (e.g., `QAPipeline`). - `pipeline_map`: Configuration for each pipeline, including prompts and models. - `model_class_map`: Mapping of model names to their classes and parameters. - `services`: Settings for individual services/interfaces. - `chat_app`: Chat interface configuration, including hostname and descriptive metadata. - `vectorstore.backend`: Vector store backend (`postgres` with pgvector). Secrets Secrets are sensitive values (passwords, API keys, etc.) that should not be stored directly in code or configuration files. Store them in a single .env file on your filesystem. Minimal deployments (chatbot with open-source LLM and embeddings) require: PG_PASSWORD : password used to secure the database. Create the secrets file with: echo \"PG_PASSWORD=my_strong_password\" > ~/.secrets.env If you are not using open-source models, supply the relevant API credentials: OPENAI_API_KEY : OpenAI API key. OPENROUTER_API_KEY : OpenRouter API key (for OpenRouterLLM ). OPENROUTER_SITE_URL : Optional site URL for OpenRouter attribution. OPENROUTER_APP_NAME : Optional app name for OpenRouter attribution. ANTHROPIC_API_KEY : Anthropic API key. HUGGINGFACEHUB_API_TOKEN : HuggingFace access token (for private models or embeddings). Other services may require additional secrets; see the User Guide for details. Creating an Archi Deployment Create your deployment with the CLI: archi create --name my-archi --config examples/deployments/basic-gpu/config.yaml --podman --env-file .secrets.env --services chatbot --gpu-ids all This command specifies: --name : Deployment name. --config : Path to the configuration file. --podman : Use Podman for container management ( docker is the default). --env-file : Path to the secrets file. --services : Services to deploy (only the chatbot service in this example but others can be included separated by commas). Note that this command will create a deployment using only the link sources specified in the data_manager.sources.links.input_lists by default, if other sources (such as git-based documentation or pages under sso) want to be included they must be included using the --sources flag and in the configuration file. Example output archi create --name my-archi --config examples/deployments/basic-gpu/config.yaml --podman --env-file .secrets.env --services chatbot --gpu-ids all Starting Archi deployment process... [archi] Creating deployment 'my-archi' with services: chatbot [archi] Auto-enabling dependencies: postgres [archi] Configuration validated successfully [archi] You are using an embedding model from HuggingFace; make sure to include a HuggingFace token if required for usage, it won't be explicitly enforced [archi] Required secrets validated: PG_PASSWORD [archi] Volume 'archi-pg-my-archi' already exists. No action needed. [archi] Volume 'archi-my-archi' already exists. No action needed. [archi] Starting compose deployment from /path/to/my/.archi/archi-my-archi [archi] Using compose file: /path/to/my/.archi/archi-my-archi/compose.yaml [archi] (This might take a minute...) [archi] Deployment started successfully Archi deployment 'my-archi' created successfully! Services running: chatbot, postgres [archi] Chatbot: http://localhost:7861 The first deployment builds the container images from scratch (which may take a few minutes). Subsequent deployments reuse the images and complete much faster (roughly a minute). Tip: Having issues? Run the command with -v 4 to enable DEBUG-level logging. A note about multiple configurations When multiple configuration files are passed, their services sections must remain consistent, otherwise the deployment fails. The current use cases for multiple configurations include swapping pipelines/prompts dynamically via the chat app and maintaining separate benchmarking configurations. Verifying a deployment List running deployments with: archi list-deployments You should see output similar to: Existing deployments: (Additional details will follow for each deployment.)","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"Deploy your first instance of Archi and walk through the important concepts.","title":"Quickstart"},{"location":"quickstart/#sources-and-services","text":"Archi can ingest data from a variety of sources and supports several services . List them with the CLI command below and decide which ones you want to use so that we can configure them. archi list-services Example output: Available Archi services: Application Services: chatbot Interactive chat interface for users to communicate with the AI agent grafana Monitoring dashboard for system and LLM performance metrics uploader Admin interface for uploading and managing documents grader Automated grading service for assignments with web interface Integration Services: piazza Integration service for Piazza posts and Slack notifications mattermost Integration service for Mattermost channels redmine-mailer Email processing and Cleo/Redmine ticket management Data Sources: git Git repository scraping for MkDocs-based documentation jira Jira issue tracking integration redmine Redmine ticket integration sso SSO-backed web crawling See the User Guide for detailed information about each service and source.","title":"Sources and Services"},{"location":"quickstart/#pipelines","text":"Archi supports several pipelines\u2014pre-defined sequences of operations that process user inputs and generate responses. Each service supports a subset of pipelines (see the User Guide for details). An example pipeline is QAPipeline , a question-answering pipeline that takes a user's question, retrieves relevant documents from the vector store, and generates an answer using a language model. You specify which pipelines should be available in the configuration file.","title":"Pipelines"},{"location":"quickstart/#configuration","text":"Once you have chosen the services, sources, and pipelines you want to use, create a configuration file that specifies their settings. You can start from one of the example configuration files under examples/deployments/ , or create your own from scratch. This file sets parameters; the selected services and sources are determined at deployment time. Important: The configuration file follows the format of src/cli/templates/base-config.yaml . Any fields not specified in your configuration will be populated with the defaults from this template. Example configuration ( examples/deployments/basic-gpu/config.yaml ) for the chatbot service using QAPipeline with a local VLLM model: name: my_archi data_manager: sources: links: visible: true # include scraped pages in the chat citations input_lists: - examples/deployments/basic-gpu/miscellanea.list embedding_name: HuggingFaceEmbeddings chunk_size: 1000 archi: pipelines: - QAPipeline pipeline_map: QAPipeline: prompts: required: condense_prompt: examples/deployments/basic-gpu/condense.prompt chat_prompt: examples/deployments/basic-gpu/qa.prompt models: required: chat_model: VLLM condense_model: VLLM model_class_map: VLLM: kwargs: base_model: 'Qwen/Qwen2.5-7B-Instruct-1M' quantization: True max_model_len: 32768 tensor_parallel_size: 2 repetition_penalty: 1.0 gpu_memory_utilization: 0.5 services: chat_app: trained_on: \"My data\" hostname: \"<your-hostname>\" vectorstore: backend: postgres # Uses PostgreSQL with pgvector (default) Explanation of configuration parameters - `name`: Name of your Archi deployment. - `data_manager`: Settings related to data ingestion and the vector store. - `sources.links.input_lists`: Lists of URLs to seed the deployment. - `sources. .visible`: Controls whether content from a given source should be surfaced to end users (defaults to `true`). - `embedding_name`: Embedding model used for vectorization. - `chunk_size`: Controls how documents are split prior to embedding. - `archi`: Core pipeline settings. - `pipelines`: Pipelines to use (e.g., `QAPipeline`). - `pipeline_map`: Configuration for each pipeline, including prompts and models. - `model_class_map`: Mapping of model names to their classes and parameters. - `services`: Settings for individual services/interfaces. - `chat_app`: Chat interface configuration, including hostname and descriptive metadata. - `vectorstore.backend`: Vector store backend (`postgres` with pgvector).","title":"Configuration"},{"location":"quickstart/#secrets","text":"Secrets are sensitive values (passwords, API keys, etc.) that should not be stored directly in code or configuration files. Store them in a single .env file on your filesystem. Minimal deployments (chatbot with open-source LLM and embeddings) require: PG_PASSWORD : password used to secure the database. Create the secrets file with: echo \"PG_PASSWORD=my_strong_password\" > ~/.secrets.env If you are not using open-source models, supply the relevant API credentials: OPENAI_API_KEY : OpenAI API key. OPENROUTER_API_KEY : OpenRouter API key (for OpenRouterLLM ). OPENROUTER_SITE_URL : Optional site URL for OpenRouter attribution. OPENROUTER_APP_NAME : Optional app name for OpenRouter attribution. ANTHROPIC_API_KEY : Anthropic API key. HUGGINGFACEHUB_API_TOKEN : HuggingFace access token (for private models or embeddings). Other services may require additional secrets; see the User Guide for details.","title":"Secrets"},{"location":"quickstart/#creating-an-archi-deployment","text":"Create your deployment with the CLI: archi create --name my-archi --config examples/deployments/basic-gpu/config.yaml --podman --env-file .secrets.env --services chatbot --gpu-ids all This command specifies: --name : Deployment name. --config : Path to the configuration file. --podman : Use Podman for container management ( docker is the default). --env-file : Path to the secrets file. --services : Services to deploy (only the chatbot service in this example but others can be included separated by commas). Note that this command will create a deployment using only the link sources specified in the data_manager.sources.links.input_lists by default, if other sources (such as git-based documentation or pages under sso) want to be included they must be included using the --sources flag and in the configuration file. Example output archi create --name my-archi --config examples/deployments/basic-gpu/config.yaml --podman --env-file .secrets.env --services chatbot --gpu-ids all Starting Archi deployment process... [archi] Creating deployment 'my-archi' with services: chatbot [archi] Auto-enabling dependencies: postgres [archi] Configuration validated successfully [archi] You are using an embedding model from HuggingFace; make sure to include a HuggingFace token if required for usage, it won't be explicitly enforced [archi] Required secrets validated: PG_PASSWORD [archi] Volume 'archi-pg-my-archi' already exists. No action needed. [archi] Volume 'archi-my-archi' already exists. No action needed. [archi] Starting compose deployment from /path/to/my/.archi/archi-my-archi [archi] Using compose file: /path/to/my/.archi/archi-my-archi/compose.yaml [archi] (This might take a minute...) [archi] Deployment started successfully Archi deployment 'my-archi' created successfully! Services running: chatbot, postgres [archi] Chatbot: http://localhost:7861 The first deployment builds the container images from scratch (which may take a few minutes). Subsequent deployments reuse the images and complete much faster (roughly a minute). Tip: Having issues? Run the command with -v 4 to enable DEBUG-level logging.","title":"Creating an Archi Deployment"},{"location":"quickstart/#a-note-about-multiple-configurations","text":"When multiple configuration files are passed, their services sections must remain consistent, otherwise the deployment fails. The current use cases for multiple configurations include swapping pipelines/prompts dynamically via the chat app and maintaining separate benchmarking configurations.","title":"A note about multiple configurations"},{"location":"quickstart/#verifying-a-deployment","text":"List running deployments with: archi list-deployments You should see output similar to: Existing deployments: (Additional details will follow for each deployment.)","title":"Verifying a deployment"},{"location":"user_guide/","text":"User Guide Overview Archi supports various data sources as easy ways to ingest your data into the vector store databased used for document retrieval. These include: Links lists (even behind SSO) : automatically scrape and ingest documents from a list of URLs Git scraping : git mkdocs repositories Ticketing systems : JIRA, Redmine, Piazza Local documents Additionally, Archi supports various interfaces/services , which are applications that interact with the RAG system. These include: Chat interface : a web-based chat application Piazza integration : read posts from Piazza and post draft responses to a Slack channel Cleo/Redmine integration : read emails and create tickets in Redmine Mattermost integration : read posts from Mattermost and post draft responses to a Mattermost channel Grafana monitoring dashboard : monitor system and LLM performance metrics Document uploader : web interface for uploading and managing documents Grader : automated grading service for assignments with web interface Both data sources and interfaces/services are enabled via flags to the archi create command, archi create [...] --services=chatbot,piazza,... --sources jira,redmine,... The parameters of the services and sources are configured via the configuration file. See below for more details. We support various pipelines which are pre-defined sequences of operations that process user inputs and generate responses. Each service may support a given pipeline. See the Services and Pipelines sections below for more details. For each pipeline, you can use different models, retrievers, and prompts for different steps of the pipeline. We support various models for both embeddings and LLMs, which can be run locally or accessed via APIs. See the Models section below for more details. Both pipelines and models are configured via the configuration file. Finally, we support various retrievers and embedding techniques for document retrieval. These are configured via the configuration file. See the Vector Store section below for more details. Agent tools (search + retrieval) The chat agent can use a few built-in tools to locate evidence. These are internal capabilities of the chat service: Metadata search : find files by name/path/source metadata. Use free-text for partial matches, or exact filters with key:value . Example: mz_dilepton.py or relative_path:full/path/to/mz_dilepton.py . Content search (grep) : line-level search inside file contents; supports regex and context lines. Example: timeout error with before=2 and after=2 . Document fetch : pull full text for a specific file by hash (truncated with max_chars ). Vectorstore search : semantic retrieval of relevant passages when you don't know exact keywords. These tools are meant to be used together: search first, then fetch only the most relevant documents. Optional command line options In addition to the required --name , --config/--config-dir , --env-file , and --services arguments, the archi create command accepts several useful flags: --podman : Run the deployment with Podman instead of Docker. --sources / -src : Enable additional ingestion sources ( git , sso , jira , redmine , ...). Provide a comma-separated list. --gpu-ids : Mount specific GPUs ( --gpu-ids all or --gpu-ids 0,1 ). The legacy --gpu flag still works but maps to all . --tag : Override the local image tag (defaults to 2000 ). Handy when building multiple configurations side-by-side. --hostmode : Use host networking for all services. --verbosity / -v : Control CLI logging level (0 = quiet, 4 = debug). --force / --dry-run : Force recreation of an existing deployment and/or show what would happen without actually deploying. You can inspect the available services and sources, together with descriptions, using archi list-services . The CLI checks that host ports are free before deploying; if a port is already in use, adjust services.*.external_port (or services.*.port in --hostmode ) and retry. GPU helpers GPU access requires the NVIDIA drivers plus the NVIDIA Container Toolkit. After installing the toolkit, generate CDI entries (for Podman) with sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml and confirm with nvidia-ctk cdi list . Docker users should run sudo nvidia-ctk runtime configure --runtime=docker . Data Sources These are the different ways to ingest data into the vector store used for document retrieval. Web Link Lists A web link list is a simple text file containing a list of URLs, one per line. Archi will fetch the content from each URL and add it to the vector store, using the Scraper class. Configuration You can define which lists of links Archi will ingest in the configuration file as follows: data_manager: sources: links: input_lists: # REQUIRED - miscellanea.list # list of websites with relevant info - [...other lists...] Each list should be a simple text file containing one URL per line, e.g., https://example.com/page1 https://example.com/page2 [...] In the case that some of the links are behind a Single Sign-On (SSO) system, enable the SSO source in your configuration and specify the collector class: data_manager: sources: sso: enabled: true links: selenium_scraper: enabled: true selenium_class: CERNSSOScraper # or whichever class is appropriate selenium_class_map: CERNSSOScraper: kwargs: headless: true max_depth: 2 Then, run archi create ... --sources sso to activate the SSO collector. Note: source configuration is persisted to PostgreSQL static_config at deployment time and used at runtime. You can customise the HTTP scraper behaviour (for example, to avoid SSL verification warnings): data_manager: sources: links: scraper: reset_data: true verify_urls: false enable_warnings: false Secrets If you are using SSO, depending on the class, you may need to provide your login credentials in a secrets file as follows: SSO_USERNAME=username SSO_PASSWORD=password Then, make sure that the links you provide in the .list file(s) start with sso- , e.g., sso-https://example.com/protected/page Running Link scraping is automatically enabled in Archi, you don't need to add any arguments to the create command unless the links are sso protected. Git scraping In some cases, the RAG input may be documentations based on MKDocs git repositories. Instead of scraping these sites as regular HTML sites you can obtain the relevant content using the GitScraper class. Configuration To configure it, enable the git source in the configuration file: data_manager: sources: git: enabled: true In the input lists, make sure to prepend git- to the URL of the repositories you are interested in scraping. git-https://github.com/example/mkdocs/documentation.git Secrets You will need to provide a git username and token in the secrets file, GIT_USERNAME=your_username GIT_TOKEN=your_token Running Enable the git source during deployment with --sources git . JIRA The JIRA integration allows Archi to fetch issues and comments from specified JIRA projects and add them to the vector store, using the JiraClient class. Configuration Select which projects to scrape in the configuration file: data_manager: sources: jira: url: https://jira.example.com projects: - PROJECT_KEY anonymize_data: true cutoff_date: \"2023-01-01\" You can further customise anonymisation via the global anonymiser settings. data_manager: utils: anonymizer: nlp_model: en_core_web_sm excluded_words: - Example greeting_patterns: - '^(hi|hello|hey|greetings|dear)\\b' signoff_patterns: - '\\b(regards|sincerely|best regards|cheers|thank you)\\b' email_pattern: '[\\w\\.-]+@[\\w\\.-]+\\.\\w+' username_pattern: '\\[~[^\\]]+\\]' The anonymizer will remove names, emails, usernames, greetings, signoffs, and any other words you specify from the fetched data. This is useful if you want to avoid having personal information in the vector store. The optional cutoff_date can be used to skip tickets created before a specified ISO-8601 date. Secrets A personal access token (PAT) is required to authenticate and authorize with JIRA. Add JIRA_PAT=<token> to your .env file before deploying with --sources jira . Running Enable the source at deploy time with: archi create [...] --services=chatbot --sources jira Adding Documents and the Uploader Interface Adding Documents There are two main ways to add documents to Archi's vector database. They are: Manually adding files while the service is running via the uploader GUI Directly copying files into the container These methods are outlined below. Manual Uploader In order to upload documents while Archi is running via an easily accessible GUI, enable the uploader service when creating the deployment: archi create [...] --services=chatbot,uploader The exact port may vary based on configuration (default external port is 5003 ). A quick podman ps or docker ps will show which port is exposed. In order to access the manager, you must first create an admin account. Grab the container ID with podman ps / docker ps and then enter the container: docker exec -it <CONTAINER-ID> bash Run the bundled helper: python -u src/bin/service_create_account.py from the /root/Archi directory inside the container. This script will guide you through creating an account; never reuse sensitive passwords here. Once you have created an account, visit the outgoing port of the data manager docker service and then log in. The GUI will then allow you to upload documents while Archi is still running. Note that it may take a few minutes for all the documents to upload. Directly copying files to the container The documents used for RAG live in the chat container at /root/data/<directory>/<files> . Thus, in a pinch, you can docker/podman cp a file at this directory level, e.g., podman/docker cp myfile.pdf <container name or ID>:/root/data/<new_dir>/ . If you need to make a new directory in the container, you can do podman exec -it <container name or ID> mkdir /root/data/<new_dir> . Data Viewer The chat interface includes a built-in Data Viewer for browsing and managing ingested documents. Access it at /data on your chat app (e.g., http://localhost:7861/data ). Features: Browse documents : View all ingested documents with metadata (source, file type, chunk count) Search and filter : Filter documents by name or source type View content : Click on a document to see its full content and individual chunks Enable/disable documents : Toggle whether specific documents are included in RAG retrieval Bulk operations : Enable or disable multiple documents at once Document States: State Description Enabled Document chunks are included in retrieval (default) Disabled Document is excluded from retrieval but remains in the database Disabling documents is useful for: - Temporarily excluding outdated content - Testing retrieval with specific document subsets - Hiding sensitive documents from certain users Redmine Use the Redmine source to ingest solved tickets (question/answer pairs) into the vector store. Configuration data_manager: sources: redmine: url: https://redmine.example.com project: my-project anonymize_data: true Secrets Add the following to your .env file: REDMINE_USER=... REDMINE_PW=... Running Enable the source at deploy time with: archi create [...] --services=chatbot --sources redmine To automate email replies, also enable the redmine-mailer service (see the Services section below). Interfaces/Services These are the different apps that Archi supports, which allow you to interact with the AI pipelines. Piazza Interface Set up Archi to read posts from your Piazza forum and post draft responses to a specified Slack channel. To do this, a Piazza login (email and password) is required, plus the network ID of your Piazza channel, and lastly, a Webhook for the slack channel Archi will post to. See below for a step-by-step description of this. Go to https://api.slack.com/apps and sign in to workspace where you will eventually want Archi to post to (note doing this in a business workspace like the MIT one will require approval of the app/bot). Click 'Create New App', and then 'From scratch'. Name your app and again select the correct workspace. Then hit 'Create App' Now you have your app, and there are a few things to configure before you can launch Archi: Go to Incoming Webhooks under Features, and toggle it on. Click 'Add New Webhook', and select the channel you want Archi to post to. Now, copy the 'Webhook URL' and paste it into the secrets file, and handle it like any other secret! Configuration Beyond standard required configuration fields, the network ID of the Piazza channel is required (see below for an example config). You can get the network ID by simply navigating to the class homepage, and grabbing the sequence that follows 'https://piazza.com/class/'. For example, the 8.01 Fall 2024 homepage is: 'https://piazza.com/class/m0g3v0ahsqm2lg'. The network ID is thus 'm0g3v0ahsqm2lg'. Example minimal config for the Piazza interface: name: bare_minimum_configuration #REQUIRED data_manager: sources: links: input_lists: - class_info.list # class info links archi: [... archi config ...] services: piazza: network_id: <your Piazza network ID here> # REQUIRED chat_app: trained_on: \"Your class materials\" # REQUIRED Secrets The necessary secrets for deploying the Piazza service are the following: PIAZZA_EMAIL=... PIAZZA_PASSWORD=... SLACK_WEBHOOK=... The Slack webhook secret is described above. The Piazza email and password should be those of one of the class instructors. Remember to put this information in files named following what is written above. Running To run the Piazza service, simply add the piazza flag. For example: archi create [...] --services=chatbot,piazza Redmine/Mailbox Interface Archi will read all new tickets in a Redmine project, and draft a response as a comment to the ticket. Once the ticket is updated to the \"Resolved\" status by an admin, Archi will send the response as an email to the user who opened the ticket. The admin can modify Archi's response before sending it out. Configuration services: redmine_mailbox: url: https://redmine.example.com project: my-project redmine_update_time: 10 mailbox_update_time: 10 answer_tag: \"-- Archi -- Resolving email was sent\" Secrets Add the following secrets to your .env file: IMAP_USER=... IMAP_PW=... REDMINE_USER=... REDMINE_PW=... SENDER_SERVER=... SENDER_PORT=587 SENDER_REPLYTO=... SENDER_USER=... SENDER_PW=... Running archi create [...] --services=chatbot,redmine-mailer Mattermost Interface Set up Archi to read posts from your Mattermost forum and post draft responses to a specified Mattermost channel. Configuration services: mattermost: update_time: 60 Secrets You need to specify a webhook, access token, and channel identifiers: MATTERMOST_WEBHOOK=... MATTERMOST_PAK=... MATTERMOST_CHANNEL_ID_READ=... MATTERMOST_CHANNEL_ID_WRITE=... Running To run the Mattermost service, include it when selecting services. For example: archi create [...] --services=chatbot,mattermost Grafana Interface Monitor the performance of your Archi instance with the Grafana interface. This service provides a web-based dashboard to visualize various metrics related to system performance, LLM usage, and more. Note, if you are deploying a version of Archi you have already used (i.e., you haven't removed the images/volumes for a given --name ), the postgres will have already been created without the Grafana user created, and it will not work, so make sure to deploy a fresh instance. Configuration services: grafana: external_port: 3000 Secrets Grafana shares the Postgres database with other services, so you need both the database password and a Grafana-specific password: PG_PASSWORD=<your_database_password> GRAFANA_PG_PASSWORD=<grafana_db_password> Running Deploy Grafana alongside your other services: archi create [...] --services=chatbot,grafana and you should see something like this CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 87f1c7289d29 docker.io/library/postgres:17 postgres 9 minutes ago Up 9 minutes (healthy) 5432/tcp postgres-gtesting2 40130e8e23de docker.io/library/grafana-gtesting2:2000 9 minutes ago Up 9 minutes 0.0.0.0:3000->3000/tcp, 3000/tcp grafana-gtesting2 d6ce8a149439 localhost/chat-gtesting2:2000 python -u archi/... 9 minutes ago Up 9 minutes 0.0.0.0:7861->7861/tcp chat-gtesting2 where the grafana interface is accessible at your-hostname:3000 . To change the external port from 3000 , you can do this in the config at services.grafana.external_port . The default login and password are both \"admin\", which you will be prompted to change should you want to after first logging in. Navigate to the Archi dashboard from the home page by going to the menu > Dashboards > Archi > Archi Usage. Note, your-hostname here is the just name of the machine. Grafana uses its default configuration which is localhost but unlike the chat interface, there are no APIs where we template with a selected hostname, so the container networking handles this nicely. Pro tip: once at the web interface, for the \"Recent Conversation Messages (Clean Text + Link)\" panel, click the three little dots in the top right hand corner of the panel, click \"Edit\", and on the right, go to e.g., \"Override 4\" (should have Fields with name: clean text, also Override 7 for context column) and override property \"Cell options > Cell value inspect\". This will allow you to expand the text boxes with messages longer than can fit. Make sure you click apply to keep the changes. Pro tip 2: If you want to download all of the information from any panel as a CSV, go to the same three dots and click \"Inspect\", and you should see the option. Grader Interface Interface to launch a website which for a provided solution and rubric (and a couple of other things detailed below), will grade scanned images of a handwritten solution for the specified problem(s). Nota bene: this is not yet fully generalized and \"service\" ready, but instead for testing grading pipelines and a base off of which to build a potential grading app. Requirements To launch the service the following files are required: users.csv . This file is .csv file that contains two columns: \"MIT email\" and \"Unique code\", e.g.: MIT email,Unique code username@mit.edu,222 For now, the system requires the emails to be in the MIT domain, namely, contain \"@mit.edu\". TODO: make this an argument that is passed (e.g., school/email domain) solution_with_rubric_*.txt . These are .txt files that contain the problem solution followed by the rubric. The naming of the files should follow exactly, where the * is the problem number. There should be one of these files for every problem you want the app to be able to grade. The top of the file should be the problem name with a line of dashes (\"-\") below, e.g.: Anti-Helmholtz Coils --------------------------------------------------- These files should live in a directory which you will pass to the config, and Archi will handle the rest. admin_password.txt . This file will be passed as a secret and be the admin code to login in to the page where you can reset attempts for students. Secrets The only grading specific secret is the admin password, which like shown above, should be put in the following file ADMIN_PASSWORD=your_password Then it behaves like any other secret. Configuration The required fields in the configuration file are different from the rest of the Archi services. Below is an example: name: grading_test # REQUIRED archi: pipelines: - GradingPipeline pipeline_map: GradingPipeline: prompts: required: final_grade_prompt: final_grade.prompt models: required: final_grade_model: OllamaInterface ImageProcessingPipeline: prompts: required: image_processing_prompt: image_processing.prompt models: required: image_processing_model: OllamaInterface services: chat_app: trained_on: \"rubrics, class info, etc.\" # REQUIRED grader_app: num_problems: 1 # REQUIRED local_rubric_dir: ~/grading/my_rubrics # REQUIRED local_users_csv_dir: ~/grading/logins # REQUIRED data_manager: [...] name -- The name of your configuration (required). archi.pipelines -- List of pipelines to use (e.g., GradingPipeline , ImageProcessingPipeline ). archi.pipeline_map -- Mapping of pipelines to their required prompts and models. archi.pipeline_map.GradingPipeline.prompts.required.final_grade_prompt -- Path to the grading prompt file for evaluating student solutions. archi.pipeline_map.GradingPipeline.models.required.final_grade_model -- Model class for grading (e.g., OllamaInterface , HuggingFaceOpenLLM ). archi.pipeline_map.ImageProcessingPipeline.prompts.required.image_processing_prompt -- Path to the prompt file for image processing. archi.pipeline_map.ImageProcessingPipeline.models.required.image_processing_model -- Model class for image processing (e.g., OllamaInterface , HuggingFaceImageLLM ). services.chat_app.trained_on -- A brief description of the data or materials Archi is trained on (required). services.grader_app.num_problems -- Number of problems the grading service should expect (must match the number of rubric files). services.grader_app.local_rubric_dir -- Directory containing the solution_with_rubric_*.txt files. services.grader_app.local_users_csv_dir -- Directory containing the users.csv file. For ReAct-style agents (e.g., CMSCompOpsAgent ), you may optionally set archi.pipeline_map.<Agent>.recursion_limit (default 100 ) to control the LangGraph recursion cap; when the limit is hit, the agent returns a final wrap-up response using the collected context. Running archi create [...] --services=grader Models Models are either: Hosted locally, either via VLLM or HuggingFace transformers. Accessed via an API, e.g., OpenAI, Anthropic, etc. Accessed via an Ollama server instance. Local Models To use a local model, specify one of the local model classes in models.py : HuggingFaceOpenLLM HuggingFaceImageLLM VLLM Models via APIs We support the following model classes in models.py for models accessed via APIs: OpenAILLM OpenRouterLLM AnthropicLLM OpenRouter OpenRouter uses the OpenAI-compatible API. Configure it by setting OpenRouterLLM in your config and providing OPENROUTER_API_KEY . Optional attribution headers can be set via OPENROUTER_SITE_URL and OPENROUTER_APP_NAME . archi: model_class_map: OpenRouterLLM: class: OpenRouterLLM kwargs: model_name: openai/gpt-4o-mini temperature: 0.7 Ollama In order to use an Ollama server instance for the chatbot, it is possible to specify OllamaInterface for the model name. To then correctly use models on the Ollama server, in the keyword args, specify both the url of the server and the name of a model hosted on the server. archi: model_class_map: OllamaInterface: kwargs: base_model: \"gemma3\" # example url: \"url-for-server\" In this case, the gemma3 model is hosted on the Ollama server at url-for-server . You can check which models are hosted on your server by going to url-for-server/models . Bring Your Own Key (BYOK) Archi supports Bring Your Own Key (BYOK), allowing users to provide their own API keys for LLM providers at runtime. This enables: Cost attribution : Users pay for their own API usage Provider flexibility : Switch between providers without admin intervention Privacy : Use personal accounts for sensitive queries Key Hierarchy API keys are resolved in the following order (highest priority first): Environment Variables : Admin-configured keys (e.g., OPENAI_API_KEY ) Docker Secrets : Keys mounted at /run/secrets/ Session Storage : User-provided keys via the Settings UI !!! note Environment variable keys always take precedence. If an admin configures a key via environment variable, users cannot override it with their own key. Using BYOK in the Chat Interface Open the Settings modal (gear icon) Expand the API Keys section For each provider you want to use: Enter your API key in the input field Click Save to store it in your session Select your preferred Provider and Model from the dropdowns Start chatting! Status Indicators: Icon Meaning \u2713 Env Key configured via environment variable (cannot be changed) \u2713 Session Key configured via your session \u25cb No key configured Supported Providers Provider Environment Variable API Key Format OpenAI OPENAI_API_KEY sk-... Anthropic ANTHROPIC_API_KEY sk-ant-... Google Gemini GOOGLE_API_KEY AIza... OpenRouter OPENROUTER_API_KEY sk-or-... Security Considerations Keys are never logged - API keys are redacted from all log output Keys are never echoed - The UI only shows masked placeholders Session-scoped - Keys are cleared when you log out or your session expires HTTPS recommended - For production deployments, always use HTTPS to protect keys in transit API Endpoints For programmatic access, the following endpoints are available: Endpoint Method Description /api/providers/keys GET Get status of all provider keys /api/providers/keys/set POST Set a session API key (validates before storing) /api/providers/keys/clear POST Clear a session API key Vector Store The vector store is a database that stores document embeddings, enabling semantic and/or lexical search over your knowledge base. Archi uses PostgreSQL with pgvector as the default vector store backend to index and retrieve relevant documents based on similarity to user queries. Backend Selection Archi uses PostgreSQL with the pgvector extension as its vector store backend. This provides production-grade vector similarity search integrated with your existing PostgreSQL database. Configure vector store settings in your configuration file: services: vectorstore: backend: postgres # PostgreSQL with pgvector (only supported backend) Configuration Vector store settings are configured under the data_manager section: data_manager: collection_name: default_collection embedding_name: OpenAIEmbeddings chunk_size: 1000 chunk_overlap: 0 reset_collection: true num_documents_to_retrieve: 5 distance_metric: cosine Core Settings collection_name : Name of the vector store collection. Default: default_collection chunk_size : Maximum size of text chunks (in characters) when splitting documents. Default: 1000 chunk_overlap : Number of overlapping characters between consecutive chunks. Default: 0 reset_collection : If true , deletes and recreates the collection on startup. Default: true num_documents_to_retrieve : Number of relevant document chunks to retrieve for each query. Default: 5 Distance Metrics The distance_metric determines how similarity is calculated between embeddings: cosine : Cosine similarity (default) - measures the angle between vectors l2 : Euclidean distance - measures straight-line distance ip : Inner product - measures dot product similarity data_manager: distance_metric: cosine # Options: cosine, l2, ip Embedding Models Embeddings convert text into numerical vectors. Archi supports multiple embedding providers: OpenAI Embeddings data_manager: embedding_name: OpenAIEmbeddings embedding_class_map: OpenAIEmbeddings: class: OpenAIEmbeddings kwargs: model: text-embedding-3-small similarity_score_reference: 10 HuggingFace Embeddings data_manager: embedding_name: HuggingFaceEmbeddings embedding_class_map: HuggingFaceEmbeddings: class: HuggingFaceEmbeddings kwargs: model_name: sentence-transformers/all-MiniLM-L6-v2 model_kwargs: device: cpu encode_kwargs: normalize_embeddings: true similarity_score_reference: 10 query_embedding_instructions: null Supported Document Formats The vector store can process the following file types: Text files : .txt , .C Markdown : .md Python : .py HTML : .html PDF : .pdf Documents are automatically loaded with the appropriate parser based on file extension. Document Synchronization Archi automatically synchronizes your data directory with the vector store: Adding documents : New files in the data directory are automatically chunked, embedded, and added to the collection Removing documents : Files deleted from the data directory are removed from the collection Source tracking : Each ingested artifact is recorded in the Postgres catalog ( resources table) with its resource hash and relative file path Hybrid Search Combine semantic search with keyword-based BM25 search for improved retrieval: data_manager: use_hybrid_search: true bm25_weight: 0.6 semantic_weight: 0.4 use_hybrid_search : Enable hybrid search combining BM25 and semantic similarity. Default: true bm25_weight : Weight for BM25 keyword scores (base config default: 0.6 ). semantic_weight : Weight for semantic similarity scores (base config default: 0.4 ). BM25 tuning : Parameters like k1 and b are set when the PostgreSQL BM25 index is created and are no longer configurable via this file. Stemming By specifying the stemming option within your configuration, stemming functionality for the documents in Archi will be enabled. By doing so, documents inserted into the retrieval pipeline, as well as the query that is matched with them, will be stemmed and simplified for faster and more accurate lookup. data_manager: stemming: enabled: true When enabled, both documents and queries are processed using the Porter Stemmer algorithm to reduce words to their root forms (e.g., \"running\" \u2192 \"run\"), improving matching accuracy. PostgreSQL Backend (Default) Archi uses PostgreSQL with pgvector for vector storage by default. The PostgreSQL service is automatically started when you deploy with the chatbot service. services: postgres: host: postgres port: 5432 database: archi vectorstore: backend: postgres Required secrets for PostgreSQL: PG_PASSWORD=your_secure_password Benchmarking Archi has benchmarking functionality provided by the evaluate CLI command. We currently support two modes: SOURCES : given a user question and a list of correct sources, check if the retrieved documents contain any of the correct sources. RAGAS : use the Ragas RAG evaluator module to return numerical values judging by 4 of their provided metrics the quality of the answer: answer_relevancy , faithfulness , context precision , and context relevancy . Preparing the queries file Provide your list of questions, answers, and relevant sources in JSON format as follows: [ { \"question\": \"\", \"sources\": [...], \"answer\": \"\" // (optional) \"sources_match_field\": [...] }, ... ] Explanation of fields: - question : The question to be answered by the Archi instance. - sources : A list of sources (e.g., URLs, ticket IDs) that contain the answer. They are identified via the sources_match_field , which must be one of the metadata fields of the documents in your vector store. - answer : The expected answer to the question, used for evaluation. - sources_match_field (optional): A list of metadata fields to match the sources against (e.g., url , ticket_id ). If not provided, defaults to what is in the configuration file under data_manager:services:benchmarking:mode_settings:sources:default_match_field . Example: (see also examples/benchmarking/queries.json ) [ { \"question\": \"Does Jorian Benke work with the PPC and what topic will she work on?\", \"sources\": [\"https://ppc.mit.edu/blog/2025/07/14/welcome-our-first-ever-in-house-masters-student/\", \"CMSPROD-42\"], \"answer\": \"Yes, Jorian works with the PPC and her topic is the study of Lorentz invariance.\", \"source_match_field\": [\"url\", \"ticket_id\"] }, ... ] N.B.: one could also provide the url for the JIRA ticket: it is just a choice that you must make, and detail in source_match_field . i.e., the following will evaluate equivalently as the above example: [ { \"question\": \"Does Jorian Benke work with the PPC and what topic will she work on?\", \"sources\": [\"https://ppc.mit.edu/blog/2025/07/14/welcome-our-first-ever-in-house-masters-student/\", \"https://its.cern.ch/jira/browse/CMSPROD-42\"], \"answer\": \"Yes, Jorian works with the PPC and her topic is the study of Lorentz invariance.\", \"source_match_field\": [\"url\", \"url\"] }, ... ] Configuration You can evaluate one or more configurations by specifying the evaluate command with the -cd flag pointing to the directory containing your configuration file(s). You can also specify individual files with the -c flag. This can be useful if you're interested in comparing different hyperparameter settings. We support two modes, which you can specify in the configuration file under services:benchmarking:modes . You can choose either or both of RAGAS and SOURCES . The RAGAS mode will use the Ragas RAG evaluator module to return numerical values judging by 4 of their provided metrics: answer_relevancy , faithfulness , context precision , and context relevancy . More information about these metrics can be found on the Ragas website . The SOURCES mode will check if the retrieved documents contain any of the correct sources. The matching is done by comparing a given metadata field for any source. The default is file_name , as per the configuration file ( data_manager:services:benchmarking:mode_settings:sources:default_match_field ). You can override this on a per-query basis by specifying the sources_match_field field in the queries file, as described above. The configuration file should look like the following: services: benchmarking: queries_path: examples/benchmarking/queries.json out_dir: bench_out modes: - \"RAGAS\" - \"SOURCES\" mode_settings: sources: default_match_field: [\"file_name\"] # default field to match sources against, can be overridden in the queries file ragas_settings: provider: <provider name> # can be one of OpenAI, HuggingFace, Ollama, and Anthropic evaluation_model_settings: model_name: <model name> # ensure this lines up with the langchain API name for your chosen model and provider base_url: <url> # address to your running Ollama server should you have chosen the Ollama provider embedding_model: <embedding provider> # OpenAI or HuggingFace Finally, before you run the command ensure out_dir , the output directory, both exists on your system and that the path is correctly specified so that results can show up inside of it. Running To run the benchmarking script simply run the following: archi evaluate -n <name> -e <env_file> -cd <configs_directory> <optionally use -c <file1>,<file2>, ...> <OPTIONS> Example: archi evaluate -n benchmark -c examples/benchmarking/benchmark_configs/example_conf.yaml --gpu-ids all Additional options You might also want to adjust the timeout setting, which is the upper limit on how long the Ragas evaluation takes on a single QA pair, or the batch_size , which determines how many QA pairs to evaluate at once, which you might want to adjust, e.g., based on hardware constraints, as Ragas doesn't pay great attention to that. The corresponding configuration options are similarly set for the benchmarking services, as follows: services: benchmarking: timeout: <time in seconds> # default is 180 batch_size: <desired batch size> # no default setting, set by Ragas... Results The output of the benchmarking will be saved in the out_dir specified in the configuration file. The results will be saved in a timestamped subdirectory, e.g., bench_out/2042-10-01_12-00-00/ . To later examine your data, check out scripts/benchmarking/ , which contains some plotting functions and an ipynotebook with some basic usage examples. This is useful to play around with the results of the benchmarking, we will soon also have instead dedicated scripts to produce the plots of interest. Other Some useful additional features supported by the framework. Configuration Management Archi uses a three-tier configuration system that allows flexibility at different levels: Configuration Hierarchy Static Configuration (deploy-time, immutable) Set during deployment from config.yaml Includes: deployment name, embedding model, available pipelines/models Cannot be changed at runtime Dynamic Configuration (admin-controlled) Runtime-modifiable settings Includes: default model, temperature, retrieval parameters, active prompts Only admins can modify via API User Preferences (per-user overrides) Individual users can override certain settings Includes: preferred model, temperature, prompt selections Takes precedence over dynamic config for that user Effective Configuration When a request is made, Archi resolves the effective value for each setting: User Preference (if set) \u2192 Dynamic Config (admin default) \u2192 Static Default For example, if an admin sets temperature: 0.7 but a user prefers 0.5 , that user's requests will use 0.5 . Admin-Only Settings The following settings require admin privileges to modify: active_pipeline - Default pipeline active_model - Default model temperature , max_tokens , top_p , top_k - Generation parameters num_documents_to_retrieve - Retrieval settings active_condense_prompt , active_chat_prompt , active_system_prompt - Default prompts verbosity - Logging level User-Overridable Settings Users can personalize these settings via the preferences API: preferred_model - Model selection preferred_temperature - Generation temperature preferred_max_tokens - Max response length preferred_num_documents - Number of documents to retrieve preferred_condense_prompt , preferred_chat_prompt , preferred_system_prompt - Prompt selections theme - UI theme Prompt Customization Archi supports customizable prompts organized by type. Prompts are stored as files in your deployment directory for easy editing and version control. Prompt Location After deployment, prompts are located at: ~/.archi/<deployment-name>/data/prompts/ Default prompt templates are provided in the repository at examples/defaults/prompts/ for reference. Prompt Types condense : Prompts for condensing conversation history chat : Main response generation prompts system : System-level instructions Prompt Directory Structure data/prompts/ \u251c\u2500\u2500 condense/ \u2502 \u251c\u2500\u2500 default.prompt \u2502 \u2514\u2500\u2500 concise.prompt \u251c\u2500\u2500 chat/ \u2502 \u251c\u2500\u2500 default.prompt \u2502 \u251c\u2500\u2500 formal.prompt \u2502 \u2514\u2500\u2500 technical.prompt \u2514\u2500\u2500 system/ \u251c\u2500\u2500 default.prompt \u2514\u2500\u2500 helpful.prompt Creating Custom Prompts Navigate to your deployment's prompt directory: ~/.archi/<deployment-name>/data/prompts/ Create or edit a .prompt file in the appropriate subdirectory Use standard prompt template syntax with placeholders: {retriever_output} - Retrieved documents {question} - User question {history} - Conversation history Example data/prompts/chat/technical.prompt : You are a technical assistant specializing in software development. Use precise technical terminology and provide code examples when appropriate. Context: {context} Question: {question} Activating Prompts Admin (deployment-wide default): PATCH /api/config/dynamic {\"active_chat_prompt\": \"technical\"} User (personal preference): PATCH /api/users/me/preferences {\"preferred_chat_prompt\": \"formal\"} Reloading Prompts After adding or modifying prompt files, reload the cache: POST /api/prompts/reload This is admin-only and updates the in-memory prompt cache without restarting services. Admin Guide Becoming an Admin Admin status is set in the PostgreSQL users table: UPDATE users SET is_admin = true WHERE email = 'admin@example.com'; Admin Responsibilities Configuration Management Set deployment-wide defaults via /api/config/dynamic Monitor configuration changes via /api/config/audit Prompt Management Add/edit prompt files in prompts/ directory Reload prompts via API after changes User Management Grant admin privileges as needed Review user activity and preferences Audit Logging All admin configuration changes are logged: GET /api/config/audit?limit=50 Returns: { \"entries\": [ { \"user_id\": \"admin_user\", \"changed_at\": \"2025-01-20T15:30:00Z\", \"config_type\": \"dynamic\", \"field_name\": \"temperature\", \"old_value\": \"0.7\", \"new_value\": \"0.8\" } ] } Best Practices Test configuration changes in a staging environment first Document changes - use meaningful commit messages for prompt file updates Monitor audit log - review for unexpected changes Backup prompts - version control your custom prompts","title":"User Guide"},{"location":"user_guide/#user-guide","text":"","title":"User Guide"},{"location":"user_guide/#overview","text":"Archi supports various data sources as easy ways to ingest your data into the vector store databased used for document retrieval. These include: Links lists (even behind SSO) : automatically scrape and ingest documents from a list of URLs Git scraping : git mkdocs repositories Ticketing systems : JIRA, Redmine, Piazza Local documents Additionally, Archi supports various interfaces/services , which are applications that interact with the RAG system. These include: Chat interface : a web-based chat application Piazza integration : read posts from Piazza and post draft responses to a Slack channel Cleo/Redmine integration : read emails and create tickets in Redmine Mattermost integration : read posts from Mattermost and post draft responses to a Mattermost channel Grafana monitoring dashboard : monitor system and LLM performance metrics Document uploader : web interface for uploading and managing documents Grader : automated grading service for assignments with web interface Both data sources and interfaces/services are enabled via flags to the archi create command, archi create [...] --services=chatbot,piazza,... --sources jira,redmine,... The parameters of the services and sources are configured via the configuration file. See below for more details. We support various pipelines which are pre-defined sequences of operations that process user inputs and generate responses. Each service may support a given pipeline. See the Services and Pipelines sections below for more details. For each pipeline, you can use different models, retrievers, and prompts for different steps of the pipeline. We support various models for both embeddings and LLMs, which can be run locally or accessed via APIs. See the Models section below for more details. Both pipelines and models are configured via the configuration file. Finally, we support various retrievers and embedding techniques for document retrieval. These are configured via the configuration file. See the Vector Store section below for more details.","title":"Overview"},{"location":"user_guide/#agent-tools-search-retrieval","text":"The chat agent can use a few built-in tools to locate evidence. These are internal capabilities of the chat service: Metadata search : find files by name/path/source metadata. Use free-text for partial matches, or exact filters with key:value . Example: mz_dilepton.py or relative_path:full/path/to/mz_dilepton.py . Content search (grep) : line-level search inside file contents; supports regex and context lines. Example: timeout error with before=2 and after=2 . Document fetch : pull full text for a specific file by hash (truncated with max_chars ). Vectorstore search : semantic retrieval of relevant passages when you don't know exact keywords. These tools are meant to be used together: search first, then fetch only the most relevant documents.","title":"Agent tools (search + retrieval)"},{"location":"user_guide/#optional-command-line-options","text":"In addition to the required --name , --config/--config-dir , --env-file , and --services arguments, the archi create command accepts several useful flags: --podman : Run the deployment with Podman instead of Docker. --sources / -src : Enable additional ingestion sources ( git , sso , jira , redmine , ...). Provide a comma-separated list. --gpu-ids : Mount specific GPUs ( --gpu-ids all or --gpu-ids 0,1 ). The legacy --gpu flag still works but maps to all . --tag : Override the local image tag (defaults to 2000 ). Handy when building multiple configurations side-by-side. --hostmode : Use host networking for all services. --verbosity / -v : Control CLI logging level (0 = quiet, 4 = debug). --force / --dry-run : Force recreation of an existing deployment and/or show what would happen without actually deploying. You can inspect the available services and sources, together with descriptions, using archi list-services . The CLI checks that host ports are free before deploying; if a port is already in use, adjust services.*.external_port (or services.*.port in --hostmode ) and retry. GPU helpers GPU access requires the NVIDIA drivers plus the NVIDIA Container Toolkit. After installing the toolkit, generate CDI entries (for Podman) with sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml and confirm with nvidia-ctk cdi list . Docker users should run sudo nvidia-ctk runtime configure --runtime=docker .","title":"Optional command line options"},{"location":"user_guide/#data-sources","text":"These are the different ways to ingest data into the vector store used for document retrieval.","title":"Data Sources"},{"location":"user_guide/#web-link-lists","text":"A web link list is a simple text file containing a list of URLs, one per line. Archi will fetch the content from each URL and add it to the vector store, using the Scraper class.","title":"Web Link Lists"},{"location":"user_guide/#configuration","text":"You can define which lists of links Archi will ingest in the configuration file as follows: data_manager: sources: links: input_lists: # REQUIRED - miscellanea.list # list of websites with relevant info - [...other lists...] Each list should be a simple text file containing one URL per line, e.g., https://example.com/page1 https://example.com/page2 [...] In the case that some of the links are behind a Single Sign-On (SSO) system, enable the SSO source in your configuration and specify the collector class: data_manager: sources: sso: enabled: true links: selenium_scraper: enabled: true selenium_class: CERNSSOScraper # or whichever class is appropriate selenium_class_map: CERNSSOScraper: kwargs: headless: true max_depth: 2 Then, run archi create ... --sources sso to activate the SSO collector. Note: source configuration is persisted to PostgreSQL static_config at deployment time and used at runtime. You can customise the HTTP scraper behaviour (for example, to avoid SSL verification warnings): data_manager: sources: links: scraper: reset_data: true verify_urls: false enable_warnings: false","title":"Configuration"},{"location":"user_guide/#secrets","text":"If you are using SSO, depending on the class, you may need to provide your login credentials in a secrets file as follows: SSO_USERNAME=username SSO_PASSWORD=password Then, make sure that the links you provide in the .list file(s) start with sso- , e.g., sso-https://example.com/protected/page","title":"Secrets"},{"location":"user_guide/#running","text":"Link scraping is automatically enabled in Archi, you don't need to add any arguments to the create command unless the links are sso protected.","title":"Running"},{"location":"user_guide/#git-scraping","text":"In some cases, the RAG input may be documentations based on MKDocs git repositories. Instead of scraping these sites as regular HTML sites you can obtain the relevant content using the GitScraper class.","title":"Git scraping"},{"location":"user_guide/#configuration_1","text":"To configure it, enable the git source in the configuration file: data_manager: sources: git: enabled: true In the input lists, make sure to prepend git- to the URL of the repositories you are interested in scraping. git-https://github.com/example/mkdocs/documentation.git","title":"Configuration"},{"location":"user_guide/#secrets_1","text":"You will need to provide a git username and token in the secrets file, GIT_USERNAME=your_username GIT_TOKEN=your_token","title":"Secrets"},{"location":"user_guide/#running_1","text":"Enable the git source during deployment with --sources git .","title":"Running"},{"location":"user_guide/#jira","text":"The JIRA integration allows Archi to fetch issues and comments from specified JIRA projects and add them to the vector store, using the JiraClient class.","title":"JIRA"},{"location":"user_guide/#configuration_2","text":"Select which projects to scrape in the configuration file: data_manager: sources: jira: url: https://jira.example.com projects: - PROJECT_KEY anonymize_data: true cutoff_date: \"2023-01-01\" You can further customise anonymisation via the global anonymiser settings. data_manager: utils: anonymizer: nlp_model: en_core_web_sm excluded_words: - Example greeting_patterns: - '^(hi|hello|hey|greetings|dear)\\b' signoff_patterns: - '\\b(regards|sincerely|best regards|cheers|thank you)\\b' email_pattern: '[\\w\\.-]+@[\\w\\.-]+\\.\\w+' username_pattern: '\\[~[^\\]]+\\]' The anonymizer will remove names, emails, usernames, greetings, signoffs, and any other words you specify from the fetched data. This is useful if you want to avoid having personal information in the vector store. The optional cutoff_date can be used to skip tickets created before a specified ISO-8601 date.","title":"Configuration"},{"location":"user_guide/#secrets_2","text":"A personal access token (PAT) is required to authenticate and authorize with JIRA. Add JIRA_PAT=<token> to your .env file before deploying with --sources jira .","title":"Secrets"},{"location":"user_guide/#running_2","text":"Enable the source at deploy time with: archi create [...] --services=chatbot --sources jira","title":"Running"},{"location":"user_guide/#adding-documents-and-the-uploader-interface","text":"","title":"Adding Documents and the Uploader Interface"},{"location":"user_guide/#adding-documents","text":"There are two main ways to add documents to Archi's vector database. They are: Manually adding files while the service is running via the uploader GUI Directly copying files into the container These methods are outlined below.","title":"Adding Documents"},{"location":"user_guide/#manual-uploader","text":"In order to upload documents while Archi is running via an easily accessible GUI, enable the uploader service when creating the deployment: archi create [...] --services=chatbot,uploader The exact port may vary based on configuration (default external port is 5003 ). A quick podman ps or docker ps will show which port is exposed. In order to access the manager, you must first create an admin account. Grab the container ID with podman ps / docker ps and then enter the container: docker exec -it <CONTAINER-ID> bash Run the bundled helper: python -u src/bin/service_create_account.py from the /root/Archi directory inside the container. This script will guide you through creating an account; never reuse sensitive passwords here. Once you have created an account, visit the outgoing port of the data manager docker service and then log in. The GUI will then allow you to upload documents while Archi is still running. Note that it may take a few minutes for all the documents to upload.","title":"Manual Uploader"},{"location":"user_guide/#directly-copying-files-to-the-container","text":"The documents used for RAG live in the chat container at /root/data/<directory>/<files> . Thus, in a pinch, you can docker/podman cp a file at this directory level, e.g., podman/docker cp myfile.pdf <container name or ID>:/root/data/<new_dir>/ . If you need to make a new directory in the container, you can do podman exec -it <container name or ID> mkdir /root/data/<new_dir> .","title":"Directly copying files to the container"},{"location":"user_guide/#data-viewer","text":"The chat interface includes a built-in Data Viewer for browsing and managing ingested documents. Access it at /data on your chat app (e.g., http://localhost:7861/data ). Features: Browse documents : View all ingested documents with metadata (source, file type, chunk count) Search and filter : Filter documents by name or source type View content : Click on a document to see its full content and individual chunks Enable/disable documents : Toggle whether specific documents are included in RAG retrieval Bulk operations : Enable or disable multiple documents at once Document States: State Description Enabled Document chunks are included in retrieval (default) Disabled Document is excluded from retrieval but remains in the database Disabling documents is useful for: - Temporarily excluding outdated content - Testing retrieval with specific document subsets - Hiding sensitive documents from certain users","title":"Data Viewer"},{"location":"user_guide/#redmine","text":"Use the Redmine source to ingest solved tickets (question/answer pairs) into the vector store.","title":"Redmine"},{"location":"user_guide/#configuration_3","text":"data_manager: sources: redmine: url: https://redmine.example.com project: my-project anonymize_data: true","title":"Configuration"},{"location":"user_guide/#secrets_3","text":"Add the following to your .env file: REDMINE_USER=... REDMINE_PW=...","title":"Secrets"},{"location":"user_guide/#running_3","text":"Enable the source at deploy time with: archi create [...] --services=chatbot --sources redmine To automate email replies, also enable the redmine-mailer service (see the Services section below).","title":"Running"},{"location":"user_guide/#interfacesservices","text":"These are the different apps that Archi supports, which allow you to interact with the AI pipelines.","title":"Interfaces/Services"},{"location":"user_guide/#piazza-interface","text":"Set up Archi to read posts from your Piazza forum and post draft responses to a specified Slack channel. To do this, a Piazza login (email and password) is required, plus the network ID of your Piazza channel, and lastly, a Webhook for the slack channel Archi will post to. See below for a step-by-step description of this. Go to https://api.slack.com/apps and sign in to workspace where you will eventually want Archi to post to (note doing this in a business workspace like the MIT one will require approval of the app/bot). Click 'Create New App', and then 'From scratch'. Name your app and again select the correct workspace. Then hit 'Create App' Now you have your app, and there are a few things to configure before you can launch Archi: Go to Incoming Webhooks under Features, and toggle it on. Click 'Add New Webhook', and select the channel you want Archi to post to. Now, copy the 'Webhook URL' and paste it into the secrets file, and handle it like any other secret!","title":"Piazza Interface"},{"location":"user_guide/#configuration_4","text":"Beyond standard required configuration fields, the network ID of the Piazza channel is required (see below for an example config). You can get the network ID by simply navigating to the class homepage, and grabbing the sequence that follows 'https://piazza.com/class/'. For example, the 8.01 Fall 2024 homepage is: 'https://piazza.com/class/m0g3v0ahsqm2lg'. The network ID is thus 'm0g3v0ahsqm2lg'. Example minimal config for the Piazza interface: name: bare_minimum_configuration #REQUIRED data_manager: sources: links: input_lists: - class_info.list # class info links archi: [... archi config ...] services: piazza: network_id: <your Piazza network ID here> # REQUIRED chat_app: trained_on: \"Your class materials\" # REQUIRED","title":"Configuration"},{"location":"user_guide/#secrets_4","text":"The necessary secrets for deploying the Piazza service are the following: PIAZZA_EMAIL=... PIAZZA_PASSWORD=... SLACK_WEBHOOK=... The Slack webhook secret is described above. The Piazza email and password should be those of one of the class instructors. Remember to put this information in files named following what is written above.","title":"Secrets"},{"location":"user_guide/#running_4","text":"To run the Piazza service, simply add the piazza flag. For example: archi create [...] --services=chatbot,piazza","title":"Running"},{"location":"user_guide/#redminemailbox-interface","text":"Archi will read all new tickets in a Redmine project, and draft a response as a comment to the ticket. Once the ticket is updated to the \"Resolved\" status by an admin, Archi will send the response as an email to the user who opened the ticket. The admin can modify Archi's response before sending it out.","title":"Redmine/Mailbox Interface"},{"location":"user_guide/#configuration_5","text":"services: redmine_mailbox: url: https://redmine.example.com project: my-project redmine_update_time: 10 mailbox_update_time: 10 answer_tag: \"-- Archi -- Resolving email was sent\"","title":"Configuration"},{"location":"user_guide/#secrets_5","text":"Add the following secrets to your .env file: IMAP_USER=... IMAP_PW=... REDMINE_USER=... REDMINE_PW=... SENDER_SERVER=... SENDER_PORT=587 SENDER_REPLYTO=... SENDER_USER=... SENDER_PW=...","title":"Secrets"},{"location":"user_guide/#running_5","text":"archi create [...] --services=chatbot,redmine-mailer","title":"Running"},{"location":"user_guide/#mattermost-interface","text":"Set up Archi to read posts from your Mattermost forum and post draft responses to a specified Mattermost channel.","title":"Mattermost Interface"},{"location":"user_guide/#configuration_6","text":"services: mattermost: update_time: 60","title":"Configuration"},{"location":"user_guide/#secrets_6","text":"You need to specify a webhook, access token, and channel identifiers: MATTERMOST_WEBHOOK=... MATTERMOST_PAK=... MATTERMOST_CHANNEL_ID_READ=... MATTERMOST_CHANNEL_ID_WRITE=...","title":"Secrets"},{"location":"user_guide/#running_6","text":"To run the Mattermost service, include it when selecting services. For example: archi create [...] --services=chatbot,mattermost","title":"Running"},{"location":"user_guide/#grafana-interface","text":"Monitor the performance of your Archi instance with the Grafana interface. This service provides a web-based dashboard to visualize various metrics related to system performance, LLM usage, and more. Note, if you are deploying a version of Archi you have already used (i.e., you haven't removed the images/volumes for a given --name ), the postgres will have already been created without the Grafana user created, and it will not work, so make sure to deploy a fresh instance.","title":"Grafana Interface"},{"location":"user_guide/#configuration_7","text":"services: grafana: external_port: 3000","title":"Configuration"},{"location":"user_guide/#secrets_7","text":"Grafana shares the Postgres database with other services, so you need both the database password and a Grafana-specific password: PG_PASSWORD=<your_database_password> GRAFANA_PG_PASSWORD=<grafana_db_password>","title":"Secrets"},{"location":"user_guide/#running_7","text":"Deploy Grafana alongside your other services: archi create [...] --services=chatbot,grafana and you should see something like this CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 87f1c7289d29 docker.io/library/postgres:17 postgres 9 minutes ago Up 9 minutes (healthy) 5432/tcp postgres-gtesting2 40130e8e23de docker.io/library/grafana-gtesting2:2000 9 minutes ago Up 9 minutes 0.0.0.0:3000->3000/tcp, 3000/tcp grafana-gtesting2 d6ce8a149439 localhost/chat-gtesting2:2000 python -u archi/... 9 minutes ago Up 9 minutes 0.0.0.0:7861->7861/tcp chat-gtesting2 where the grafana interface is accessible at your-hostname:3000 . To change the external port from 3000 , you can do this in the config at services.grafana.external_port . The default login and password are both \"admin\", which you will be prompted to change should you want to after first logging in. Navigate to the Archi dashboard from the home page by going to the menu > Dashboards > Archi > Archi Usage. Note, your-hostname here is the just name of the machine. Grafana uses its default configuration which is localhost but unlike the chat interface, there are no APIs where we template with a selected hostname, so the container networking handles this nicely. Pro tip: once at the web interface, for the \"Recent Conversation Messages (Clean Text + Link)\" panel, click the three little dots in the top right hand corner of the panel, click \"Edit\", and on the right, go to e.g., \"Override 4\" (should have Fields with name: clean text, also Override 7 for context column) and override property \"Cell options > Cell value inspect\". This will allow you to expand the text boxes with messages longer than can fit. Make sure you click apply to keep the changes. Pro tip 2: If you want to download all of the information from any panel as a CSV, go to the same three dots and click \"Inspect\", and you should see the option.","title":"Running"},{"location":"user_guide/#grader-interface","text":"Interface to launch a website which for a provided solution and rubric (and a couple of other things detailed below), will grade scanned images of a handwritten solution for the specified problem(s). Nota bene: this is not yet fully generalized and \"service\" ready, but instead for testing grading pipelines and a base off of which to build a potential grading app.","title":"Grader Interface"},{"location":"user_guide/#requirements","text":"To launch the service the following files are required: users.csv . This file is .csv file that contains two columns: \"MIT email\" and \"Unique code\", e.g.: MIT email,Unique code username@mit.edu,222 For now, the system requires the emails to be in the MIT domain, namely, contain \"@mit.edu\". TODO: make this an argument that is passed (e.g., school/email domain) solution_with_rubric_*.txt . These are .txt files that contain the problem solution followed by the rubric. The naming of the files should follow exactly, where the * is the problem number. There should be one of these files for every problem you want the app to be able to grade. The top of the file should be the problem name with a line of dashes (\"-\") below, e.g.: Anti-Helmholtz Coils --------------------------------------------------- These files should live in a directory which you will pass to the config, and Archi will handle the rest. admin_password.txt . This file will be passed as a secret and be the admin code to login in to the page where you can reset attempts for students.","title":"Requirements"},{"location":"user_guide/#secrets_8","text":"The only grading specific secret is the admin password, which like shown above, should be put in the following file ADMIN_PASSWORD=your_password Then it behaves like any other secret.","title":"Secrets"},{"location":"user_guide/#configuration_8","text":"The required fields in the configuration file are different from the rest of the Archi services. Below is an example: name: grading_test # REQUIRED archi: pipelines: - GradingPipeline pipeline_map: GradingPipeline: prompts: required: final_grade_prompt: final_grade.prompt models: required: final_grade_model: OllamaInterface ImageProcessingPipeline: prompts: required: image_processing_prompt: image_processing.prompt models: required: image_processing_model: OllamaInterface services: chat_app: trained_on: \"rubrics, class info, etc.\" # REQUIRED grader_app: num_problems: 1 # REQUIRED local_rubric_dir: ~/grading/my_rubrics # REQUIRED local_users_csv_dir: ~/grading/logins # REQUIRED data_manager: [...] name -- The name of your configuration (required). archi.pipelines -- List of pipelines to use (e.g., GradingPipeline , ImageProcessingPipeline ). archi.pipeline_map -- Mapping of pipelines to their required prompts and models. archi.pipeline_map.GradingPipeline.prompts.required.final_grade_prompt -- Path to the grading prompt file for evaluating student solutions. archi.pipeline_map.GradingPipeline.models.required.final_grade_model -- Model class for grading (e.g., OllamaInterface , HuggingFaceOpenLLM ). archi.pipeline_map.ImageProcessingPipeline.prompts.required.image_processing_prompt -- Path to the prompt file for image processing. archi.pipeline_map.ImageProcessingPipeline.models.required.image_processing_model -- Model class for image processing (e.g., OllamaInterface , HuggingFaceImageLLM ). services.chat_app.trained_on -- A brief description of the data or materials Archi is trained on (required). services.grader_app.num_problems -- Number of problems the grading service should expect (must match the number of rubric files). services.grader_app.local_rubric_dir -- Directory containing the solution_with_rubric_*.txt files. services.grader_app.local_users_csv_dir -- Directory containing the users.csv file. For ReAct-style agents (e.g., CMSCompOpsAgent ), you may optionally set archi.pipeline_map.<Agent>.recursion_limit (default 100 ) to control the LangGraph recursion cap; when the limit is hit, the agent returns a final wrap-up response using the collected context.","title":"Configuration"},{"location":"user_guide/#running_8","text":"archi create [...] --services=grader","title":"Running"},{"location":"user_guide/#models","text":"Models are either: Hosted locally, either via VLLM or HuggingFace transformers. Accessed via an API, e.g., OpenAI, Anthropic, etc. Accessed via an Ollama server instance.","title":"Models"},{"location":"user_guide/#local-models","text":"To use a local model, specify one of the local model classes in models.py : HuggingFaceOpenLLM HuggingFaceImageLLM VLLM","title":"Local Models"},{"location":"user_guide/#models-via-apis","text":"We support the following model classes in models.py for models accessed via APIs: OpenAILLM OpenRouterLLM AnthropicLLM","title":"Models via APIs"},{"location":"user_guide/#openrouter","text":"OpenRouter uses the OpenAI-compatible API. Configure it by setting OpenRouterLLM in your config and providing OPENROUTER_API_KEY . Optional attribution headers can be set via OPENROUTER_SITE_URL and OPENROUTER_APP_NAME . archi: model_class_map: OpenRouterLLM: class: OpenRouterLLM kwargs: model_name: openai/gpt-4o-mini temperature: 0.7","title":"OpenRouter"},{"location":"user_guide/#ollama","text":"In order to use an Ollama server instance for the chatbot, it is possible to specify OllamaInterface for the model name. To then correctly use models on the Ollama server, in the keyword args, specify both the url of the server and the name of a model hosted on the server. archi: model_class_map: OllamaInterface: kwargs: base_model: \"gemma3\" # example url: \"url-for-server\" In this case, the gemma3 model is hosted on the Ollama server at url-for-server . You can check which models are hosted on your server by going to url-for-server/models .","title":"Ollama"},{"location":"user_guide/#bring-your-own-key-byok","text":"Archi supports Bring Your Own Key (BYOK), allowing users to provide their own API keys for LLM providers at runtime. This enables: Cost attribution : Users pay for their own API usage Provider flexibility : Switch between providers without admin intervention Privacy : Use personal accounts for sensitive queries","title":"Bring Your Own Key (BYOK)"},{"location":"user_guide/#key-hierarchy","text":"API keys are resolved in the following order (highest priority first): Environment Variables : Admin-configured keys (e.g., OPENAI_API_KEY ) Docker Secrets : Keys mounted at /run/secrets/ Session Storage : User-provided keys via the Settings UI !!! note Environment variable keys always take precedence. If an admin configures a key via environment variable, users cannot override it with their own key.","title":"Key Hierarchy"},{"location":"user_guide/#using-byok-in-the-chat-interface","text":"Open the Settings modal (gear icon) Expand the API Keys section For each provider you want to use: Enter your API key in the input field Click Save to store it in your session Select your preferred Provider and Model from the dropdowns Start chatting! Status Indicators: Icon Meaning \u2713 Env Key configured via environment variable (cannot be changed) \u2713 Session Key configured via your session \u25cb No key configured","title":"Using BYOK in the Chat Interface"},{"location":"user_guide/#supported-providers","text":"Provider Environment Variable API Key Format OpenAI OPENAI_API_KEY sk-... Anthropic ANTHROPIC_API_KEY sk-ant-... Google Gemini GOOGLE_API_KEY AIza... OpenRouter OPENROUTER_API_KEY sk-or-...","title":"Supported Providers"},{"location":"user_guide/#security-considerations","text":"Keys are never logged - API keys are redacted from all log output Keys are never echoed - The UI only shows masked placeholders Session-scoped - Keys are cleared when you log out or your session expires HTTPS recommended - For production deployments, always use HTTPS to protect keys in transit","title":"Security Considerations"},{"location":"user_guide/#api-endpoints","text":"For programmatic access, the following endpoints are available: Endpoint Method Description /api/providers/keys GET Get status of all provider keys /api/providers/keys/set POST Set a session API key (validates before storing) /api/providers/keys/clear POST Clear a session API key","title":"API Endpoints"},{"location":"user_guide/#vector-store","text":"The vector store is a database that stores document embeddings, enabling semantic and/or lexical search over your knowledge base. Archi uses PostgreSQL with pgvector as the default vector store backend to index and retrieve relevant documents based on similarity to user queries.","title":"Vector Store"},{"location":"user_guide/#backend-selection","text":"Archi uses PostgreSQL with the pgvector extension as its vector store backend. This provides production-grade vector similarity search integrated with your existing PostgreSQL database. Configure vector store settings in your configuration file: services: vectorstore: backend: postgres # PostgreSQL with pgvector (only supported backend)","title":"Backend Selection"},{"location":"user_guide/#configuration_9","text":"Vector store settings are configured under the data_manager section: data_manager: collection_name: default_collection embedding_name: OpenAIEmbeddings chunk_size: 1000 chunk_overlap: 0 reset_collection: true num_documents_to_retrieve: 5 distance_metric: cosine","title":"Configuration"},{"location":"user_guide/#core-settings","text":"collection_name : Name of the vector store collection. Default: default_collection chunk_size : Maximum size of text chunks (in characters) when splitting documents. Default: 1000 chunk_overlap : Number of overlapping characters between consecutive chunks. Default: 0 reset_collection : If true , deletes and recreates the collection on startup. Default: true num_documents_to_retrieve : Number of relevant document chunks to retrieve for each query. Default: 5","title":"Core Settings"},{"location":"user_guide/#distance-metrics","text":"The distance_metric determines how similarity is calculated between embeddings: cosine : Cosine similarity (default) - measures the angle between vectors l2 : Euclidean distance - measures straight-line distance ip : Inner product - measures dot product similarity data_manager: distance_metric: cosine # Options: cosine, l2, ip","title":"Distance Metrics"},{"location":"user_guide/#embedding-models","text":"Embeddings convert text into numerical vectors. Archi supports multiple embedding providers:","title":"Embedding Models"},{"location":"user_guide/#openai-embeddings","text":"data_manager: embedding_name: OpenAIEmbeddings embedding_class_map: OpenAIEmbeddings: class: OpenAIEmbeddings kwargs: model: text-embedding-3-small similarity_score_reference: 10","title":"OpenAI Embeddings"},{"location":"user_guide/#huggingface-embeddings","text":"data_manager: embedding_name: HuggingFaceEmbeddings embedding_class_map: HuggingFaceEmbeddings: class: HuggingFaceEmbeddings kwargs: model_name: sentence-transformers/all-MiniLM-L6-v2 model_kwargs: device: cpu encode_kwargs: normalize_embeddings: true similarity_score_reference: 10 query_embedding_instructions: null","title":"HuggingFace Embeddings"},{"location":"user_guide/#supported-document-formats","text":"The vector store can process the following file types: Text files : .txt , .C Markdown : .md Python : .py HTML : .html PDF : .pdf Documents are automatically loaded with the appropriate parser based on file extension.","title":"Supported Document Formats"},{"location":"user_guide/#document-synchronization","text":"Archi automatically synchronizes your data directory with the vector store: Adding documents : New files in the data directory are automatically chunked, embedded, and added to the collection Removing documents : Files deleted from the data directory are removed from the collection Source tracking : Each ingested artifact is recorded in the Postgres catalog ( resources table) with its resource hash and relative file path","title":"Document Synchronization"},{"location":"user_guide/#hybrid-search","text":"Combine semantic search with keyword-based BM25 search for improved retrieval: data_manager: use_hybrid_search: true bm25_weight: 0.6 semantic_weight: 0.4 use_hybrid_search : Enable hybrid search combining BM25 and semantic similarity. Default: true bm25_weight : Weight for BM25 keyword scores (base config default: 0.6 ). semantic_weight : Weight for semantic similarity scores (base config default: 0.4 ). BM25 tuning : Parameters like k1 and b are set when the PostgreSQL BM25 index is created and are no longer configurable via this file.","title":"Hybrid Search"},{"location":"user_guide/#stemming","text":"By specifying the stemming option within your configuration, stemming functionality for the documents in Archi will be enabled. By doing so, documents inserted into the retrieval pipeline, as well as the query that is matched with them, will be stemmed and simplified for faster and more accurate lookup. data_manager: stemming: enabled: true When enabled, both documents and queries are processed using the Porter Stemmer algorithm to reduce words to their root forms (e.g., \"running\" \u2192 \"run\"), improving matching accuracy.","title":"Stemming"},{"location":"user_guide/#postgresql-backend-default","text":"Archi uses PostgreSQL with pgvector for vector storage by default. The PostgreSQL service is automatically started when you deploy with the chatbot service. services: postgres: host: postgres port: 5432 database: archi vectorstore: backend: postgres Required secrets for PostgreSQL: PG_PASSWORD=your_secure_password","title":"PostgreSQL Backend (Default)"},{"location":"user_guide/#benchmarking","text":"Archi has benchmarking functionality provided by the evaluate CLI command. We currently support two modes: SOURCES : given a user question and a list of correct sources, check if the retrieved documents contain any of the correct sources. RAGAS : use the Ragas RAG evaluator module to return numerical values judging by 4 of their provided metrics the quality of the answer: answer_relevancy , faithfulness , context precision , and context relevancy .","title":"Benchmarking"},{"location":"user_guide/#preparing-the-queries-file","text":"Provide your list of questions, answers, and relevant sources in JSON format as follows: [ { \"question\": \"\", \"sources\": [...], \"answer\": \"\" // (optional) \"sources_match_field\": [...] }, ... ] Explanation of fields: - question : The question to be answered by the Archi instance. - sources : A list of sources (e.g., URLs, ticket IDs) that contain the answer. They are identified via the sources_match_field , which must be one of the metadata fields of the documents in your vector store. - answer : The expected answer to the question, used for evaluation. - sources_match_field (optional): A list of metadata fields to match the sources against (e.g., url , ticket_id ). If not provided, defaults to what is in the configuration file under data_manager:services:benchmarking:mode_settings:sources:default_match_field . Example: (see also examples/benchmarking/queries.json ) [ { \"question\": \"Does Jorian Benke work with the PPC and what topic will she work on?\", \"sources\": [\"https://ppc.mit.edu/blog/2025/07/14/welcome-our-first-ever-in-house-masters-student/\", \"CMSPROD-42\"], \"answer\": \"Yes, Jorian works with the PPC and her topic is the study of Lorentz invariance.\", \"source_match_field\": [\"url\", \"ticket_id\"] }, ... ] N.B.: one could also provide the url for the JIRA ticket: it is just a choice that you must make, and detail in source_match_field . i.e., the following will evaluate equivalently as the above example: [ { \"question\": \"Does Jorian Benke work with the PPC and what topic will she work on?\", \"sources\": [\"https://ppc.mit.edu/blog/2025/07/14/welcome-our-first-ever-in-house-masters-student/\", \"https://its.cern.ch/jira/browse/CMSPROD-42\"], \"answer\": \"Yes, Jorian works with the PPC and her topic is the study of Lorentz invariance.\", \"source_match_field\": [\"url\", \"url\"] }, ... ]","title":"Preparing the queries file"},{"location":"user_guide/#configuration_10","text":"You can evaluate one or more configurations by specifying the evaluate command with the -cd flag pointing to the directory containing your configuration file(s). You can also specify individual files with the -c flag. This can be useful if you're interested in comparing different hyperparameter settings. We support two modes, which you can specify in the configuration file under services:benchmarking:modes . You can choose either or both of RAGAS and SOURCES . The RAGAS mode will use the Ragas RAG evaluator module to return numerical values judging by 4 of their provided metrics: answer_relevancy , faithfulness , context precision , and context relevancy . More information about these metrics can be found on the Ragas website . The SOURCES mode will check if the retrieved documents contain any of the correct sources. The matching is done by comparing a given metadata field for any source. The default is file_name , as per the configuration file ( data_manager:services:benchmarking:mode_settings:sources:default_match_field ). You can override this on a per-query basis by specifying the sources_match_field field in the queries file, as described above. The configuration file should look like the following: services: benchmarking: queries_path: examples/benchmarking/queries.json out_dir: bench_out modes: - \"RAGAS\" - \"SOURCES\" mode_settings: sources: default_match_field: [\"file_name\"] # default field to match sources against, can be overridden in the queries file ragas_settings: provider: <provider name> # can be one of OpenAI, HuggingFace, Ollama, and Anthropic evaluation_model_settings: model_name: <model name> # ensure this lines up with the langchain API name for your chosen model and provider base_url: <url> # address to your running Ollama server should you have chosen the Ollama provider embedding_model: <embedding provider> # OpenAI or HuggingFace Finally, before you run the command ensure out_dir , the output directory, both exists on your system and that the path is correctly specified so that results can show up inside of it.","title":"Configuration"},{"location":"user_guide/#running_9","text":"To run the benchmarking script simply run the following: archi evaluate -n <name> -e <env_file> -cd <configs_directory> <optionally use -c <file1>,<file2>, ...> <OPTIONS> Example: archi evaluate -n benchmark -c examples/benchmarking/benchmark_configs/example_conf.yaml --gpu-ids all","title":"Running"},{"location":"user_guide/#additional-options","text":"You might also want to adjust the timeout setting, which is the upper limit on how long the Ragas evaluation takes on a single QA pair, or the batch_size , which determines how many QA pairs to evaluate at once, which you might want to adjust, e.g., based on hardware constraints, as Ragas doesn't pay great attention to that. The corresponding configuration options are similarly set for the benchmarking services, as follows: services: benchmarking: timeout: <time in seconds> # default is 180 batch_size: <desired batch size> # no default setting, set by Ragas...","title":"Additional options"},{"location":"user_guide/#results","text":"The output of the benchmarking will be saved in the out_dir specified in the configuration file. The results will be saved in a timestamped subdirectory, e.g., bench_out/2042-10-01_12-00-00/ . To later examine your data, check out scripts/benchmarking/ , which contains some plotting functions and an ipynotebook with some basic usage examples. This is useful to play around with the results of the benchmarking, we will soon also have instead dedicated scripts to produce the plots of interest.","title":"Results"},{"location":"user_guide/#other","text":"Some useful additional features supported by the framework.","title":"Other"},{"location":"user_guide/#configuration-management","text":"Archi uses a three-tier configuration system that allows flexibility at different levels:","title":"Configuration Management"},{"location":"user_guide/#configuration-hierarchy","text":"Static Configuration (deploy-time, immutable) Set during deployment from config.yaml Includes: deployment name, embedding model, available pipelines/models Cannot be changed at runtime Dynamic Configuration (admin-controlled) Runtime-modifiable settings Includes: default model, temperature, retrieval parameters, active prompts Only admins can modify via API User Preferences (per-user overrides) Individual users can override certain settings Includes: preferred model, temperature, prompt selections Takes precedence over dynamic config for that user","title":"Configuration Hierarchy"},{"location":"user_guide/#effective-configuration","text":"When a request is made, Archi resolves the effective value for each setting: User Preference (if set) \u2192 Dynamic Config (admin default) \u2192 Static Default For example, if an admin sets temperature: 0.7 but a user prefers 0.5 , that user's requests will use 0.5 .","title":"Effective Configuration"},{"location":"user_guide/#admin-only-settings","text":"The following settings require admin privileges to modify: active_pipeline - Default pipeline active_model - Default model temperature , max_tokens , top_p , top_k - Generation parameters num_documents_to_retrieve - Retrieval settings active_condense_prompt , active_chat_prompt , active_system_prompt - Default prompts verbosity - Logging level","title":"Admin-Only Settings"},{"location":"user_guide/#user-overridable-settings","text":"Users can personalize these settings via the preferences API: preferred_model - Model selection preferred_temperature - Generation temperature preferred_max_tokens - Max response length preferred_num_documents - Number of documents to retrieve preferred_condense_prompt , preferred_chat_prompt , preferred_system_prompt - Prompt selections theme - UI theme","title":"User-Overridable Settings"},{"location":"user_guide/#prompt-customization","text":"Archi supports customizable prompts organized by type. Prompts are stored as files in your deployment directory for easy editing and version control.","title":"Prompt Customization"},{"location":"user_guide/#prompt-location","text":"After deployment, prompts are located at: ~/.archi/<deployment-name>/data/prompts/ Default prompt templates are provided in the repository at examples/defaults/prompts/ for reference.","title":"Prompt Location"},{"location":"user_guide/#prompt-types","text":"condense : Prompts for condensing conversation history chat : Main response generation prompts system : System-level instructions","title":"Prompt Types"},{"location":"user_guide/#prompt-directory-structure","text":"data/prompts/ \u251c\u2500\u2500 condense/ \u2502 \u251c\u2500\u2500 default.prompt \u2502 \u2514\u2500\u2500 concise.prompt \u251c\u2500\u2500 chat/ \u2502 \u251c\u2500\u2500 default.prompt \u2502 \u251c\u2500\u2500 formal.prompt \u2502 \u2514\u2500\u2500 technical.prompt \u2514\u2500\u2500 system/ \u251c\u2500\u2500 default.prompt \u2514\u2500\u2500 helpful.prompt","title":"Prompt Directory Structure"},{"location":"user_guide/#creating-custom-prompts","text":"Navigate to your deployment's prompt directory: ~/.archi/<deployment-name>/data/prompts/ Create or edit a .prompt file in the appropriate subdirectory Use standard prompt template syntax with placeholders: {retriever_output} - Retrieved documents {question} - User question {history} - Conversation history Example data/prompts/chat/technical.prompt : You are a technical assistant specializing in software development. Use precise technical terminology and provide code examples when appropriate. Context: {context} Question: {question}","title":"Creating Custom Prompts"},{"location":"user_guide/#activating-prompts","text":"Admin (deployment-wide default): PATCH /api/config/dynamic {\"active_chat_prompt\": \"technical\"} User (personal preference): PATCH /api/users/me/preferences {\"preferred_chat_prompt\": \"formal\"}","title":"Activating Prompts"},{"location":"user_guide/#reloading-prompts","text":"After adding or modifying prompt files, reload the cache: POST /api/prompts/reload This is admin-only and updates the in-memory prompt cache without restarting services.","title":"Reloading Prompts"},{"location":"user_guide/#admin-guide","text":"","title":"Admin Guide"},{"location":"user_guide/#becoming-an-admin","text":"Admin status is set in the PostgreSQL users table: UPDATE users SET is_admin = true WHERE email = 'admin@example.com';","title":"Becoming an Admin"},{"location":"user_guide/#admin-responsibilities","text":"Configuration Management Set deployment-wide defaults via /api/config/dynamic Monitor configuration changes via /api/config/audit Prompt Management Add/edit prompt files in prompts/ directory Reload prompts via API after changes User Management Grant admin privileges as needed Review user activity and preferences","title":"Admin Responsibilities"},{"location":"user_guide/#audit-logging","text":"All admin configuration changes are logged: GET /api/config/audit?limit=50 Returns: { \"entries\": [ { \"user_id\": \"admin_user\", \"changed_at\": \"2025-01-20T15:30:00Z\", \"config_type\": \"dynamic\", \"field_name\": \"temperature\", \"old_value\": \"0.7\", \"new_value\": \"0.8\" } ] }","title":"Audit Logging"},{"location":"user_guide/#best-practices","text":"Test configuration changes in a staging environment first Document changes - use meaningful commit messages for prompt file updates Monitor audit log - review for unexpected changes Backup prompts - version control your custom prompts","title":"Best Practices"}]}