{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Archi Archi (AI Augmented Research Chat Intelligence) is a retrieval-augmented generation (RAG) framework designed to be a low-barrier, open-source, private, and customizable AI solution for research and educational support. Archi makes it easy to deploy AI assistants with a suite of tools that connect to knowledge bases such as web links, files, JIRA tickets, and documentation communication platforms such as Piazza, Slack, Mattermost, and email It is modular and extensible, allowing users to add connectors and customize pipeline behavior for a wide range of tasks\u2014from answering simple questions to delivering detailed explanations. Start Here If you are new to Archi, follow this path: Install Quickstart User Guide About Archi is developed by Prof. Paus (MIT Physics), Prof. Kraska (MIT EECS), and their students. It has already been successfully deployed as a user chatbot and technical assistant at SubMIT (the MIT Physics Department's computing cluster) and as an educational assistant for several MIT courses, including 8.01 and 8.511. What sets Archi apart is that it is fully open source, configurable across foundational models and LLM libraries, and designed for private deployment. Under the hood, Archi is a highly configurable RAG system tailored for educational and scientific support. Given its success, the scope now spans additional MIT classes, CERN, Harvard, and internal deployments such as CSAIL's support staff. Research Resource Support Archi serves technical support teams and end users. At SubMIT, it functions both as a user-facing chatbot and as a ticket assistant. Integration with Redmine enables Archi to prepare draft responses to support tickets that staff can review before sending. In both roles, Archi accesses the corpus of tickets and documentation, citing relevant sources in its answers. Educational Support Archi assists TAs, lecturers, and support staff\u2014or students directly\u2014by preparing answers based on curated class resources. In MIT course deployments, Archi leverages Piazza posts, documentation, and other class-specific materials. The Piazza integration can draft answers for staff to review or send, while the system continuously learns from revisions and new posts, improving over time. Documentation Section Description Install System requirements and installation Quickstart Deploy your first instance in minutes User Guide Overview of all capabilities Data Sources Configure web links, git, JIRA, Redmine, and more Services Chat, uploader, data manager, Piazza, Mattermost, and other interfaces Models & Providers LLM providers (OpenAI, Anthropic, Gemini, OpenRouter, Local), embeddings, BYOK Agents & Tools Agent specs, tools, MCP integration, pipelines Configuration Full YAML config schema reference CLI Reference All CLI commands and options API Reference REST API endpoints Benchmarking Evaluate retrieval and response quality Developer Guide Architecture, contributing, extending the stack Advanced Setup GPU setup, multi-node, production deployment Troubleshooting Common issues and fixes","title":"Home"},{"location":"#archi","text":"Archi (AI Augmented Research Chat Intelligence) is a retrieval-augmented generation (RAG) framework designed to be a low-barrier, open-source, private, and customizable AI solution for research and educational support. Archi makes it easy to deploy AI assistants with a suite of tools that connect to knowledge bases such as web links, files, JIRA tickets, and documentation communication platforms such as Piazza, Slack, Mattermost, and email It is modular and extensible, allowing users to add connectors and customize pipeline behavior for a wide range of tasks\u2014from answering simple questions to delivering detailed explanations.","title":"Archi"},{"location":"#start-here","text":"If you are new to Archi, follow this path: Install Quickstart User Guide","title":"Start Here"},{"location":"#about","text":"Archi is developed by Prof. Paus (MIT Physics), Prof. Kraska (MIT EECS), and their students. It has already been successfully deployed as a user chatbot and technical assistant at SubMIT (the MIT Physics Department's computing cluster) and as an educational assistant for several MIT courses, including 8.01 and 8.511. What sets Archi apart is that it is fully open source, configurable across foundational models and LLM libraries, and designed for private deployment. Under the hood, Archi is a highly configurable RAG system tailored for educational and scientific support. Given its success, the scope now spans additional MIT classes, CERN, Harvard, and internal deployments such as CSAIL's support staff.","title":"About"},{"location":"#research-resource-support","text":"Archi serves technical support teams and end users. At SubMIT, it functions both as a user-facing chatbot and as a ticket assistant. Integration with Redmine enables Archi to prepare draft responses to support tickets that staff can review before sending. In both roles, Archi accesses the corpus of tickets and documentation, citing relevant sources in its answers.","title":"Research Resource Support"},{"location":"#educational-support","text":"Archi assists TAs, lecturers, and support staff\u2014or students directly\u2014by preparing answers based on curated class resources. In MIT course deployments, Archi leverages Piazza posts, documentation, and other class-specific materials. The Piazza integration can draft answers for staff to review or send, while the system continuously learns from revisions and new posts, improving over time.","title":"Educational Support"},{"location":"#documentation","text":"Section Description Install System requirements and installation Quickstart Deploy your first instance in minutes User Guide Overview of all capabilities Data Sources Configure web links, git, JIRA, Redmine, and more Services Chat, uploader, data manager, Piazza, Mattermost, and other interfaces Models & Providers LLM providers (OpenAI, Anthropic, Gemini, OpenRouter, Local), embeddings, BYOK Agents & Tools Agent specs, tools, MCP integration, pipelines Configuration Full YAML config schema reference CLI Reference All CLI commands and options API Reference REST API endpoints Benchmarking Evaluate retrieval and response quality Developer Guide Architecture, contributing, extending the stack Advanced Setup GPU setup, multi-node, production deployment Troubleshooting Common issues and fixes","title":"Documentation"},{"location":"advanced_setup_deploy/","text":"Advanced Setup & Deployment Topics related to advanced setup and deployment of Archi. Configuring Podman To ensure your Podman containers stay running for extended periods, you need to enable lingering. To do this, run: loginctl enable-linger To check or confirm the lingering status, run: loginctl user-status | grep -m1 Linger See the Red Hat documentation for additional context. Running LLMs locally on your GPUs There are a few additional system requirements for this to work: Make sure you have NVIDIA drivers installed. (Optional) For the containers where Archi will run to access the GPUs, install the NVIDIA container toolkit . Configure the container runtime to access the GPUs. For Podman Run the following command: sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml Then list the devices: nvidia-ctk cdi list You should see output similar to: INFO[0000] Found 9 CDI devices ... nvidia.com/gpu=0 nvidia.com/gpu=1 ... nvidia.com/gpu=all ... These listed \"CDI devices\" will be referenced to run Archi on the GPUs, so make sure they are present. To learn more, consult the [Podman GPU documentation](https://podman-desktop.io/docs/podman/gpu). For Docker Run the following command: sudo nvidia-ctk runtime configure --runtime=docker The remaining steps mirror the Podman flow. NOTE: this has not yet been fully tested with Docker. Refer to the [NVIDIA toolkit documentation](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#configuration) for details. Once these requirements are met, the archi create [...] --gpu-ids <gpus> option will deploy Archi across your GPUs. Helpful Notes for Production Deployments You may wish to use the CLI in order to stage production deployments. This section covers some useful notes to keep in mind. Running multiple deployments on the same machine The CLI allows multiple deployments to run on the same daemon in the case of Docker (Podman has no daemon). The container networks between all the deployments are separate, so there is very little risk of them accidentally communicating with one another. However, you need to be careful with the external ports. Suppose you're running two deployments and both of them are running the chat on external port 8000. There is no way to view both deployments at the same time from the same port, so instead you should forward the deployments to other external ports. Generally, this can be done in the configuration: services: chat_app: external_port: 7862 # default is 7861 uploader_app: external_port: 5004 # default is 5003 grafana: external_port: 3001 # default is 3000 postgres: port: 5432 # default is 5432 Persisting data between deployments Volumes persist between deployments, so if you deploy an instance and upload additional documents, you do not need to redo this every time you deploy. If you are editing any data, explicitly remove this information from the volume, or remove the volume itself with: docker/podman volume rm <volume-name> To see what volumes are currently present, run: docker/podman volume ls HTTPS Configuration for Production For production deployments, especially when using BYOK (Bring Your Own Key), HTTPS is strongly recommended to protect API keys in transit. Using a Reverse Proxy The recommended approach is to terminate TLS at a reverse proxy (nginx, Caddy, Traefik): Example nginx configuration: server { listen 443 ssl; server_name your-domain.com; ssl_certificate /path/to/cert.pem; ssl_certificate_key /path/to/key.pem; location / { proxy_pass http://localhost:7861; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; # SSE streaming support proxy_buffering off; proxy_cache off; } } Example Caddy configuration (automatic HTTPS): your-domain.com { reverse_proxy localhost:7861 } Session Cookie Security When running behind HTTPS, enable secure cookies by setting the environment variable: FLASK_SESSION_COOKIE_SECURE=true Or configure in your deployment's environment file: FLASK_SESSION_COOKIE_SECURE=true This ensures session cookies (which may contain API keys) are only sent over encrypted connections. Security Checklist [ ] TLS termination at reverse proxy or load balancer [ ] FLASK_SESSION_COOKIE_SECURE=true in production [ ] Strong FLASK_UPLOADER_APP_SECRET_KEY configured (not auto-generated) [ ] Firewall rules limiting direct access to internal ports [ ] Regular certificate renewal (use Let's Encrypt/certbot)","title":"Advanced Setup"},{"location":"advanced_setup_deploy/#advanced-setup-deployment","text":"Topics related to advanced setup and deployment of Archi.","title":"Advanced Setup &amp; Deployment"},{"location":"advanced_setup_deploy/#configuring-podman","text":"To ensure your Podman containers stay running for extended periods, you need to enable lingering. To do this, run: loginctl enable-linger To check or confirm the lingering status, run: loginctl user-status | grep -m1 Linger See the Red Hat documentation for additional context.","title":"Configuring Podman"},{"location":"advanced_setup_deploy/#running-llms-locally-on-your-gpus","text":"There are a few additional system requirements for this to work: Make sure you have NVIDIA drivers installed. (Optional) For the containers where Archi will run to access the GPUs, install the NVIDIA container toolkit . Configure the container runtime to access the GPUs. For Podman Run the following command: sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml Then list the devices: nvidia-ctk cdi list You should see output similar to: INFO[0000] Found 9 CDI devices ... nvidia.com/gpu=0 nvidia.com/gpu=1 ... nvidia.com/gpu=all ... These listed \"CDI devices\" will be referenced to run Archi on the GPUs, so make sure they are present. To learn more, consult the [Podman GPU documentation](https://podman-desktop.io/docs/podman/gpu). For Docker Run the following command: sudo nvidia-ctk runtime configure --runtime=docker The remaining steps mirror the Podman flow. NOTE: this has not yet been fully tested with Docker. Refer to the [NVIDIA toolkit documentation](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#configuration) for details. Once these requirements are met, the archi create [...] --gpu-ids <gpus> option will deploy Archi across your GPUs.","title":"Running LLMs locally on your GPUs"},{"location":"advanced_setup_deploy/#helpful-notes-for-production-deployments","text":"You may wish to use the CLI in order to stage production deployments. This section covers some useful notes to keep in mind.","title":"Helpful Notes for Production Deployments"},{"location":"advanced_setup_deploy/#running-multiple-deployments-on-the-same-machine","text":"The CLI allows multiple deployments to run on the same daemon in the case of Docker (Podman has no daemon). The container networks between all the deployments are separate, so there is very little risk of them accidentally communicating with one another. However, you need to be careful with the external ports. Suppose you're running two deployments and both of them are running the chat on external port 8000. There is no way to view both deployments at the same time from the same port, so instead you should forward the deployments to other external ports. Generally, this can be done in the configuration: services: chat_app: external_port: 7862 # default is 7861 uploader_app: external_port: 5004 # default is 5003 grafana: external_port: 3001 # default is 3000 postgres: port: 5432 # default is 5432","title":"Running multiple deployments on the same machine"},{"location":"advanced_setup_deploy/#persisting-data-between-deployments","text":"Volumes persist between deployments, so if you deploy an instance and upload additional documents, you do not need to redo this every time you deploy. If you are editing any data, explicitly remove this information from the volume, or remove the volume itself with: docker/podman volume rm <volume-name> To see what volumes are currently present, run: docker/podman volume ls","title":"Persisting data between deployments"},{"location":"advanced_setup_deploy/#https-configuration-for-production","text":"For production deployments, especially when using BYOK (Bring Your Own Key), HTTPS is strongly recommended to protect API keys in transit.","title":"HTTPS Configuration for Production"},{"location":"advanced_setup_deploy/#using-a-reverse-proxy","text":"The recommended approach is to terminate TLS at a reverse proxy (nginx, Caddy, Traefik): Example nginx configuration: server { listen 443 ssl; server_name your-domain.com; ssl_certificate /path/to/cert.pem; ssl_certificate_key /path/to/key.pem; location / { proxy_pass http://localhost:7861; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; # SSE streaming support proxy_buffering off; proxy_cache off; } } Example Caddy configuration (automatic HTTPS): your-domain.com { reverse_proxy localhost:7861 }","title":"Using a Reverse Proxy"},{"location":"advanced_setup_deploy/#session-cookie-security","text":"When running behind HTTPS, enable secure cookies by setting the environment variable: FLASK_SESSION_COOKIE_SECURE=true Or configure in your deployment's environment file: FLASK_SESSION_COOKIE_SECURE=true This ensures session cookies (which may contain API keys) are only sent over encrypted connections.","title":"Session Cookie Security"},{"location":"advanced_setup_deploy/#security-checklist","text":"[ ] TLS termination at reverse proxy or load balancer [ ] FLASK_SESSION_COOKIE_SECURE=true in production [ ] Strong FLASK_UPLOADER_APP_SECRET_KEY configured (not auto-generated) [ ] Firewall rules limiting direct access to internal ports [ ] Regular certificate renewal (use Let's Encrypt/certbot)","title":"Security Checklist"},{"location":"agents_tools/","text":"Agents & Tools Archi uses an agent-based architecture where AI assistants are defined by agent specs \u2014 Markdown files that specify the agent's name, available tools, and system prompt. Agent Specs Agent specs live in the directory configured at services.chat_app.agents_dir . Each *.md file defines one agent. Format An agent spec is a Markdown file with YAML frontmatter: --- name: CMS Comp Ops tools: - search_local_files - search_metadata_index - list_metadata_schema - fetch_catalog_document - search_vectorstore_hybrid - mcp --- You are the CMS Comp Ops assistant. You help with operational questions, troubleshooting, and documentation lookups. Use tools when needed, cite evidence from retrieved sources, and keep responses concise and actionable. Required fields: name (string): Display name for the agent in the UI dropdown. tools (list of strings): Tools this agent can use \u2014 a subset of tools defined by the agent class. Prompt body: Everything after the frontmatter is the system prompt. Practical Agent Spec Examples Minimal default agent --- name: CMS CompOps Default tools: - search_local_files - search_metadata_index - list_metadata_schema - fetch_catalog_document - search_vectorstore_hybrid --- You are a CMS CompOps assistant. Rules: - Use tools to find evidence before answering. - Prefer concise, operationally actionable responses. - Cite relevant files or tickets when possible. - If evidence is missing, say so and suggest the next query/tool call. MCP-enabled agent --- name: CMS CompOps + MCP tools: - search_vectorstore_hybrid - fetch_catalog_document - mcp --- You are a research-focused assistant. Use vectorstore tools for internal docs first. Use MCP tools for external/system checks when internal evidence is insufficient. Always distinguish internal evidence from MCP-derived evidence. MONIT-focused agent (if MONIT tools are enabled by the agent class) --- name: CMS CompOps MONIT tools: - search_vectorstore_hybrid - fetch_catalog_document - monit_opensearch_search - monit_opensearch_aggregation --- You support CompOps incident triage. When a request is about rates, failures, or timeseries: 1. Query MONIT tools. 2. Correlate with internal docs. 3. Return likely cause, confidence, and next checks. These files should live in services.chat_app.agents_dir and be selected by services.chat_app.agent_class . File Discovery The service loads all *.md files from the agents directory. The first agent in lexicographic order is selected as the default. Users can switch agents via the dropdown in the chat UI header. Available Tools Agent classes define a tool registry \u2014 a mapping of tool names to tool builders. Each agent spec selects a subset of available tools via the tools list. The default agent class ( CMSCompOpsAgent ) provides these tools: search_local_files Line-level search inside file contents. Supports regex and configurable context lines. Use for: Finding specific text, code patterns, or error messages Example query: timeout error with before=2 and after=2 search_metadata_index Search the document catalog by file name, path, or source metadata. Use free-text for partial matches or key:value for exact filters. Use for: Finding files by name, path, or source Example query: mz_dilepton.py or relative_path:full/path/to/file.py list_metadata_schema Returns the metadata schema and available filter keys. Helps the agent understand what metadata fields exist. fetch_catalog_document Pull the full text of a specific file by its hash. Supports truncation with max_chars . Use for: Reading a specific document after finding it via search search_vectorstore_hybrid Semantic and keyword (BM25) hybrid retrieval of relevant passages from the vector store. Use for: Answering questions when you don't know exact keywords mcp Enables Model Context Protocol (MCP) tools from external servers. See MCP Integration below. Agent Management in the Chat UI The chat interface provides a full agent management experience: Agent dropdown : Switch between available agents using the dropdown in the header Create agents : Click the \"+\" button to create a new agent with name, tools, and prompt Edit agents : Click the pencil icon on any agent to modify its configuration Delete agents : Remove agents you no longer need (with confirmation) Changes are persisted and take effect on the next request. Pipelines Archi supports several pipeline classes. The active pipeline is configured per service via the agent_class key: services: chat_app: agent_class: CMSCompOpsAgent CMSCompOpsAgent The default pipeline. A ReAct agent with tool-use capabilities \u2014 it can search documents, fetch content, and query the vector store. Supports MCP tools and streaming responses. QAPipeline A simpler question-answering pipeline for straightforward retrieval-augmented generation without tool use. Pipeline Configuration Pipeline selection is configured per service: services: chat_app: agent_class: CMSCompOpsAgent agents_dir: examples/agents The class ( CMSCompOpsAgent , QAPipeline , etc.) defines available tools and runtime behavior. The selected agent spec file defines the active prompt and tool subset. MCP Integration Archi supports the Model Context Protocol (MCP) for connecting agents to external tool servers. This allows agents to use tools defined by third-party MCP servers \u2014 such as web search, code execution, or custom APIs \u2014 alongside Archi's built-in tools. How It Works MCP servers are defined in your deployment runtime configuration An agent spec includes mcp in its tools list to opt in At agent initialization, Archi connects to the MCP servers and discovers available tools The MCP tools are added to the agent's toolset alongside built-in tools Configuration Define MCP servers in your deployment configuration: mcp_servers: my_server: transport: \"stdio\" command: \"uvx\" args: - \"mcp-server-example\" web_search: transport: \"sse\" url: \"http://localhost:8080/sse\" Each server entry follows the format expected by the langchain-mcp-adapters library: transport : Communication method \u2014 \"stdio\" (subprocess) or \"sse\" (HTTP Server-Sent Events) command / args : For stdio transport, the command to launch the server url : For sse transport, the server endpoint Agent Spec Example Include mcp in the tools list to enable MCP tools for an agent: --- name: Research Assistant tools: - search_vectorstore_hybrid - fetch_catalog_document - mcp --- You are a research assistant with access to external tools via MCP. Use vectorstore search for internal documents and MCP tools for external information retrieval. Runtime Behavior MCP sessions are maintained in a background event loop for the lifetime of the service Each MCP tool is wrapped for synchronous execution so it integrates seamlessly with the ReAct agent loop Tool names from MCP servers are namespaced to avoid conflicts with built-in tools Vector Store & Retrieval The vector store powers document retrieval in Archi. It uses PostgreSQL with pgvector for production-grade vector similarity search. Core Settings data_manager: collection_name: default_collection embedding_name: OpenAIEmbeddings chunk_size: 1000 chunk_overlap: 0 reset_collection: true distance_metric: cosine retrievers: hybrid_retriever: num_documents_to_retrieve: 5 bm25_weight: 0.6 semantic_weight: 0.4 Setting Description Default collection_name Name of the vector store collection default_collection chunk_size Maximum characters per text chunk 1000 chunk_overlap Overlapping characters between chunks 0 reset_collection Wipe and recreate collection on startup true retrievers.hybrid_retriever.num_documents_to_retrieve Top-k documents per query 5 distance_metric Similarity metric: cosine , l2 , or ip cosine Hybrid Search Hybrid search (semantic + BM25 keyword retrieval) is enabled by default as a dynamic runtime setting. The weights are configurable in your YAML: data_manager: retrievers: hybrid_retriever: bm25_weight: 0.6 semantic_weight: 0.4 Stemming Enable stemming to reduce words to root forms for improved matching: data_manager: stemming: enabled: true Supported Document Formats .txt , .md , .py , .c , .C , .h , .sh , .html , .htm , .pdf , .json , .yaml , .yml , .csv , .tsv , .log , .rst , .php Document Synchronization Archi automatically synchronizes documents with the vector store: New files are chunked, embedded, and indexed Deleted files are removed from the collection All artifacts are tracked in the PostgreSQL resources catalog","title":"Agents & Tools"},{"location":"agents_tools/#agents-tools","text":"Archi uses an agent-based architecture where AI assistants are defined by agent specs \u2014 Markdown files that specify the agent's name, available tools, and system prompt.","title":"Agents &amp; Tools"},{"location":"agents_tools/#agent-specs","text":"Agent specs live in the directory configured at services.chat_app.agents_dir . Each *.md file defines one agent.","title":"Agent Specs"},{"location":"agents_tools/#format","text":"An agent spec is a Markdown file with YAML frontmatter: --- name: CMS Comp Ops tools: - search_local_files - search_metadata_index - list_metadata_schema - fetch_catalog_document - search_vectorstore_hybrid - mcp --- You are the CMS Comp Ops assistant. You help with operational questions, troubleshooting, and documentation lookups. Use tools when needed, cite evidence from retrieved sources, and keep responses concise and actionable. Required fields: name (string): Display name for the agent in the UI dropdown. tools (list of strings): Tools this agent can use \u2014 a subset of tools defined by the agent class. Prompt body: Everything after the frontmatter is the system prompt.","title":"Format"},{"location":"agents_tools/#practical-agent-spec-examples","text":"","title":"Practical Agent Spec Examples"},{"location":"agents_tools/#minimal-default-agent","text":"--- name: CMS CompOps Default tools: - search_local_files - search_metadata_index - list_metadata_schema - fetch_catalog_document - search_vectorstore_hybrid --- You are a CMS CompOps assistant. Rules: - Use tools to find evidence before answering. - Prefer concise, operationally actionable responses. - Cite relevant files or tickets when possible. - If evidence is missing, say so and suggest the next query/tool call.","title":"Minimal default agent"},{"location":"agents_tools/#mcp-enabled-agent","text":"--- name: CMS CompOps + MCP tools: - search_vectorstore_hybrid - fetch_catalog_document - mcp --- You are a research-focused assistant. Use vectorstore tools for internal docs first. Use MCP tools for external/system checks when internal evidence is insufficient. Always distinguish internal evidence from MCP-derived evidence.","title":"MCP-enabled agent"},{"location":"agents_tools/#monit-focused-agent-if-monit-tools-are-enabled-by-the-agent-class","text":"--- name: CMS CompOps MONIT tools: - search_vectorstore_hybrid - fetch_catalog_document - monit_opensearch_search - monit_opensearch_aggregation --- You support CompOps incident triage. When a request is about rates, failures, or timeseries: 1. Query MONIT tools. 2. Correlate with internal docs. 3. Return likely cause, confidence, and next checks. These files should live in services.chat_app.agents_dir and be selected by services.chat_app.agent_class .","title":"MONIT-focused agent (if MONIT tools are enabled by the agent class)"},{"location":"agents_tools/#file-discovery","text":"The service loads all *.md files from the agents directory. The first agent in lexicographic order is selected as the default. Users can switch agents via the dropdown in the chat UI header.","title":"File Discovery"},{"location":"agents_tools/#available-tools","text":"Agent classes define a tool registry \u2014 a mapping of tool names to tool builders. Each agent spec selects a subset of available tools via the tools list. The default agent class ( CMSCompOpsAgent ) provides these tools:","title":"Available Tools"},{"location":"agents_tools/#search_local_files","text":"Line-level search inside file contents. Supports regex and configurable context lines. Use for: Finding specific text, code patterns, or error messages Example query: timeout error with before=2 and after=2","title":"search_local_files"},{"location":"agents_tools/#search_metadata_index","text":"Search the document catalog by file name, path, or source metadata. Use free-text for partial matches or key:value for exact filters. Use for: Finding files by name, path, or source Example query: mz_dilepton.py or relative_path:full/path/to/file.py","title":"search_metadata_index"},{"location":"agents_tools/#list_metadata_schema","text":"Returns the metadata schema and available filter keys. Helps the agent understand what metadata fields exist.","title":"list_metadata_schema"},{"location":"agents_tools/#fetch_catalog_document","text":"Pull the full text of a specific file by its hash. Supports truncation with max_chars . Use for: Reading a specific document after finding it via search","title":"fetch_catalog_document"},{"location":"agents_tools/#search_vectorstore_hybrid","text":"Semantic and keyword (BM25) hybrid retrieval of relevant passages from the vector store. Use for: Answering questions when you don't know exact keywords","title":"search_vectorstore_hybrid"},{"location":"agents_tools/#mcp","text":"Enables Model Context Protocol (MCP) tools from external servers. See MCP Integration below.","title":"mcp"},{"location":"agents_tools/#agent-management-in-the-chat-ui","text":"The chat interface provides a full agent management experience: Agent dropdown : Switch between available agents using the dropdown in the header Create agents : Click the \"+\" button to create a new agent with name, tools, and prompt Edit agents : Click the pencil icon on any agent to modify its configuration Delete agents : Remove agents you no longer need (with confirmation) Changes are persisted and take effect on the next request.","title":"Agent Management in the Chat UI"},{"location":"agents_tools/#pipelines","text":"Archi supports several pipeline classes. The active pipeline is configured per service via the agent_class key: services: chat_app: agent_class: CMSCompOpsAgent","title":"Pipelines"},{"location":"agents_tools/#cmscompopsagent","text":"The default pipeline. A ReAct agent with tool-use capabilities \u2014 it can search documents, fetch content, and query the vector store. Supports MCP tools and streaming responses.","title":"CMSCompOpsAgent"},{"location":"agents_tools/#qapipeline","text":"A simpler question-answering pipeline for straightforward retrieval-augmented generation without tool use.","title":"QAPipeline"},{"location":"agents_tools/#pipeline-configuration","text":"Pipeline selection is configured per service: services: chat_app: agent_class: CMSCompOpsAgent agents_dir: examples/agents The class ( CMSCompOpsAgent , QAPipeline , etc.) defines available tools and runtime behavior. The selected agent spec file defines the active prompt and tool subset.","title":"Pipeline Configuration"},{"location":"agents_tools/#mcp-integration","text":"Archi supports the Model Context Protocol (MCP) for connecting agents to external tool servers. This allows agents to use tools defined by third-party MCP servers \u2014 such as web search, code execution, or custom APIs \u2014 alongside Archi's built-in tools.","title":"MCP Integration"},{"location":"agents_tools/#how-it-works","text":"MCP servers are defined in your deployment runtime configuration An agent spec includes mcp in its tools list to opt in At agent initialization, Archi connects to the MCP servers and discovers available tools The MCP tools are added to the agent's toolset alongside built-in tools","title":"How It Works"},{"location":"agents_tools/#configuration","text":"Define MCP servers in your deployment configuration: mcp_servers: my_server: transport: \"stdio\" command: \"uvx\" args: - \"mcp-server-example\" web_search: transport: \"sse\" url: \"http://localhost:8080/sse\" Each server entry follows the format expected by the langchain-mcp-adapters library: transport : Communication method \u2014 \"stdio\" (subprocess) or \"sse\" (HTTP Server-Sent Events) command / args : For stdio transport, the command to launch the server url : For sse transport, the server endpoint","title":"Configuration"},{"location":"agents_tools/#agent-spec-example","text":"Include mcp in the tools list to enable MCP tools for an agent: --- name: Research Assistant tools: - search_vectorstore_hybrid - fetch_catalog_document - mcp --- You are a research assistant with access to external tools via MCP. Use vectorstore search for internal documents and MCP tools for external information retrieval.","title":"Agent Spec Example"},{"location":"agents_tools/#runtime-behavior","text":"MCP sessions are maintained in a background event loop for the lifetime of the service Each MCP tool is wrapped for synchronous execution so it integrates seamlessly with the ReAct agent loop Tool names from MCP servers are namespaced to avoid conflicts with built-in tools","title":"Runtime Behavior"},{"location":"agents_tools/#vector-store-retrieval","text":"The vector store powers document retrieval in Archi. It uses PostgreSQL with pgvector for production-grade vector similarity search.","title":"Vector Store &amp; Retrieval"},{"location":"agents_tools/#core-settings","text":"data_manager: collection_name: default_collection embedding_name: OpenAIEmbeddings chunk_size: 1000 chunk_overlap: 0 reset_collection: true distance_metric: cosine retrievers: hybrid_retriever: num_documents_to_retrieve: 5 bm25_weight: 0.6 semantic_weight: 0.4 Setting Description Default collection_name Name of the vector store collection default_collection chunk_size Maximum characters per text chunk 1000 chunk_overlap Overlapping characters between chunks 0 reset_collection Wipe and recreate collection on startup true retrievers.hybrid_retriever.num_documents_to_retrieve Top-k documents per query 5 distance_metric Similarity metric: cosine , l2 , or ip cosine","title":"Core Settings"},{"location":"agents_tools/#hybrid-search","text":"Hybrid search (semantic + BM25 keyword retrieval) is enabled by default as a dynamic runtime setting. The weights are configurable in your YAML: data_manager: retrievers: hybrid_retriever: bm25_weight: 0.6 semantic_weight: 0.4","title":"Hybrid Search"},{"location":"agents_tools/#stemming","text":"Enable stemming to reduce words to root forms for improved matching: data_manager: stemming: enabled: true","title":"Stemming"},{"location":"agents_tools/#supported-document-formats","text":".txt , .md , .py , .c , .C , .h , .sh , .html , .htm , .pdf , .json , .yaml , .yml , .csv , .tsv , .log , .rst , .php","title":"Supported Document Formats"},{"location":"agents_tools/#document-synchronization","text":"Archi automatically synchronizes documents with the vector store: New files are chunked, embedded, and indexed Deleted files are removed from the collection All artifacts are tracked in the PostgreSQL resources catalog","title":"Document Synchronization"},{"location":"api_reference/","text":"API Reference REST API endpoints for the Archi chat application. All endpoints are prefixed with /api/ . Note: For the CLI reference, see CLI Reference . For the configuration YAML schema, see Configuration Reference . How to Read This Page Base URL is your running chat service (for example http://localhost:7861 ). Most /api/* endpoints require an authenticated session. Endpoints marked Admin only require an admin user. Authentication routes ( /login , /logout , /auth/user ) are not under /api/ . Chat POST /api/get_chat_response Send a message and receive a complete response. POST /api/get_chat_response_stream Send a message and receive a streaming response via NDJSON ( application/x-ndjson ). Each line is a JSON object with a type field. Event types: Type Description meta Stream metadata (sent first, includes padding) text Response text delta tool_start Agent is invoking a tool tool_output Tool result thinking_start Reasoning model thinking begins thinking_end Reasoning model thinking ends final Final response with full message and metadata error Error occurred POST /api/cancel_stream Cancel an in-progress streaming response. GET /api/trace/<trace_id> Retrieve the full trace of a previous request. POST /api/ab/create Create an A/B comparison between two model responses. Authentication Authentication routes are served at the application root (not under /api/ ). GET|POST /login Authenticate with email and password. GET renders the login page; POST processes credentials. GET /logout End the current session. GET /auth/user Get the current authenticated user. User Management GET /api/users/me Get or create the current user. Response: { \"id\": \"user_abc123\", \"display_name\": \"John Doe\", \"email\": \"john@example.com\", \"auth_provider\": \"basic\", \"theme\": \"dark\", \"preferred_model\": \"gpt-4o\", \"preferred_temperature\": 0.7, \"has_openrouter_key\": true, \"has_openai_key\": false, \"has_anthropic_key\": false } PATCH /api/users/me/preferences Update user preferences (model, temperature, prompts, theme). Request: { \"theme\": \"light\", \"preferred_model\": \"claude-3-opus\", \"preferred_temperature\": 0.5 } PUT /api/users/me/api-keys/{provider} Set a BYOK API key. Provider: openrouter , openai , anthropic . DELETE /api/users/me/api-keys/{provider} Delete a BYOK API key. Provider Keys (BYOK) GET /api/providers/keys Get status of all provider API keys. POST /api/providers/keys/set Set a session API key (validates before storing). POST /api/providers/keys/clear Clear a session API key. Configuration GET /api/config/static Get static (deploy-time) configuration. Response: { \"deployment_name\": \"my-archi\", \"embedding_model\": \"text-embedding-3-small\", \"available_pipelines\": [\"QAPipeline\", \"CMSCompOpsAgent\"], \"available_models\": [\"gpt-4o\", \"claude-3-opus\"], \"auth_enabled\": true, \"prompts_path\": \"/root/archi/data/prompts/\" } GET /api/config/dynamic Get dynamic (runtime) configuration. Response: { \"active_pipeline\": \"QAPipeline\", \"active_model\": \"gpt-4o\", \"temperature\": 0.7, \"max_tokens\": 4096, \"top_p\": 0.9, \"top_k\": 50, \"num_documents_to_retrieve\": 10, \"verbosity\": 3 } PATCH /api/config/dynamic Update dynamic configuration. Admin only. Request: { \"active_model\": \"gpt-4o\", \"temperature\": 0.8, \"num_documents_to_retrieve\": 5 } GET /api/config/effective Get effective configuration for the current user (user preferences applied). GET /api/config/audit Get configuration change audit log. Admin only. Query params: limit (default: 100) Agents GET /api/agents/list List all available agent specs. GET /api/agents/spec Get a specific agent spec (name, tools, prompt). Pass name as a query parameter. GET /api/agents/template Get the template for creating a new agent (available tools, defaults). POST /api/agents Create or update an agent spec. Request: { \"name\": \"My Agent\", \"tools\": [\"search_vectorstore_hybrid\", \"fetch_catalog_document\"], \"prompt\": \"You are a helpful assistant...\" } DELETE /api/agents Delete an agent spec. Pass name as a query parameter or in the request body. POST /api/agents/active Set the active agent for the current session. Request: { \"agent_name\": \"CMS Comp Ops\" } Prompts GET /api/prompts List all available prompts by type. Response: { \"condense\": [\"default\", \"concise\"], \"chat\": [\"default\", \"formal\", \"technical\"], \"system\": [\"default\", \"helpful\"] } GET /api/prompts/{type} List prompts for a specific type. GET /api/prompts/{type}/{name} Get prompt content. POST /api/prompts/reload Reload prompt cache from disk. Admin only. Document Selection Three-tier document selection: conversation override \u2192 user default \u2192 system default. GET /api/documents/selection Get enabled documents. Query param: conversation_id . PUT /api/documents/user-defaults Set user's default for a document. Request: { \"document_id\": 42, \"enabled\": false } PUT /api/documents/conversation-override Set conversation-specific override. DELETE /api/documents/conversation-override Clear conversation override (fall back to user default). Data Viewer GET /api/data/documents List ingested documents with pagination and filtering. Query params: limit (default: 100), offset , search , source_type Response: { \"documents\": [ { \"hash\": \"5e90ca54526f3e11\", \"file_name\": \"readme.md\", \"source_type\": \"links\", \"chunk_count\": 5, \"enabled\": true, \"ingested_at\": \"2025-01-29T10:30:00Z\" } ], \"total\": 42 } GET /api/data/documents/<hash>/content Get document content and chunks. POST /api/data/documents/<hash>/enable Enable a document for retrieval. POST /api/data/documents/<hash>/disable Disable a document from retrieval. POST /api/data/bulk-enable Enable multiple documents. Request: { \"hashes\": [\"5e90ca54526f3e11\", \"a1b2c3d4e5f67890\"] } POST /api/data/bulk-disable Disable multiple documents. GET /api/data/stats Get document statistics (total, enabled, disabled, by source type). Analytics GET /api/analytics/model-usage Get model usage statistics. Query params: start_date , end_date , service . GET /api/analytics/ab-comparisons Get A/B comparison statistics with win rates. Query params: model_a , model_b , start_date , end_date . Data Manager These endpoints are served by the Data Manager service (default port: 7871). GET /api/ingestion/status Get current ingestion progress. POST /api/reload-schedules Trigger schedule reload from database. GET /api/schedules Get current schedule status. Health & Info GET /api/health Health check with database connectivity status. GET /api/info Get API version and available features.","title":"API Reference"},{"location":"api_reference/#api-reference","text":"REST API endpoints for the Archi chat application. All endpoints are prefixed with /api/ . Note: For the CLI reference, see CLI Reference . For the configuration YAML schema, see Configuration Reference .","title":"API Reference"},{"location":"api_reference/#how-to-read-this-page","text":"Base URL is your running chat service (for example http://localhost:7861 ). Most /api/* endpoints require an authenticated session. Endpoints marked Admin only require an admin user. Authentication routes ( /login , /logout , /auth/user ) are not under /api/ .","title":"How to Read This Page"},{"location":"api_reference/#chat","text":"","title":"Chat"},{"location":"api_reference/#post-apiget_chat_response","text":"Send a message and receive a complete response.","title":"POST /api/get_chat_response"},{"location":"api_reference/#post-apiget_chat_response_stream","text":"Send a message and receive a streaming response via NDJSON ( application/x-ndjson ). Each line is a JSON object with a type field. Event types: Type Description meta Stream metadata (sent first, includes padding) text Response text delta tool_start Agent is invoking a tool tool_output Tool result thinking_start Reasoning model thinking begins thinking_end Reasoning model thinking ends final Final response with full message and metadata error Error occurred","title":"POST /api/get_chat_response_stream"},{"location":"api_reference/#post-apicancel_stream","text":"Cancel an in-progress streaming response.","title":"POST /api/cancel_stream"},{"location":"api_reference/#get-apitracetrace_id","text":"Retrieve the full trace of a previous request.","title":"GET /api/trace/&lt;trace_id&gt;"},{"location":"api_reference/#post-apiabcreate","text":"Create an A/B comparison between two model responses.","title":"POST /api/ab/create"},{"location":"api_reference/#authentication","text":"Authentication routes are served at the application root (not under /api/ ).","title":"Authentication"},{"location":"api_reference/#getpost-login","text":"Authenticate with email and password. GET renders the login page; POST processes credentials.","title":"GET|POST /login"},{"location":"api_reference/#get-logout","text":"End the current session.","title":"GET /logout"},{"location":"api_reference/#get-authuser","text":"Get the current authenticated user.","title":"GET /auth/user"},{"location":"api_reference/#user-management","text":"","title":"User Management"},{"location":"api_reference/#get-apiusersme","text":"Get or create the current user. Response: { \"id\": \"user_abc123\", \"display_name\": \"John Doe\", \"email\": \"john@example.com\", \"auth_provider\": \"basic\", \"theme\": \"dark\", \"preferred_model\": \"gpt-4o\", \"preferred_temperature\": 0.7, \"has_openrouter_key\": true, \"has_openai_key\": false, \"has_anthropic_key\": false }","title":"GET /api/users/me"},{"location":"api_reference/#patch-apiusersmepreferences","text":"Update user preferences (model, temperature, prompts, theme). Request: { \"theme\": \"light\", \"preferred_model\": \"claude-3-opus\", \"preferred_temperature\": 0.5 }","title":"PATCH /api/users/me/preferences"},{"location":"api_reference/#put-apiusersmeapi-keysprovider","text":"Set a BYOK API key. Provider: openrouter , openai , anthropic .","title":"PUT /api/users/me/api-keys/{provider}"},{"location":"api_reference/#delete-apiusersmeapi-keysprovider","text":"Delete a BYOK API key.","title":"DELETE /api/users/me/api-keys/{provider}"},{"location":"api_reference/#provider-keys-byok","text":"","title":"Provider Keys (BYOK)"},{"location":"api_reference/#get-apiproviderskeys","text":"Get status of all provider API keys.","title":"GET /api/providers/keys"},{"location":"api_reference/#post-apiproviderskeysset","text":"Set a session API key (validates before storing).","title":"POST /api/providers/keys/set"},{"location":"api_reference/#post-apiproviderskeysclear","text":"Clear a session API key.","title":"POST /api/providers/keys/clear"},{"location":"api_reference/#configuration","text":"","title":"Configuration"},{"location":"api_reference/#get-apiconfigstatic","text":"Get static (deploy-time) configuration. Response: { \"deployment_name\": \"my-archi\", \"embedding_model\": \"text-embedding-3-small\", \"available_pipelines\": [\"QAPipeline\", \"CMSCompOpsAgent\"], \"available_models\": [\"gpt-4o\", \"claude-3-opus\"], \"auth_enabled\": true, \"prompts_path\": \"/root/archi/data/prompts/\" }","title":"GET /api/config/static"},{"location":"api_reference/#get-apiconfigdynamic","text":"Get dynamic (runtime) configuration. Response: { \"active_pipeline\": \"QAPipeline\", \"active_model\": \"gpt-4o\", \"temperature\": 0.7, \"max_tokens\": 4096, \"top_p\": 0.9, \"top_k\": 50, \"num_documents_to_retrieve\": 10, \"verbosity\": 3 }","title":"GET /api/config/dynamic"},{"location":"api_reference/#patch-apiconfigdynamic","text":"Update dynamic configuration. Admin only. Request: { \"active_model\": \"gpt-4o\", \"temperature\": 0.8, \"num_documents_to_retrieve\": 5 }","title":"PATCH /api/config/dynamic"},{"location":"api_reference/#get-apiconfigeffective","text":"Get effective configuration for the current user (user preferences applied).","title":"GET /api/config/effective"},{"location":"api_reference/#get-apiconfigaudit","text":"Get configuration change audit log. Admin only. Query params: limit (default: 100)","title":"GET /api/config/audit"},{"location":"api_reference/#agents","text":"","title":"Agents"},{"location":"api_reference/#get-apiagentslist","text":"List all available agent specs.","title":"GET /api/agents/list"},{"location":"api_reference/#get-apiagentsspec","text":"Get a specific agent spec (name, tools, prompt). Pass name as a query parameter.","title":"GET /api/agents/spec"},{"location":"api_reference/#get-apiagentstemplate","text":"Get the template for creating a new agent (available tools, defaults).","title":"GET /api/agents/template"},{"location":"api_reference/#post-apiagents","text":"Create or update an agent spec. Request: { \"name\": \"My Agent\", \"tools\": [\"search_vectorstore_hybrid\", \"fetch_catalog_document\"], \"prompt\": \"You are a helpful assistant...\" }","title":"POST /api/agents"},{"location":"api_reference/#delete-apiagents","text":"Delete an agent spec. Pass name as a query parameter or in the request body.","title":"DELETE /api/agents"},{"location":"api_reference/#post-apiagentsactive","text":"Set the active agent for the current session. Request: { \"agent_name\": \"CMS Comp Ops\" }","title":"POST /api/agents/active"},{"location":"api_reference/#prompts","text":"","title":"Prompts"},{"location":"api_reference/#get-apiprompts","text":"List all available prompts by type. Response: { \"condense\": [\"default\", \"concise\"], \"chat\": [\"default\", \"formal\", \"technical\"], \"system\": [\"default\", \"helpful\"] }","title":"GET /api/prompts"},{"location":"api_reference/#get-apipromptstype","text":"List prompts for a specific type.","title":"GET /api/prompts/{type}"},{"location":"api_reference/#get-apipromptstypename","text":"Get prompt content.","title":"GET /api/prompts/{type}/{name}"},{"location":"api_reference/#post-apipromptsreload","text":"Reload prompt cache from disk. Admin only.","title":"POST /api/prompts/reload"},{"location":"api_reference/#document-selection","text":"Three-tier document selection: conversation override \u2192 user default \u2192 system default.","title":"Document Selection"},{"location":"api_reference/#get-apidocumentsselection","text":"Get enabled documents. Query param: conversation_id .","title":"GET /api/documents/selection"},{"location":"api_reference/#put-apidocumentsuser-defaults","text":"Set user's default for a document. Request: { \"document_id\": 42, \"enabled\": false }","title":"PUT /api/documents/user-defaults"},{"location":"api_reference/#put-apidocumentsconversation-override","text":"Set conversation-specific override.","title":"PUT /api/documents/conversation-override"},{"location":"api_reference/#delete-apidocumentsconversation-override","text":"Clear conversation override (fall back to user default).","title":"DELETE /api/documents/conversation-override"},{"location":"api_reference/#data-viewer","text":"","title":"Data Viewer"},{"location":"api_reference/#get-apidatadocuments","text":"List ingested documents with pagination and filtering. Query params: limit (default: 100), offset , search , source_type Response: { \"documents\": [ { \"hash\": \"5e90ca54526f3e11\", \"file_name\": \"readme.md\", \"source_type\": \"links\", \"chunk_count\": 5, \"enabled\": true, \"ingested_at\": \"2025-01-29T10:30:00Z\" } ], \"total\": 42 }","title":"GET /api/data/documents"},{"location":"api_reference/#get-apidatadocumentshashcontent","text":"Get document content and chunks.","title":"GET /api/data/documents/&lt;hash&gt;/content"},{"location":"api_reference/#post-apidatadocumentshashenable","text":"Enable a document for retrieval.","title":"POST /api/data/documents/&lt;hash&gt;/enable"},{"location":"api_reference/#post-apidatadocumentshashdisable","text":"Disable a document from retrieval.","title":"POST /api/data/documents/&lt;hash&gt;/disable"},{"location":"api_reference/#post-apidatabulk-enable","text":"Enable multiple documents. Request: { \"hashes\": [\"5e90ca54526f3e11\", \"a1b2c3d4e5f67890\"] }","title":"POST /api/data/bulk-enable"},{"location":"api_reference/#post-apidatabulk-disable","text":"Disable multiple documents.","title":"POST /api/data/bulk-disable"},{"location":"api_reference/#get-apidatastats","text":"Get document statistics (total, enabled, disabled, by source type).","title":"GET /api/data/stats"},{"location":"api_reference/#analytics","text":"","title":"Analytics"},{"location":"api_reference/#get-apianalyticsmodel-usage","text":"Get model usage statistics. Query params: start_date , end_date , service .","title":"GET /api/analytics/model-usage"},{"location":"api_reference/#get-apianalyticsab-comparisons","text":"Get A/B comparison statistics with win rates. Query params: model_a , model_b , start_date , end_date .","title":"GET /api/analytics/ab-comparisons"},{"location":"api_reference/#data-manager","text":"These endpoints are served by the Data Manager service (default port: 7871).","title":"Data Manager"},{"location":"api_reference/#get-apiingestionstatus","text":"Get current ingestion progress.","title":"GET /api/ingestion/status"},{"location":"api_reference/#post-apireload-schedules","text":"Trigger schedule reload from database.","title":"POST /api/reload-schedules"},{"location":"api_reference/#get-apischedules","text":"Get current schedule status.","title":"GET /api/schedules"},{"location":"api_reference/#health-info","text":"","title":"Health &amp; Info"},{"location":"api_reference/#get-apihealth","text":"Health check with database connectivity status.","title":"GET /api/health"},{"location":"api_reference/#get-apiinfo","text":"Get API version and available features.","title":"GET /api/info"},{"location":"benchmarking/","text":"Benchmarking Archi provides benchmarking functionality via the archi evaluate CLI command to measure retrieval and response quality. Evaluation Modes Two modes are supported (can be used together): SOURCES Mode Checks if retrieved documents contain the correct sources by comparing metadata fields. Default match field: file_name (configurable per-query) Override with sources_match_field in the queries file RAGAS Mode Uses the Ragas evaluator for four metrics: Answer relevancy : How relevant the answer is to the question Faithfulness : Whether the answer is grounded in the retrieved context Context precision : How relevant the retrieved documents are Context relevancy : How much of the retrieved context is useful Preparing the Queries File Provide questions, expected answers, and correct sources in JSON format: [ { \"question\": \"Does Jorian Benke work with the PPC?\", \"sources\": [ \"https://ppc.mit.edu/blog/2025/07/14/welcome-our-first-ever-in-house-masters-student/\", \"CMSPROD-42\" ], \"answer\": \"Yes, Jorian works with the PPC and her topic is Lorentz invariance.\", \"source_match_field\": [\"url\", \"ticket_id\"] } ] Field Required Description question Yes The question to ask sources Yes List of source identifiers (URLs, ticket IDs, etc.) answer Yes Expected answer (used for RAGAS evaluation) source_match_field No Metadata fields to match sources against (defaults to config value) See examples/benchmarking/queries.json for a complete example. Configuration services: benchmarking: queries_path: examples/benchmarking/queries.json out_dir: bench_out modes: - \"RAGAS\" - \"SOURCES\" timeout: 180 batch_size: 10 mode_settings: sources: default_match_field: [\"file_name\"] ragas_settings: provider: OpenAI evaluation_model_settings: model_name: gpt-4o embedding_model: OpenAI Key Default Description queries_path \u2014 Path to the queries JSON file out_dir \u2014 Output directory for results (must exist) modes \u2014 List of evaluation modes ( RAGAS , SOURCES ) timeout 180 Max seconds per QA pair for RAGAS evaluation batch_size Ragas default Number of QA pairs to evaluate at once RAGAS Settings Key Description provider One of: OpenAI , HuggingFace , Ollama , Anthropic evaluation_model_settings.model_name LangChain model name for evaluation evaluation_model_settings.base_url For Ollama: address of the running server embedding_model OpenAI or HuggingFace Running Evaluate one or more configurations: # Single config file archi evaluate -n benchmark -c config.yaml -e .secrets.env # Directory of configs (for comparing hyperparameters) archi evaluate -n benchmark -cd configs/ -e .secrets.env # With GPU support archi evaluate -n benchmark -c config.yaml -e .secrets.env --gpu-ids all Make sure the out_dir exists before running. Results Results are saved in a timestamped subdirectory of out_dir (e.g., bench_out/2042-10-01_12-00-00/ ). To analyze results, see scripts/benchmarking/ which contains: Plotting functions An IPython notebook with usage examples ( benchmark_handler.ipynb )","title":"Benchmarking"},{"location":"benchmarking/#benchmarking","text":"Archi provides benchmarking functionality via the archi evaluate CLI command to measure retrieval and response quality.","title":"Benchmarking"},{"location":"benchmarking/#evaluation-modes","text":"Two modes are supported (can be used together):","title":"Evaluation Modes"},{"location":"benchmarking/#sources-mode","text":"Checks if retrieved documents contain the correct sources by comparing metadata fields. Default match field: file_name (configurable per-query) Override with sources_match_field in the queries file","title":"SOURCES Mode"},{"location":"benchmarking/#ragas-mode","text":"Uses the Ragas evaluator for four metrics: Answer relevancy : How relevant the answer is to the question Faithfulness : Whether the answer is grounded in the retrieved context Context precision : How relevant the retrieved documents are Context relevancy : How much of the retrieved context is useful","title":"RAGAS Mode"},{"location":"benchmarking/#preparing-the-queries-file","text":"Provide questions, expected answers, and correct sources in JSON format: [ { \"question\": \"Does Jorian Benke work with the PPC?\", \"sources\": [ \"https://ppc.mit.edu/blog/2025/07/14/welcome-our-first-ever-in-house-masters-student/\", \"CMSPROD-42\" ], \"answer\": \"Yes, Jorian works with the PPC and her topic is Lorentz invariance.\", \"source_match_field\": [\"url\", \"ticket_id\"] } ] Field Required Description question Yes The question to ask sources Yes List of source identifiers (URLs, ticket IDs, etc.) answer Yes Expected answer (used for RAGAS evaluation) source_match_field No Metadata fields to match sources against (defaults to config value) See examples/benchmarking/queries.json for a complete example.","title":"Preparing the Queries File"},{"location":"benchmarking/#configuration","text":"services: benchmarking: queries_path: examples/benchmarking/queries.json out_dir: bench_out modes: - \"RAGAS\" - \"SOURCES\" timeout: 180 batch_size: 10 mode_settings: sources: default_match_field: [\"file_name\"] ragas_settings: provider: OpenAI evaluation_model_settings: model_name: gpt-4o embedding_model: OpenAI Key Default Description queries_path \u2014 Path to the queries JSON file out_dir \u2014 Output directory for results (must exist) modes \u2014 List of evaluation modes ( RAGAS , SOURCES ) timeout 180 Max seconds per QA pair for RAGAS evaluation batch_size Ragas default Number of QA pairs to evaluate at once","title":"Configuration"},{"location":"benchmarking/#ragas-settings","text":"Key Description provider One of: OpenAI , HuggingFace , Ollama , Anthropic evaluation_model_settings.model_name LangChain model name for evaluation evaluation_model_settings.base_url For Ollama: address of the running server embedding_model OpenAI or HuggingFace","title":"RAGAS Settings"},{"location":"benchmarking/#running","text":"Evaluate one or more configurations: # Single config file archi evaluate -n benchmark -c config.yaml -e .secrets.env # Directory of configs (for comparing hyperparameters) archi evaluate -n benchmark -cd configs/ -e .secrets.env # With GPU support archi evaluate -n benchmark -c config.yaml -e .secrets.env --gpu-ids all Make sure the out_dir exists before running.","title":"Running"},{"location":"benchmarking/#results","text":"Results are saved in a timestamped subdirectory of out_dir (e.g., bench_out/2042-10-01_12-00-00/ ). To analyze results, see scripts/benchmarking/ which contains: Plotting functions An IPython notebook with usage examples ( benchmark_handler.ipynb )","title":"Results"},{"location":"cli_reference/","text":"CLI Reference The Archi CLI provides commands to create, manage, and monitor deployments. Installation The CLI is installed automatically with pip install -e . from the repository root. Verify with: which archi Commands archi create Create a new Archi deployment. archi create --name <name> --config <config.yaml> --env-file <secrets.env> --services <services> [OPTIONS] Required options: Option Description --name , -n Name of the deployment --config , -c Path to YAML configuration file (repeatable for multiple files) Recommended options: Option Description --env-file , -e Path to the secrets .env file --services , -s Comma-separated list of services to enable (e.g., chatbot,uploader ) Optional flags: Option Description Default --config-dir , -cd Directory containing configuration files \u2014 --podman , -p Use Podman instead of Docker Docker --gpu-ids GPU configuration: all or comma-separated IDs (e.g., 0,1 ) None --tag , -t Image tag for built containers 2000 --hostmode Use host network mode for all services Off --verbosity , -v Logging verbosity level (0=quiet, 4=debug) 3 --force , -f Overwrite existing deployment if it exists Off --dry , --dry-run Validate and show what would be created without deploying Off Examples: # Basic deployment with Ollama archi create -n my-archi -c config.yaml -e .secrets.env \\ --services chatbot --podman # Full deployment with GPU and multiple services archi create -n prod-archi -c config.yaml -e .secrets.env \\ --services chatbot,uploader,grafana \\ --gpu-ids all # Dry run to validate configuration archi create -n test -c config.yaml -e .secrets.env \\ --services chatbot --dry-run Notes: The CLI checks that host ports are free before deploying. If a port is in use, adjust services.*.external_port in your config. The first deployment builds container images from scratch (may take several minutes). Subsequent deployments reuse images. Use -v 4 for debug-level logging when troubleshooting. archi delete Delete an existing deployment. archi delete --name <name> [OPTIONS] Option Description --name , -n Name of the deployment to delete --rmi Also remove container images --rmv Also remove volumes --keep-files Keep deployment files on disk --list List all deployments Examples: # Delete deployment and clean up everything archi delete -n my-archi --rmi --rmv # Delete but keep data volumes archi delete -n my-archi --rmi archi restart Restart a specific service in an existing deployment without restarting the entire stack. archi restart --name <name> --service <service> [OPTIONS] Option Description Default --name , -n Name of the existing deployment Required --service , -s Service to restart chatbot --config , -c Updated configuration file(s) \u2014 --config-dir , -cd Directory containing configuration files \u2014 --env-file , -e Updated secrets file \u2014 --no-build Restart without rebuilding the container image Off --with-deps Also restart dependent services Off --podman , -p Use Podman instead of Docker Docker --verbosity , -v Logging verbosity (0-4) 3 Examples: # Quick config update (no rebuild needed) archi restart -n my-archi --service chatbot --no-build # Rebuild after code changes archi restart -n my-archi --service chatbot -c updated_config.yaml # Re-scrape data sources archi restart -n my-archi --service data_manager # Restart with updated secrets archi restart -n my-archi --service chatbot -e new_secrets.env --no-build archi list-services List all available services and data sources with descriptions. archi list-services archi list-deployments List all existing deployments. archi list-deployments archi evaluate Launch the benchmarking runtime to evaluate configurations against a set of questions and answers. archi evaluate --name <name> --env-file <secrets.env> --config <config.yaml> [OPTIONS] Supports the same flags as create ( --podman , --gpu-ids , --tag , --hostmode , --verbosity , --force ). Configuration files should define the services.benchmarking section. Example: archi evaluate -n benchmark \\ -c examples/benchmarking/benchmark_configs/example_conf.yaml \\ -e .secrets.env --gpu-ids all See Benchmarking for full details on query format and evaluation modes. Environment Variables Variable Description ARCHI_DIR Override the deployment directory (default: ~/.archi ) OLLAMA_HOST Ollama server address (default: http://localhost:11434 ) Troubleshooting Port Conflicts If a port is already in use, the CLI will report an error. Adjust services.*.external_port in your config: services: chat_app: external_port: 7862 # default: 7861 grafana: external_port: 3001 # default: 3000 GPU Issues GPU access requires NVIDIA drivers and the NVIDIA Container Toolkit. Podman: sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml nvidia-ctk cdi list Docker: sudo nvidia-ctk runtime configure --runtime=docker Verbose Logging Add -v 4 to any command for debug-level output: archi create [...] -v 4 Multiple Deployments Multiple deployments can run on the same machine. Container networks are separate, but be careful with external port assignments. See Advanced Setup .","title":"CLI Reference"},{"location":"cli_reference/#cli-reference","text":"The Archi CLI provides commands to create, manage, and monitor deployments.","title":"CLI Reference"},{"location":"cli_reference/#installation","text":"The CLI is installed automatically with pip install -e . from the repository root. Verify with: which archi","title":"Installation"},{"location":"cli_reference/#commands","text":"","title":"Commands"},{"location":"cli_reference/#archi-create","text":"Create a new Archi deployment. archi create --name <name> --config <config.yaml> --env-file <secrets.env> --services <services> [OPTIONS] Required options: Option Description --name , -n Name of the deployment --config , -c Path to YAML configuration file (repeatable for multiple files) Recommended options: Option Description --env-file , -e Path to the secrets .env file --services , -s Comma-separated list of services to enable (e.g., chatbot,uploader ) Optional flags: Option Description Default --config-dir , -cd Directory containing configuration files \u2014 --podman , -p Use Podman instead of Docker Docker --gpu-ids GPU configuration: all or comma-separated IDs (e.g., 0,1 ) None --tag , -t Image tag for built containers 2000 --hostmode Use host network mode for all services Off --verbosity , -v Logging verbosity level (0=quiet, 4=debug) 3 --force , -f Overwrite existing deployment if it exists Off --dry , --dry-run Validate and show what would be created without deploying Off Examples: # Basic deployment with Ollama archi create -n my-archi -c config.yaml -e .secrets.env \\ --services chatbot --podman # Full deployment with GPU and multiple services archi create -n prod-archi -c config.yaml -e .secrets.env \\ --services chatbot,uploader,grafana \\ --gpu-ids all # Dry run to validate configuration archi create -n test -c config.yaml -e .secrets.env \\ --services chatbot --dry-run Notes: The CLI checks that host ports are free before deploying. If a port is in use, adjust services.*.external_port in your config. The first deployment builds container images from scratch (may take several minutes). Subsequent deployments reuse images. Use -v 4 for debug-level logging when troubleshooting.","title":"archi create"},{"location":"cli_reference/#archi-delete","text":"Delete an existing deployment. archi delete --name <name> [OPTIONS] Option Description --name , -n Name of the deployment to delete --rmi Also remove container images --rmv Also remove volumes --keep-files Keep deployment files on disk --list List all deployments Examples: # Delete deployment and clean up everything archi delete -n my-archi --rmi --rmv # Delete but keep data volumes archi delete -n my-archi --rmi","title":"archi delete"},{"location":"cli_reference/#archi-restart","text":"Restart a specific service in an existing deployment without restarting the entire stack. archi restart --name <name> --service <service> [OPTIONS] Option Description Default --name , -n Name of the existing deployment Required --service , -s Service to restart chatbot --config , -c Updated configuration file(s) \u2014 --config-dir , -cd Directory containing configuration files \u2014 --env-file , -e Updated secrets file \u2014 --no-build Restart without rebuilding the container image Off --with-deps Also restart dependent services Off --podman , -p Use Podman instead of Docker Docker --verbosity , -v Logging verbosity (0-4) 3 Examples: # Quick config update (no rebuild needed) archi restart -n my-archi --service chatbot --no-build # Rebuild after code changes archi restart -n my-archi --service chatbot -c updated_config.yaml # Re-scrape data sources archi restart -n my-archi --service data_manager # Restart with updated secrets archi restart -n my-archi --service chatbot -e new_secrets.env --no-build","title":"archi restart"},{"location":"cli_reference/#archi-list-services","text":"List all available services and data sources with descriptions. archi list-services","title":"archi list-services"},{"location":"cli_reference/#archi-list-deployments","text":"List all existing deployments. archi list-deployments","title":"archi list-deployments"},{"location":"cli_reference/#archi-evaluate","text":"Launch the benchmarking runtime to evaluate configurations against a set of questions and answers. archi evaluate --name <name> --env-file <secrets.env> --config <config.yaml> [OPTIONS] Supports the same flags as create ( --podman , --gpu-ids , --tag , --hostmode , --verbosity , --force ). Configuration files should define the services.benchmarking section. Example: archi evaluate -n benchmark \\ -c examples/benchmarking/benchmark_configs/example_conf.yaml \\ -e .secrets.env --gpu-ids all See Benchmarking for full details on query format and evaluation modes.","title":"archi evaluate"},{"location":"cli_reference/#environment-variables","text":"Variable Description ARCHI_DIR Override the deployment directory (default: ~/.archi ) OLLAMA_HOST Ollama server address (default: http://localhost:11434 )","title":"Environment Variables"},{"location":"cli_reference/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"cli_reference/#port-conflicts","text":"If a port is already in use, the CLI will report an error. Adjust services.*.external_port in your config: services: chat_app: external_port: 7862 # default: 7861 grafana: external_port: 3001 # default: 3000","title":"Port Conflicts"},{"location":"cli_reference/#gpu-issues","text":"GPU access requires NVIDIA drivers and the NVIDIA Container Toolkit. Podman: sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml nvidia-ctk cdi list Docker: sudo nvidia-ctk runtime configure --runtime=docker","title":"GPU Issues"},{"location":"cli_reference/#verbose-logging","text":"Add -v 4 to any command for debug-level output: archi create [...] -v 4","title":"Verbose Logging"},{"location":"cli_reference/#multiple-deployments","text":"Multiple deployments can run on the same machine. Container networks are separate, but be careful with external port assignments. See Advanced Setup .","title":"Multiple Deployments"},{"location":"configuration/","text":"Configuration Reference Archi deployments are configured via YAML files passed to the CLI with --config . Any fields not specified are populated from the base template at src/cli/templates/base-config.yaml . Tip: Start from one of the example configs in examples/deployments/ and customize from there. Top-Level Fields name Type: string (required) Name of your deployment. Used for container naming and directory structure. name: my_deployment global Global settings shared across all services. Key Type Default Description DATA_PATH string /root/data/ Path for persisted data inside containers ACCOUNTS_PATH string /root/.accounts/ Path for uploader/grader account data ACCEPTED_FILES list See below File extensions allowed for manual uploads LOGGING.input_output_filename string chain_input_output.log Pipeline I/O log filename verbosity int 3 Default logging level for services (0-4) Default accepted files: .pdf , .md , .txt , .docx , .html , .htm , .json , .yaml , .yml , .py , .js , .ts , .jsx , .tsx , .java , .go , .rs , .c , .cpp , .h , .sh services Configuration for containerized services. Each service has its own subsection. services.chat_app The main chat interface. Key Type Default Description agent_class string CMSCompOpsAgent Pipeline class to run agents_dir string \u2014 Path to agent markdown files default_provider string local Default LLM provider default_model string llama3.2 Default model tools dict {} Agent-class-specific tool settings (for example tools.monit.url ) trained_on string \u2014 Description shown in the chat UI hostname string localhost Public hostname for the chat interface port int 7861 Internal container port external_port int 7861 Host-mapped port host string 0.0.0.0 Network binding num_responses_until_feedback int 3 Responses before prompting for feedback auth.enabled bool false Enable authentication Provider Configuration services: chat_app: providers: local: enabled: true base_url: http://localhost:11434 mode: ollama # or openai_compat default_model: llama3.2 models: - llama3.2 gemini: enabled: true services.postgres PostgreSQL database settings. Key Type Default Description host string postgres Database hostname port int 5432 Database port user string archi Database user database string archi-db Database name services.vectorstore Key Type Default Description backend string postgres Vector store backend (only postgres supported) services.data_manager Key Type Default Description port int 7871 Internal port external_port int 7871 Host-mapped port host string 0.0.0.0 Network binding enabled bool true Enable data manager service services.grafana Key Type Default Description port int 3000 Grafana port external_port int 3000 Host-mapped port services.grader_app Key Type Default Description port int 7861 Internal port external_port int 7862 Host-mapped port provider string \u2014 Provider for grading pipelines model string \u2014 Model for grading pipelines num_problems int \u2014 Number of problems (must match rubric files) local_rubric_dir string \u2014 Path to rubric files local_users_csv_dir string \u2014 Path to users CSV Other Services services.piazza : Requires network_id , agent_class , provider , model services.mattermost : Requires update_time services.redmine_mailbox : Requires url , project , redmine_update_time , mailbox_update_time services.benchmarking : See Benchmarking data_manager Controls data ingestion, vectorstore behaviour, and retrieval settings. Core Settings Key Type Default Description collection_name string default_collection Vector store collection name embedding_name string OpenAIEmbeddings Embedding backend chunk_size int 1000 Max characters per text chunk chunk_overlap int 0 Overlapping characters between chunks parallel_workers int 32 Parallel ingestion workers reset_collection bool true Wipe collection on startup distance_metric string cosine Similarity metric: cosine , l2 , ip Retrieval Settings Key Type Default Description retrievers.hybrid_retriever.num_documents_to_retrieve int 5 Top-k documents per query retrievers.hybrid_retriever.bm25_weight float 0.6 BM25 keyword score weight retrievers.hybrid_retriever.semantic_weight float 0.4 Semantic similarity weight stemming.enabled bool false Enable Porter Stemmer for improved matching Note: use_hybrid_search is a dynamic runtime setting (managed via the configuration API), not a YAML config key. Sources data_manager: sources: links: input_lists: - miscellanea.list scraper: reset_data: true verify_urls: false enable_warnings: false selenium_scraper: enabled: false git: enabled: false sso: enabled: false jira: url: https://jira.example.com projects: [] anonymize_data: true cutoff_date: null redmine: url: https://redmine.example.com project: null anonymize_data: true The visible flag on any source ( sources.<name>.visible ) controls whether content appears in chat citations (default: true ). Embedding Configuration data_manager: embedding_name: OpenAIEmbeddings embedding_class_map: OpenAIEmbeddings: class: OpenAIEmbeddings kwargs: model: text-embedding-3-small similarity_score_reference: 10 See Models & Providers for all embedding options. Anonymizer data_manager: utils: anonymizer: nlp_model: en_core_web_sm excluded_words: [] greeting_patterns: [] signoff_patterns: [] email_pattern: '[\\w\\.-]+@[\\w\\.-]+\\.\\w+' username_pattern: '\\[~[^\\]]+\\]' Agent Configuration Model Archi no longer uses a top-level archi: block in standard deployment YAML. Agent behavior is defined by: services.chat_app.agent_class : which pipeline class runs (for example CMSCompOpsAgent ) services.chat_app.agents_dir : where agent spec markdown files live agent specs ( *.md ): selected tool subset ( tools ) and system prompt body services.chat_app.tools : optional agent-class-specific tool settings Example: services: chat_app: agent_class: CMSCompOpsAgent agents_dir: examples/agents tools: monit: url: https://monit-grafana.cern.ch See Agents & Tools for agent spec format and tool selection. Complete Example name: my_deployment global: DATA_PATH: \"/root/data/\" ACCEPTED_FILES: [\".txt\", \".pdf\", \".md\"] verbosity: 3 services: chat_app: agent_class: CMSCompOpsAgent agents_dir: examples/agents default_provider: local default_model: llama3.2 trained_on: \"Course documentation\" hostname: \"example.mit.edu\" external_port: 7861 providers: local: enabled: true base_url: http://localhost:11434 mode: ollama models: - llama3.2 postgres: port: 5432 database: archi-db vectorstore: backend: postgres data_manager: sources: links: input_lists: - examples/deployments/basic-gpu/miscellanea.list scraper: reset_data: true verify_urls: false embedding_name: OpenAIEmbeddings chunk_size: 1000 chunk_overlap: 0 Tip: For the full base template with all defaults, see src/cli/templates/base-config.yaml in the repository.","title":"Configuration"},{"location":"configuration/#configuration-reference","text":"Archi deployments are configured via YAML files passed to the CLI with --config . Any fields not specified are populated from the base template at src/cli/templates/base-config.yaml . Tip: Start from one of the example configs in examples/deployments/ and customize from there.","title":"Configuration Reference"},{"location":"configuration/#top-level-fields","text":"","title":"Top-Level Fields"},{"location":"configuration/#name","text":"Type: string (required) Name of your deployment. Used for container naming and directory structure. name: my_deployment","title":"name"},{"location":"configuration/#global","text":"Global settings shared across all services. Key Type Default Description DATA_PATH string /root/data/ Path for persisted data inside containers ACCOUNTS_PATH string /root/.accounts/ Path for uploader/grader account data ACCEPTED_FILES list See below File extensions allowed for manual uploads LOGGING.input_output_filename string chain_input_output.log Pipeline I/O log filename verbosity int 3 Default logging level for services (0-4) Default accepted files: .pdf , .md , .txt , .docx , .html , .htm , .json , .yaml , .yml , .py , .js , .ts , .jsx , .tsx , .java , .go , .rs , .c , .cpp , .h , .sh","title":"global"},{"location":"configuration/#services","text":"Configuration for containerized services. Each service has its own subsection.","title":"services"},{"location":"configuration/#serviceschat_app","text":"The main chat interface. Key Type Default Description agent_class string CMSCompOpsAgent Pipeline class to run agents_dir string \u2014 Path to agent markdown files default_provider string local Default LLM provider default_model string llama3.2 Default model tools dict {} Agent-class-specific tool settings (for example tools.monit.url ) trained_on string \u2014 Description shown in the chat UI hostname string localhost Public hostname for the chat interface port int 7861 Internal container port external_port int 7861 Host-mapped port host string 0.0.0.0 Network binding num_responses_until_feedback int 3 Responses before prompting for feedback auth.enabled bool false Enable authentication","title":"services.chat_app"},{"location":"configuration/#provider-configuration","text":"services: chat_app: providers: local: enabled: true base_url: http://localhost:11434 mode: ollama # or openai_compat default_model: llama3.2 models: - llama3.2 gemini: enabled: true","title":"Provider Configuration"},{"location":"configuration/#servicespostgres","text":"PostgreSQL database settings. Key Type Default Description host string postgres Database hostname port int 5432 Database port user string archi Database user database string archi-db Database name","title":"services.postgres"},{"location":"configuration/#servicesvectorstore","text":"Key Type Default Description backend string postgres Vector store backend (only postgres supported)","title":"services.vectorstore"},{"location":"configuration/#servicesdata_manager","text":"Key Type Default Description port int 7871 Internal port external_port int 7871 Host-mapped port host string 0.0.0.0 Network binding enabled bool true Enable data manager service","title":"services.data_manager"},{"location":"configuration/#servicesgrafana","text":"Key Type Default Description port int 3000 Grafana port external_port int 3000 Host-mapped port","title":"services.grafana"},{"location":"configuration/#servicesgrader_app","text":"Key Type Default Description port int 7861 Internal port external_port int 7862 Host-mapped port provider string \u2014 Provider for grading pipelines model string \u2014 Model for grading pipelines num_problems int \u2014 Number of problems (must match rubric files) local_rubric_dir string \u2014 Path to rubric files local_users_csv_dir string \u2014 Path to users CSV","title":"services.grader_app"},{"location":"configuration/#other-services","text":"services.piazza : Requires network_id , agent_class , provider , model services.mattermost : Requires update_time services.redmine_mailbox : Requires url , project , redmine_update_time , mailbox_update_time services.benchmarking : See Benchmarking","title":"Other Services"},{"location":"configuration/#data_manager","text":"Controls data ingestion, vectorstore behaviour, and retrieval settings.","title":"data_manager"},{"location":"configuration/#core-settings","text":"Key Type Default Description collection_name string default_collection Vector store collection name embedding_name string OpenAIEmbeddings Embedding backend chunk_size int 1000 Max characters per text chunk chunk_overlap int 0 Overlapping characters between chunks parallel_workers int 32 Parallel ingestion workers reset_collection bool true Wipe collection on startup distance_metric string cosine Similarity metric: cosine , l2 , ip","title":"Core Settings"},{"location":"configuration/#retrieval-settings","text":"Key Type Default Description retrievers.hybrid_retriever.num_documents_to_retrieve int 5 Top-k documents per query retrievers.hybrid_retriever.bm25_weight float 0.6 BM25 keyword score weight retrievers.hybrid_retriever.semantic_weight float 0.4 Semantic similarity weight stemming.enabled bool false Enable Porter Stemmer for improved matching Note: use_hybrid_search is a dynamic runtime setting (managed via the configuration API), not a YAML config key.","title":"Retrieval Settings"},{"location":"configuration/#sources","text":"data_manager: sources: links: input_lists: - miscellanea.list scraper: reset_data: true verify_urls: false enable_warnings: false selenium_scraper: enabled: false git: enabled: false sso: enabled: false jira: url: https://jira.example.com projects: [] anonymize_data: true cutoff_date: null redmine: url: https://redmine.example.com project: null anonymize_data: true The visible flag on any source ( sources.<name>.visible ) controls whether content appears in chat citations (default: true ).","title":"Sources"},{"location":"configuration/#embedding-configuration","text":"data_manager: embedding_name: OpenAIEmbeddings embedding_class_map: OpenAIEmbeddings: class: OpenAIEmbeddings kwargs: model: text-embedding-3-small similarity_score_reference: 10 See Models & Providers for all embedding options.","title":"Embedding Configuration"},{"location":"configuration/#anonymizer","text":"data_manager: utils: anonymizer: nlp_model: en_core_web_sm excluded_words: [] greeting_patterns: [] signoff_patterns: [] email_pattern: '[\\w\\.-]+@[\\w\\.-]+\\.\\w+' username_pattern: '\\[~[^\\]]+\\]'","title":"Anonymizer"},{"location":"configuration/#agent-configuration-model","text":"Archi no longer uses a top-level archi: block in standard deployment YAML. Agent behavior is defined by: services.chat_app.agent_class : which pipeline class runs (for example CMSCompOpsAgent ) services.chat_app.agents_dir : where agent spec markdown files live agent specs ( *.md ): selected tool subset ( tools ) and system prompt body services.chat_app.tools : optional agent-class-specific tool settings Example: services: chat_app: agent_class: CMSCompOpsAgent agents_dir: examples/agents tools: monit: url: https://monit-grafana.cern.ch See Agents & Tools for agent spec format and tool selection.","title":"Agent Configuration Model"},{"location":"configuration/#complete-example","text":"name: my_deployment global: DATA_PATH: \"/root/data/\" ACCEPTED_FILES: [\".txt\", \".pdf\", \".md\"] verbosity: 3 services: chat_app: agent_class: CMSCompOpsAgent agents_dir: examples/agents default_provider: local default_model: llama3.2 trained_on: \"Course documentation\" hostname: \"example.mit.edu\" external_port: 7861 providers: local: enabled: true base_url: http://localhost:11434 mode: ollama models: - llama3.2 postgres: port: 5432 database: archi-db vectorstore: backend: postgres data_manager: sources: links: input_lists: - examples/deployments/basic-gpu/miscellanea.list scraper: reset_data: true verify_urls: false embedding_name: OpenAIEmbeddings chunk_size: 1000 chunk_overlap: 0 Tip: For the full base template with all defaults, see src/cli/templates/base-config.yaml in the repository.","title":"Complete Example"},{"location":"data_sources/","text":"Data Sources Archi ingests content from a variety of data sources into the PostgreSQL-backed vector store used for document retrieval. Sources are configured in data_manager.sources in your YAML configuration. Note: The links source is always enabled by default \u2014 you do not need to pass it explicitly. Web Link Lists A web link list is a simple text file containing one URL per line. Archi fetches the content from each URL and adds it to the vector store using the Scraper class. Configuration Define which link lists to ingest in your configuration file: data_manager: sources: links: input_lists: - miscellanea.list - additional_urls.list Each .list file contains one URL per line: https://example.com/page1 https://example.com/page2 Customizing the Scraper You can tune HTTP scraping behaviour: data_manager: sources: links: scraper: reset_data: true verify_urls: false enable_warnings: false SSO-Protected Links If some links are behind a Single Sign-On (SSO) system, enable the SSO source and configure the Selenium-based collector: data_manager: sources: sso: enabled: true links: selenium_scraper: enabled: true selenium_class: CERNSSOScraper selenium_class_map: CERNSSOScraper: kwargs: headless: true max_depth: 2 With sso.enabled: true , prefix protected URLs with sso- : sso-https://example.com/protected/page Secrets: SSO_USERNAME=username SSO_PASSWORD=password Running Link scraping is controlled by your config ( data_manager.sources.links.enabled ). Git Scraping Ingest content from MkDocs-based git repositories using the GitScraper class, which extracts Markdown content directly instead of scraping rendered HTML. Configuration data_manager: sources: git: enabled: true In your link lists, prefix repository URLs with git- : git-https://github.com/example/mkdocs/documentation.git Secrets GIT_USERNAME=your_username GIT_TOKEN=your_token Once enabled in config, deploy normally with archi create --config <config.yaml> --services <...> . JIRA Fetch issues and comments from specified JIRA projects using the JiraClient class. Configuration data_manager: sources: jira: url: https://jira.example.com projects: - PROJECT_KEY anonymize_data: true cutoff_date: \"2023-01-01\" The optional cutoff_date skips tickets created before the specified ISO-8601 date. Anonymization Customize data anonymization to remove personal information: data_manager: utils: anonymizer: nlp_model: en_core_web_sm excluded_words: - Example greeting_patterns: - '^(hi|hello|hey|greetings|dear)\\b' signoff_patterns: - '\\b(regards|sincerely|best regards|cheers|thank you)\\b' email_pattern: '[\\w\\.-]+@[\\w\\.-]+\\.\\w+' username_pattern: '\\[~[^\\]]+\\]' Secrets JIRA_PAT=<your_jira_personal_access_token> Once enabled in config, deploy normally with archi create --config <config.yaml> --services <...> . Redmine Ingest solved tickets (question/answer pairs) from Redmine into the vector store. Configuration data_manager: sources: redmine: url: https://redmine.example.com project: my-project anonymize_data: true Secrets REDMINE_USER=... REDMINE_PW=... Once enabled in config, deploy normally with archi create --config <config.yaml> --services <...> . To automate email replies to resolved tickets, also enable the redmine-mailer service. See Services . Adding Documents Manually Document Upload (via Chat UI) The chatbot service includes a built-in document upload interface. When logged in to the chat UI, navigate to /upload to upload documents through your browser. First-time setup \u2014 create an admin account: docker exec -it <CONTAINER-ID> bash python -u src/bin/service_create_account.py Run the script from the /root/archi directory inside the container. After creating an account, visit the chat UI to log in and upload documents. Directly Copying Files Documents used for RAG live in the container at /root/data/<directory>/ . You can copy files directly: docker cp myfile.pdf <container-id>:/root/data/my_docs/ To create a new directory inside the container: docker exec -it <container-id> mkdir /root/data/my_docs Data Viewer The chat interface includes a built-in Data Viewer for browsing and managing ingested documents. Access it at /data on your chat app (e.g., http://localhost:7861/data ). Features Browse documents : View all ingested documents with metadata (source, file type, chunk count) Search and filter : Filter documents by name or source type View content : Click a document to see its full content and individual chunks Enable/disable documents : Toggle whether specific documents are included in RAG retrieval Bulk operations : Enable or disable multiple documents at once Document States State Description Enabled Document chunks are included in retrieval (default) Disabled Document is excluded from retrieval but remains in the database Disabling documents is useful for temporarily excluding outdated content, testing retrieval with specific document subsets, or hiding sensitive documents from certain users. Source Configuration Notes Source configuration is persisted to PostgreSQL ( static_config.sources_config ) at deployment time and used at runtime. The visible flag on a source ( sources.<name>.visible ) controls whether content from that source appears in chat citations and user-facing listings. It defaults to true . All sources can be listed with archi list-services .","title":"Data Sources"},{"location":"data_sources/#data-sources","text":"Archi ingests content from a variety of data sources into the PostgreSQL-backed vector store used for document retrieval. Sources are configured in data_manager.sources in your YAML configuration. Note: The links source is always enabled by default \u2014 you do not need to pass it explicitly.","title":"Data Sources"},{"location":"data_sources/#web-link-lists","text":"A web link list is a simple text file containing one URL per line. Archi fetches the content from each URL and adds it to the vector store using the Scraper class.","title":"Web Link Lists"},{"location":"data_sources/#configuration","text":"Define which link lists to ingest in your configuration file: data_manager: sources: links: input_lists: - miscellanea.list - additional_urls.list Each .list file contains one URL per line: https://example.com/page1 https://example.com/page2","title":"Configuration"},{"location":"data_sources/#customizing-the-scraper","text":"You can tune HTTP scraping behaviour: data_manager: sources: links: scraper: reset_data: true verify_urls: false enable_warnings: false","title":"Customizing the Scraper"},{"location":"data_sources/#sso-protected-links","text":"If some links are behind a Single Sign-On (SSO) system, enable the SSO source and configure the Selenium-based collector: data_manager: sources: sso: enabled: true links: selenium_scraper: enabled: true selenium_class: CERNSSOScraper selenium_class_map: CERNSSOScraper: kwargs: headless: true max_depth: 2 With sso.enabled: true , prefix protected URLs with sso- : sso-https://example.com/protected/page Secrets: SSO_USERNAME=username SSO_PASSWORD=password","title":"SSO-Protected Links"},{"location":"data_sources/#running","text":"Link scraping is controlled by your config ( data_manager.sources.links.enabled ).","title":"Running"},{"location":"data_sources/#git-scraping","text":"Ingest content from MkDocs-based git repositories using the GitScraper class, which extracts Markdown content directly instead of scraping rendered HTML.","title":"Git Scraping"},{"location":"data_sources/#configuration_1","text":"data_manager: sources: git: enabled: true In your link lists, prefix repository URLs with git- : git-https://github.com/example/mkdocs/documentation.git","title":"Configuration"},{"location":"data_sources/#secrets","text":"GIT_USERNAME=your_username GIT_TOKEN=your_token Once enabled in config, deploy normally with archi create --config <config.yaml> --services <...> .","title":"Secrets"},{"location":"data_sources/#jira","text":"Fetch issues and comments from specified JIRA projects using the JiraClient class.","title":"JIRA"},{"location":"data_sources/#configuration_2","text":"data_manager: sources: jira: url: https://jira.example.com projects: - PROJECT_KEY anonymize_data: true cutoff_date: \"2023-01-01\" The optional cutoff_date skips tickets created before the specified ISO-8601 date.","title":"Configuration"},{"location":"data_sources/#anonymization","text":"Customize data anonymization to remove personal information: data_manager: utils: anonymizer: nlp_model: en_core_web_sm excluded_words: - Example greeting_patterns: - '^(hi|hello|hey|greetings|dear)\\b' signoff_patterns: - '\\b(regards|sincerely|best regards|cheers|thank you)\\b' email_pattern: '[\\w\\.-]+@[\\w\\.-]+\\.\\w+' username_pattern: '\\[~[^\\]]+\\]'","title":"Anonymization"},{"location":"data_sources/#secrets_1","text":"JIRA_PAT=<your_jira_personal_access_token> Once enabled in config, deploy normally with archi create --config <config.yaml> --services <...> .","title":"Secrets"},{"location":"data_sources/#redmine","text":"Ingest solved tickets (question/answer pairs) from Redmine into the vector store.","title":"Redmine"},{"location":"data_sources/#configuration_3","text":"data_manager: sources: redmine: url: https://redmine.example.com project: my-project anonymize_data: true","title":"Configuration"},{"location":"data_sources/#secrets_2","text":"REDMINE_USER=... REDMINE_PW=... Once enabled in config, deploy normally with archi create --config <config.yaml> --services <...> . To automate email replies to resolved tickets, also enable the redmine-mailer service. See Services .","title":"Secrets"},{"location":"data_sources/#adding-documents-manually","text":"","title":"Adding Documents Manually"},{"location":"data_sources/#document-upload-via-chat-ui","text":"The chatbot service includes a built-in document upload interface. When logged in to the chat UI, navigate to /upload to upload documents through your browser. First-time setup \u2014 create an admin account: docker exec -it <CONTAINER-ID> bash python -u src/bin/service_create_account.py Run the script from the /root/archi directory inside the container. After creating an account, visit the chat UI to log in and upload documents.","title":"Document Upload (via Chat UI)"},{"location":"data_sources/#directly-copying-files","text":"Documents used for RAG live in the container at /root/data/<directory>/ . You can copy files directly: docker cp myfile.pdf <container-id>:/root/data/my_docs/ To create a new directory inside the container: docker exec -it <container-id> mkdir /root/data/my_docs","title":"Directly Copying Files"},{"location":"data_sources/#data-viewer","text":"The chat interface includes a built-in Data Viewer for browsing and managing ingested documents. Access it at /data on your chat app (e.g., http://localhost:7861/data ).","title":"Data Viewer"},{"location":"data_sources/#features","text":"Browse documents : View all ingested documents with metadata (source, file type, chunk count) Search and filter : Filter documents by name or source type View content : Click a document to see its full content and individual chunks Enable/disable documents : Toggle whether specific documents are included in RAG retrieval Bulk operations : Enable or disable multiple documents at once","title":"Features"},{"location":"data_sources/#document-states","text":"State Description Enabled Document chunks are included in retrieval (default) Disabled Document is excluded from retrieval but remains in the database Disabling documents is useful for temporarily excluding outdated content, testing retrieval with specific document subsets, or hiding sensitive documents from certain users.","title":"Document States"},{"location":"data_sources/#source-configuration-notes","text":"Source configuration is persisted to PostgreSQL ( static_config.sources_config ) at deployment time and used at runtime. The visible flag on a source ( sources.<name>.visible ) controls whether content from that source appears in chat citations and user-facing listings. It defaults to true . All sources can be listed with archi list-services .","title":"Source Configuration Notes"},{"location":"developer_guide/","text":"Developers Guide Below is all the information developers may need to get started contributing to the Archi project. Architecture Overview Archi is a containerized RAG framework with these core components: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 CLI (archi create) \u2502 \u2502 Renders configs, builds images, launches containers \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Chat Service \u2502 \u2502 Data Manager \u2502 \u2502 (Flask, port 7861)\u2502 \u2502 (Flask, port 7871) \u2502 \u2502 - ReAct agent \u2502 \u2502 - Collectors \u2502 \u2502 - Provider layer \u2502 \u2502 - PersistenceService\u2502 \u2502 - Auth, BYOK \u2502 \u2502 - VectorStoreManager\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 PostgreSQL (pgvector) \u2502 \u2502 Tables: resources, document_chunks, conversations, \u2502 \u2502 conversation_metadata, feedback, timing, \u2502 \u2502 agent_tool_calls, users, keys \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Key source directories: Directory Purpose src/archi/ Core orchestration \u2014 agent base class, pipelines, providers src/bin/ Service entrypoints (Flask app factories) src/cli/ CLI commands, service/source registries, deployment manager src/data_manager/ Collectors, persistence, vectorstore indexing src/interfaces/ Flask blueprints for chat, uploader, data viewer src/utils/ Config loader, logging, shared utilities Provider Architecture All LLM providers extend BaseProvider ( src/archi/providers/base_provider.py ): class BaseProvider(ABC): @abstractmethod def get_model(self, model_name, **kwargs) -> BaseChatModel: ... @abstractmethod def get_embedding_model(self, model_name, **kwargs) -> Embeddings: ... @abstractmethod def list_models(self) -> list[str]: ... Providers register themselves via ProviderType enum and are registered through register_provider() . Factory functions in src/archi/providers/__init__.py : get_provider(provider_type) \u2014 returns a provider instance get_model(provider_type, model_name) \u2014 returns a LangChain BaseChatModel get_provider_with_api_key(provider_type, api_key) \u2014 for BYOK Built-in providers: Provider Module Models OpenAIProvider openai_provider.py gpt-4o, gpt-4o-mini, etc. AnthropicProvider anthropic_provider.py claude-sonnet-4-20250514, claude-3.5-haiku, etc. GeminiProvider gemini_provider.py gemini-2.0-flash, gemini-2.5-pro, etc. OpenRouterProvider openrouter_provider.py Any model via OpenRouter API LocalProvider local_provider.py Ollama or OpenAI-compatible local models Adding a New Provider Create src/archi/providers/my_provider.py extending BaseProvider . Add an entry to the ProviderType enum. Call register_provider(ProviderType.MY_PROVIDER, MyProvider) to register it. Implement get_model() , get_embedding_model() , and list_models() . Add config keys under services.chat_app.providers.my_provider . Agent & Pipeline Architecture The agent system is built around BaseReActAgent ( src/archi/archi.py , ~975 lines): Implements a ReAct (Reasoning + Acting) loop with tool calling Manages conversation history, streaming, and tool execution Loads agent specs from markdown files with YAML frontmatter AgentSpec ( src/archi/pipelines/agents/agent_spec.py ) is a dataclass: @dataclass class AgentSpec: name: str tools: list[str] prompt: str source_path: str Agent specs are discovered via list_agent_files() and loaded via load_agent_spec() . The select_agent_spec() function picks the correct spec given a name. Pipeline Classes CMSCompOpsAgent \u2014 default ReAct agent with 6 built-in tools (search_local_files, search_metadata_index, list_metadata_schema, fetch_catalog_document, search_vectorstore_hybrid, mcp) QAPipeline \u2014 simpler retrieval-augmented QA without tool calling Adding a New Tool Define the tool function in the agent class or as a LangChain @tool . Register it in the tool mapping within the agent's _build_tools() method. Add the tool name to agent spec YAML frontmatter tools list. Contribution Workflow Branch : Create a feature branch from main (e.g., dev/my-feature ). Develop : Follow PEP 8, use snake_case for functions, PascalCase for classes. Test : Run smoke tests locally (see below). Commit : Use short, lowercase commit summaries (e.g., add gemini provider ). PR : Include a brief summary, test results, and documentation impact. Link related issues. Editing Documentation Editing documentation requires the mkdocs Python package: pip install mkdocs To edit documentation, update the .md and .yml files in the ./docs folder. To preview changes locally, run: cd docs mkdocs serve Add the -a IP:HOST argument (default is localhost:8000 ) to specify the host and port. Publish your changes with: mkdocs gh-deploy Always open a PR to merge documentation changes into main . Do not edit files directly in the gh-pages branch. Smoke Tests If you want the full CI-like smoke run (create deployment, wait for readiness, run checks, and clean up) you can use the shared runner: export ARCHI_DIR=~/.archi export DEPLOYMENT_NAME=local-smoke export USE_PODMAN=false export SMOKE_FORCE_CREATE=true export SMOKE_OLLAMA_MODEL=qwen3:4b scripts/dev/run_smoke_preview.sh \"${DEPLOYMENT_NAME}\" The shared runner performs these checks in order (ensuring the configured Ollama model is available via ollama pull before running the checks): Create a deployment from the preview config and wait for the chat app health endpoint. Wait for initial data ingestion to complete (5 minute timeout). Preflight checks: Postgres reachable, data-manager catalog searchable. Tool probes: catalog tools and vectorstore retriever (executed inside the chatbot container to match the agent runtime). ReAct agent smoke: stream response and observe at least one tool call. The combined smoke workflow alone does not start Archi for you. Start a deployment first, then run the checks (it validates Postgres, data-manager catalog, Ollama model availability, ReAct streaming, and direct tool probes inside the chatbot container): export Archi_CONFIG_PATH=~/.archi/archi-<deployment-name>/configs/<config-name>.yaml export Archi_CONFIG_NAME=<config-name> export Archi_PIPELINE_NAME=CMSCompOpsAgent export USE_PODMAN=false export OLLAMA_MODEL=<ollama-model-name> export PGHOST=localhost export PGPORT=<postgres-port> export PGUSER=archi export PGPASSWORD=<pg-password> export PGDATABASE=archi-db export BASE_URL=http://localhost:2786 export DM_BASE_URL=http://localhost:<data-manager-port> # from your deployment config export OLLAMA_URL=http://localhost:11434 ./tests/smoke/combined_smoke.sh <deployment-name> Optional environment variables for deterministic queries: export REACT_SMOKE_PROMPT=\"Use the search_local_files tool to find ... and summarize.\" export FILE_SEARCH_QUERY=\"first linux server installation\" export METADATA_SEARCH_QUERY=\"ppc.mit.edu\" export VECTORSTORE_QUERY=\"cms\" CI / CD Architecture All CI workflows run on GitHub-hosted ubuntu-latest runners with Docker (not Podman). PR Preview ( pr-preview.yml ) Every pull request triggers four parallel/sequential jobs: Job Runner Purpose lint ubuntu-latest black --check . and isort --check . unit-tests ubuntu-latest pytest tests/unit/ -v --tb=short build-base-images ubuntu-latest Detects changes to base image inputs; builds if needed preview ubuntu-latest Smoke deployment + Playwright UI tests The preview job: Installs Ollama and pulls qwen3:4b (~2.6GB). Builds and deploys the app via archi create with Docker. Runs integration smoke tests ( combined_smoke.sh ). Runs Playwright UI tests against http://localhost:2786 . Release ( test-and-build-tag.yml ) Manually dispatched; builds Docker base images, pushes to DockerHub, runs smoke tests, then tags and releases. Publish Base Images ( publish-base-images.yml ) Triggered on push to main ; rebuilds and pushes base images when requirements or Dockerfiles change. Docker Layer Caching All workflows that build Docker images use docker/setup-buildx-action with actions/cache for layer caching, reducing rebuild times on cache hits. Local Development For local smoke testing, Docker is the default container runtime ( USE_PODMAN=false ). To use Podman locally, set USE_PODMAN=true and use the --podman flag with archi CLI commands. Postgres Usage Overview Archi relies on Postgres as the durable metadata store across services. Core usage falls into two buckets: Ingestion catalog : the resources table tracks persisted files and metadata for the data manager catalog ( CatalogService ). Conversation history : the conversation_metadata and conversations tables store chat/session metadata plus message history for interfaces like the chat app and ticketing integrations (e.g., Redmine mailer). The conversations table tracks: - model_used (string) - The model that generated the response (e.g., \"openai/gpt-4o\") - pipeline_used (string) - The pipeline that processed the request (e.g., \"QAPipeline\") Additional supporting tables store interaction telemetry and feedback: feedback captures like/dislike/comment feedback tied to conversation messages. timing tracks per-message latency milestones. agent_tool_calls stores tool call inputs/outputs extracted from agent messages. When extending an interface that writes to conversations , make sure a matching conversation_metadata row exists (create or update before inserting messages) to satisfy foreign key constraints. DockerHub Images Archi loads different base images hosted on Docker Hub. The Python base image is used when GPUs are not required; otherwise the PyTorch base image is used. The Dockerfiles for these base images live in src/cli/templates/dockerfiles/base-X-image . Images are hosted at: Python: https://hub.docker.com/r/a2rchi/a2rchi-python-base PyTorch: https://hub.docker.com/r/a2rchi/a2rchi-pytorch-base To rebuild a base image, navigate to the relevant base-xxx-image directory under src/cli/templates/dockerfiles/ . Each directory contains the Dockerfile, requirements, and license information. Regenerate the requirements files with: # Python image cat requirements/cpu-requirementsHEADER.txt requirements/requirements-base.txt > src/cli/templates/dockerfiles/base-python-image/requirements.txt # PyTorch image cat requirements/gpu-requirementsHEADER.txt requirements/requirements-base.txt > src/cli/templates/dockerfiles/base-pytorch-image/requirements.txt Build the image: podman build -t a2rchi/<image-name>:<tag> . After verifying the image, log in to Docker Hub (ask a senior developer for credentials): podman login docker.io Push the image: podman push a2rchi/<image-name>:<tag> Data Ingestion Architecture Archi ingests content through sources which are collected by collectors ( data_manager/collectors ). These documents are written to persistent, local files via the PersistenceService , which uses Resource objects as an abstraction for different content types, and ResourceMetadata for associated metadata. A catalog of persisted files and metadata is maintained in Postgres via CatalogService (table: resources ). Finally, the VectorStoreManager reads these files, splits them into chunks, generates embeddings, and indexes them in PostgreSQL with pgvector. Resources and BaseResource Every collected artifact from the collectors is represented as a subclass of BaseResource ( src/data_manager/collectors/resource_base.py ). Subclasses must implement: get_hash() : a stable identifier used as the key in the filesystem catalog. get_filename() : the on-disk file name (including extension). get_content() : returns the textual or binary payload that should be persisted. Resources may optionally override: get_metadata() : returns a metadata object (typically ResourceMetadata ) describing the item. Keys should be serialisable strings and are flattened into the vector store metadata. get_metadata_path() : legacy helper for .meta.yaml paths (metadata is now stored in Postgres). ResourceMetadata ( src/data_manager/collectors/utils/metadata.py ) enforces a required file_name and normalises the extra dictionary so all values become strings. Optional UI labels like display_name live in extra , alongside source-specific information such as URLs, ticket identifiers, or visibility flags. The guiding philosophy is that resources describe content , but never write to disk themselves. This separation keeps collectors simple, testable, and ensures consistent validation when persisting different resource types. Persistence Service PersistenceService ( src/data_manager/collectors/persistence.py ) centralises all filesystem writes for document content and metadata catalog updates. When persist_resource() is called it: Resolves the target path under the configured DATA_PATH . Validates and writes the resource content (rejecting empty payloads or unknown types). Normalises metadata (if provided) for storage. Upserts a row into the Postgres resources catalog with file and metadata fields. Collectors only interact with PersistenceService ; they should not touch the filesystem directly. Vector Database The vector store lives under the data_manager/vectorstore package. VectorStoreManager reads the Postgres catalog and manages embeddings in PostgreSQL: Loads the tracked files and metadata hashes from the Postgres catalog. Splits documents into chunks, optional stemming, and builds embeddings via the configured model. Adds chunks to the document_chunks table with embeddings and flattened metadata (including resource hash, filename, human-readable display fields, and any source-specific extras). Deletes stale entries when the underlying files disappear or are superseded. Because the manager defers to the catalog, any resource persisted through PersistenceService automatically becomes eligible for indexing\u2014no extra plumbing is required. Catalog Verification Checklist Confirm the Postgres resources table exists and is reachable from the service containers. Ingest or upload a new document and verify a new row appears in resources . Verify VectorStoreManager can update the collection using the Postgres catalog. Extending the Stack Adding a New Data Source When integrating a new source, create a collector under data_manager/collectors . Collectors should yield Resource objects. A new Resource subclass is only needed if the content type is not already represented (e.g., text, HTML, markdown, images, etc.), but it must implement the required methods described above. When integrating a new collector, ensure that any per-source configuration is encoded in the resource metadata so downstream consumers\u2014such as the chat app\u2014can honour it. Adding a New Service Create a Flask blueprint under src/interfaces/ . Register the service in src/cli/service_registry.py with its name, port, and dependencies. Add a service entrypoint in src/bin/ . Add any service-specific config keys under services.<name> in the base config template. Extending Embeddings or Storage When extending the embedding pipeline or storage schema, keep this flow in mind: collectors produce resources \u2192 PersistenceService writes files and updates the Postgres catalog \u2192 VectorStoreManager indexes embeddings in PostgreSQL. Keeping responsibilities narrowly scoped makes the ingestion stack easier to reason about and evolve.","title":"Developer Guide"},{"location":"developer_guide/#developers-guide","text":"Below is all the information developers may need to get started contributing to the Archi project.","title":"Developers Guide"},{"location":"developer_guide/#architecture-overview","text":"Archi is a containerized RAG framework with these core components: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 CLI (archi create) \u2502 \u2502 Renders configs, builds images, launches containers \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Chat Service \u2502 \u2502 Data Manager \u2502 \u2502 (Flask, port 7861)\u2502 \u2502 (Flask, port 7871) \u2502 \u2502 - ReAct agent \u2502 \u2502 - Collectors \u2502 \u2502 - Provider layer \u2502 \u2502 - PersistenceService\u2502 \u2502 - Auth, BYOK \u2502 \u2502 - VectorStoreManager\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 PostgreSQL (pgvector) \u2502 \u2502 Tables: resources, document_chunks, conversations, \u2502 \u2502 conversation_metadata, feedback, timing, \u2502 \u2502 agent_tool_calls, users, keys \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Key source directories: Directory Purpose src/archi/ Core orchestration \u2014 agent base class, pipelines, providers src/bin/ Service entrypoints (Flask app factories) src/cli/ CLI commands, service/source registries, deployment manager src/data_manager/ Collectors, persistence, vectorstore indexing src/interfaces/ Flask blueprints for chat, uploader, data viewer src/utils/ Config loader, logging, shared utilities","title":"Architecture Overview"},{"location":"developer_guide/#provider-architecture","text":"All LLM providers extend BaseProvider ( src/archi/providers/base_provider.py ): class BaseProvider(ABC): @abstractmethod def get_model(self, model_name, **kwargs) -> BaseChatModel: ... @abstractmethod def get_embedding_model(self, model_name, **kwargs) -> Embeddings: ... @abstractmethod def list_models(self) -> list[str]: ... Providers register themselves via ProviderType enum and are registered through register_provider() . Factory functions in src/archi/providers/__init__.py : get_provider(provider_type) \u2014 returns a provider instance get_model(provider_type, model_name) \u2014 returns a LangChain BaseChatModel get_provider_with_api_key(provider_type, api_key) \u2014 for BYOK Built-in providers: Provider Module Models OpenAIProvider openai_provider.py gpt-4o, gpt-4o-mini, etc. AnthropicProvider anthropic_provider.py claude-sonnet-4-20250514, claude-3.5-haiku, etc. GeminiProvider gemini_provider.py gemini-2.0-flash, gemini-2.5-pro, etc. OpenRouterProvider openrouter_provider.py Any model via OpenRouter API LocalProvider local_provider.py Ollama or OpenAI-compatible local models","title":"Provider Architecture"},{"location":"developer_guide/#adding-a-new-provider","text":"Create src/archi/providers/my_provider.py extending BaseProvider . Add an entry to the ProviderType enum. Call register_provider(ProviderType.MY_PROVIDER, MyProvider) to register it. Implement get_model() , get_embedding_model() , and list_models() . Add config keys under services.chat_app.providers.my_provider .","title":"Adding a New Provider"},{"location":"developer_guide/#agent-pipeline-architecture","text":"The agent system is built around BaseReActAgent ( src/archi/archi.py , ~975 lines): Implements a ReAct (Reasoning + Acting) loop with tool calling Manages conversation history, streaming, and tool execution Loads agent specs from markdown files with YAML frontmatter AgentSpec ( src/archi/pipelines/agents/agent_spec.py ) is a dataclass: @dataclass class AgentSpec: name: str tools: list[str] prompt: str source_path: str Agent specs are discovered via list_agent_files() and loaded via load_agent_spec() . The select_agent_spec() function picks the correct spec given a name.","title":"Agent &amp; Pipeline Architecture"},{"location":"developer_guide/#pipeline-classes","text":"CMSCompOpsAgent \u2014 default ReAct agent with 6 built-in tools (search_local_files, search_metadata_index, list_metadata_schema, fetch_catalog_document, search_vectorstore_hybrid, mcp) QAPipeline \u2014 simpler retrieval-augmented QA without tool calling","title":"Pipeline Classes"},{"location":"developer_guide/#adding-a-new-tool","text":"Define the tool function in the agent class or as a LangChain @tool . Register it in the tool mapping within the agent's _build_tools() method. Add the tool name to agent spec YAML frontmatter tools list.","title":"Adding a New Tool"},{"location":"developer_guide/#contribution-workflow","text":"Branch : Create a feature branch from main (e.g., dev/my-feature ). Develop : Follow PEP 8, use snake_case for functions, PascalCase for classes. Test : Run smoke tests locally (see below). Commit : Use short, lowercase commit summaries (e.g., add gemini provider ). PR : Include a brief summary, test results, and documentation impact. Link related issues.","title":"Contribution Workflow"},{"location":"developer_guide/#editing-documentation","text":"Editing documentation requires the mkdocs Python package: pip install mkdocs To edit documentation, update the .md and .yml files in the ./docs folder. To preview changes locally, run: cd docs mkdocs serve Add the -a IP:HOST argument (default is localhost:8000 ) to specify the host and port. Publish your changes with: mkdocs gh-deploy Always open a PR to merge documentation changes into main . Do not edit files directly in the gh-pages branch.","title":"Editing Documentation"},{"location":"developer_guide/#smoke-tests","text":"If you want the full CI-like smoke run (create deployment, wait for readiness, run checks, and clean up) you can use the shared runner: export ARCHI_DIR=~/.archi export DEPLOYMENT_NAME=local-smoke export USE_PODMAN=false export SMOKE_FORCE_CREATE=true export SMOKE_OLLAMA_MODEL=qwen3:4b scripts/dev/run_smoke_preview.sh \"${DEPLOYMENT_NAME}\" The shared runner performs these checks in order (ensuring the configured Ollama model is available via ollama pull before running the checks): Create a deployment from the preview config and wait for the chat app health endpoint. Wait for initial data ingestion to complete (5 minute timeout). Preflight checks: Postgres reachable, data-manager catalog searchable. Tool probes: catalog tools and vectorstore retriever (executed inside the chatbot container to match the agent runtime). ReAct agent smoke: stream response and observe at least one tool call. The combined smoke workflow alone does not start Archi for you. Start a deployment first, then run the checks (it validates Postgres, data-manager catalog, Ollama model availability, ReAct streaming, and direct tool probes inside the chatbot container): export Archi_CONFIG_PATH=~/.archi/archi-<deployment-name>/configs/<config-name>.yaml export Archi_CONFIG_NAME=<config-name> export Archi_PIPELINE_NAME=CMSCompOpsAgent export USE_PODMAN=false export OLLAMA_MODEL=<ollama-model-name> export PGHOST=localhost export PGPORT=<postgres-port> export PGUSER=archi export PGPASSWORD=<pg-password> export PGDATABASE=archi-db export BASE_URL=http://localhost:2786 export DM_BASE_URL=http://localhost:<data-manager-port> # from your deployment config export OLLAMA_URL=http://localhost:11434 ./tests/smoke/combined_smoke.sh <deployment-name> Optional environment variables for deterministic queries: export REACT_SMOKE_PROMPT=\"Use the search_local_files tool to find ... and summarize.\" export FILE_SEARCH_QUERY=\"first linux server installation\" export METADATA_SEARCH_QUERY=\"ppc.mit.edu\" export VECTORSTORE_QUERY=\"cms\"","title":"Smoke Tests"},{"location":"developer_guide/#ci-cd-architecture","text":"All CI workflows run on GitHub-hosted ubuntu-latest runners with Docker (not Podman).","title":"CI / CD Architecture"},{"location":"developer_guide/#pr-preview-pr-previewyml","text":"Every pull request triggers four parallel/sequential jobs: Job Runner Purpose lint ubuntu-latest black --check . and isort --check . unit-tests ubuntu-latest pytest tests/unit/ -v --tb=short build-base-images ubuntu-latest Detects changes to base image inputs; builds if needed preview ubuntu-latest Smoke deployment + Playwright UI tests The preview job: Installs Ollama and pulls qwen3:4b (~2.6GB). Builds and deploys the app via archi create with Docker. Runs integration smoke tests ( combined_smoke.sh ). Runs Playwright UI tests against http://localhost:2786 .","title":"PR Preview (pr-preview.yml)"},{"location":"developer_guide/#release-test-and-build-tagyml","text":"Manually dispatched; builds Docker base images, pushes to DockerHub, runs smoke tests, then tags and releases.","title":"Release (test-and-build-tag.yml)"},{"location":"developer_guide/#publish-base-images-publish-base-imagesyml","text":"Triggered on push to main ; rebuilds and pushes base images when requirements or Dockerfiles change.","title":"Publish Base Images (publish-base-images.yml)"},{"location":"developer_guide/#docker-layer-caching","text":"All workflows that build Docker images use docker/setup-buildx-action with actions/cache for layer caching, reducing rebuild times on cache hits.","title":"Docker Layer Caching"},{"location":"developer_guide/#local-development","text":"For local smoke testing, Docker is the default container runtime ( USE_PODMAN=false ). To use Podman locally, set USE_PODMAN=true and use the --podman flag with archi CLI commands.","title":"Local Development"},{"location":"developer_guide/#postgres-usage-overview","text":"Archi relies on Postgres as the durable metadata store across services. Core usage falls into two buckets: Ingestion catalog : the resources table tracks persisted files and metadata for the data manager catalog ( CatalogService ). Conversation history : the conversation_metadata and conversations tables store chat/session metadata plus message history for interfaces like the chat app and ticketing integrations (e.g., Redmine mailer). The conversations table tracks: - model_used (string) - The model that generated the response (e.g., \"openai/gpt-4o\") - pipeline_used (string) - The pipeline that processed the request (e.g., \"QAPipeline\") Additional supporting tables store interaction telemetry and feedback: feedback captures like/dislike/comment feedback tied to conversation messages. timing tracks per-message latency milestones. agent_tool_calls stores tool call inputs/outputs extracted from agent messages. When extending an interface that writes to conversations , make sure a matching conversation_metadata row exists (create or update before inserting messages) to satisfy foreign key constraints.","title":"Postgres Usage Overview"},{"location":"developer_guide/#dockerhub-images","text":"Archi loads different base images hosted on Docker Hub. The Python base image is used when GPUs are not required; otherwise the PyTorch base image is used. The Dockerfiles for these base images live in src/cli/templates/dockerfiles/base-X-image . Images are hosted at: Python: https://hub.docker.com/r/a2rchi/a2rchi-python-base PyTorch: https://hub.docker.com/r/a2rchi/a2rchi-pytorch-base To rebuild a base image, navigate to the relevant base-xxx-image directory under src/cli/templates/dockerfiles/ . Each directory contains the Dockerfile, requirements, and license information. Regenerate the requirements files with: # Python image cat requirements/cpu-requirementsHEADER.txt requirements/requirements-base.txt > src/cli/templates/dockerfiles/base-python-image/requirements.txt # PyTorch image cat requirements/gpu-requirementsHEADER.txt requirements/requirements-base.txt > src/cli/templates/dockerfiles/base-pytorch-image/requirements.txt Build the image: podman build -t a2rchi/<image-name>:<tag> . After verifying the image, log in to Docker Hub (ask a senior developer for credentials): podman login docker.io Push the image: podman push a2rchi/<image-name>:<tag>","title":"DockerHub Images"},{"location":"developer_guide/#data-ingestion-architecture","text":"Archi ingests content through sources which are collected by collectors ( data_manager/collectors ). These documents are written to persistent, local files via the PersistenceService , which uses Resource objects as an abstraction for different content types, and ResourceMetadata for associated metadata. A catalog of persisted files and metadata is maintained in Postgres via CatalogService (table: resources ). Finally, the VectorStoreManager reads these files, splits them into chunks, generates embeddings, and indexes them in PostgreSQL with pgvector.","title":"Data Ingestion Architecture"},{"location":"developer_guide/#resources-and-baseresource","text":"Every collected artifact from the collectors is represented as a subclass of BaseResource ( src/data_manager/collectors/resource_base.py ). Subclasses must implement: get_hash() : a stable identifier used as the key in the filesystem catalog. get_filename() : the on-disk file name (including extension). get_content() : returns the textual or binary payload that should be persisted. Resources may optionally override: get_metadata() : returns a metadata object (typically ResourceMetadata ) describing the item. Keys should be serialisable strings and are flattened into the vector store metadata. get_metadata_path() : legacy helper for .meta.yaml paths (metadata is now stored in Postgres). ResourceMetadata ( src/data_manager/collectors/utils/metadata.py ) enforces a required file_name and normalises the extra dictionary so all values become strings. Optional UI labels like display_name live in extra , alongside source-specific information such as URLs, ticket identifiers, or visibility flags. The guiding philosophy is that resources describe content , but never write to disk themselves. This separation keeps collectors simple, testable, and ensures consistent validation when persisting different resource types.","title":"Resources and BaseResource"},{"location":"developer_guide/#persistence-service","text":"PersistenceService ( src/data_manager/collectors/persistence.py ) centralises all filesystem writes for document content and metadata catalog updates. When persist_resource() is called it: Resolves the target path under the configured DATA_PATH . Validates and writes the resource content (rejecting empty payloads or unknown types). Normalises metadata (if provided) for storage. Upserts a row into the Postgres resources catalog with file and metadata fields. Collectors only interact with PersistenceService ; they should not touch the filesystem directly.","title":"Persistence Service"},{"location":"developer_guide/#vector-database","text":"The vector store lives under the data_manager/vectorstore package. VectorStoreManager reads the Postgres catalog and manages embeddings in PostgreSQL: Loads the tracked files and metadata hashes from the Postgres catalog. Splits documents into chunks, optional stemming, and builds embeddings via the configured model. Adds chunks to the document_chunks table with embeddings and flattened metadata (including resource hash, filename, human-readable display fields, and any source-specific extras). Deletes stale entries when the underlying files disappear or are superseded. Because the manager defers to the catalog, any resource persisted through PersistenceService automatically becomes eligible for indexing\u2014no extra plumbing is required.","title":"Vector Database"},{"location":"developer_guide/#catalog-verification-checklist","text":"Confirm the Postgres resources table exists and is reachable from the service containers. Ingest or upload a new document and verify a new row appears in resources . Verify VectorStoreManager can update the collection using the Postgres catalog.","title":"Catalog Verification Checklist"},{"location":"developer_guide/#extending-the-stack","text":"","title":"Extending the Stack"},{"location":"developer_guide/#adding-a-new-data-source","text":"When integrating a new source, create a collector under data_manager/collectors . Collectors should yield Resource objects. A new Resource subclass is only needed if the content type is not already represented (e.g., text, HTML, markdown, images, etc.), but it must implement the required methods described above. When integrating a new collector, ensure that any per-source configuration is encoded in the resource metadata so downstream consumers\u2014such as the chat app\u2014can honour it.","title":"Adding a New Data Source"},{"location":"developer_guide/#adding-a-new-service","text":"Create a Flask blueprint under src/interfaces/ . Register the service in src/cli/service_registry.py with its name, port, and dependencies. Add a service entrypoint in src/bin/ . Add any service-specific config keys under services.<name> in the base config template.","title":"Adding a New Service"},{"location":"developer_guide/#extending-embeddings-or-storage","text":"When extending the embedding pipeline or storage schema, keep this flow in mind: collectors produce resources \u2192 PersistenceService writes files and updates the Postgres catalog \u2192 VectorStoreManager indexes embeddings in PostgreSQL. Keeping responsibilities narrowly scoped makes the ingestion stack easier to reason about and evolve.","title":"Extending Embeddings or Storage"},{"location":"install/","text":"Install System Requirements Archi is deployed using a Python-based CLI onto containers. It requires: docker version 24+ or podman version 5.4.0+ (for containers) python 3.10.0+ (for the CLI) Note: We support either running open-source models locally or connecting to existing APIs. If you plan to run open-source models on your machine's GPUs, see the Advanced Setup & Deployment section. Installation Clone the Archi repository: git clone https://github.com/archi-physics/archi.git Check out the latest stable tag (recommended for users; stay on main only if you're actively developing): cd archi git checkout $(git describe --tags $(git rev-list --tags --max-count=1)) Install Archi (from inside the repository): pip install -e . This installs Archi's dependencies and the CLI tool. Verify the installation with: which archi The command prints the path to the archi executable. Show Full Installation Script # Clone the repository git clone https://github.com/archi-physics/archi.git cd archi export ARCHI_DIR=$(pwd) # (Optional) Checkout the latest stable tag (recommended for users) # Skip this if you're developing and want the tip of main. git checkout $(git describe --tags $(git rev-list --tags --max-count=1)) # (Optional) Create and activate a virtual environment python3 -m venv archi_venv source archi_venv/bin/activate # Install dependencies cd \"$ARCHI_DIR\" pip install -e . # Verify installation which archi","title":"Install"},{"location":"install/#install","text":"","title":"Install"},{"location":"install/#system-requirements","text":"Archi is deployed using a Python-based CLI onto containers. It requires: docker version 24+ or podman version 5.4.0+ (for containers) python 3.10.0+ (for the CLI) Note: We support either running open-source models locally or connecting to existing APIs. If you plan to run open-source models on your machine's GPUs, see the Advanced Setup & Deployment section.","title":"System Requirements"},{"location":"install/#installation","text":"Clone the Archi repository: git clone https://github.com/archi-physics/archi.git Check out the latest stable tag (recommended for users; stay on main only if you're actively developing): cd archi git checkout $(git describe --tags $(git rev-list --tags --max-count=1)) Install Archi (from inside the repository): pip install -e . This installs Archi's dependencies and the CLI tool. Verify the installation with: which archi The command prints the path to the archi executable. Show Full Installation Script # Clone the repository git clone https://github.com/archi-physics/archi.git cd archi export ARCHI_DIR=$(pwd) # (Optional) Checkout the latest stable tag (recommended for users) # Skip this if you're developing and want the tip of main. git checkout $(git describe --tags $(git rev-list --tags --max-count=1)) # (Optional) Create and activate a virtual environment python3 -m venv archi_venv source archi_venv/bin/activate # Install dependencies cd \"$ARCHI_DIR\" pip install -e . # Verify installation which archi","title":"Installation"},{"location":"models_providers/","text":"Models & Providers Archi uses a provider-based architecture for LLM access. Each provider wraps a specific LLM service and exposes a unified interface for model listing, connection validation, and chat model creation. Provider Architecture All providers extend the BaseProvider abstract class and are registered in a global provider registry. The system supports five provider types: Provider Type API Key Env Var Default Model LangChain Backend OpenAI openai OPENAI_API_KEY gpt-4o ChatOpenAI Anthropic anthropic ANTHROPIC_API_KEY claude-sonnet-4-20250514 ChatAnthropic Google Gemini gemini GOOGLE_API_KEY gemini-2.0-flash ChatGoogleGenerativeAI OpenRouter openrouter OPENROUTER_API_KEY anthropic/claude-3.5-sonnet ChatOpenAI (custom base URL) Local (Ollama/vLLM) local N/A Dynamic (fetched from server) ChatOllama or ChatOpenAI Key Concepts ProviderType : An enum of supported provider names ( OPENAI , ANTHROPIC , GEMINI , OPENROUTER , LOCAL ). ProviderConfig : A dataclass holding provider settings \u2014 type, API key, base URL, enabled state, models list, and extra kwargs. ModelInfo : Describes a model's capabilities \u2014 context window, tool support, streaming support, vision support, and max output tokens. Provider Registry : Providers are lazily registered at first use. Factory functions ( get_provider , get_model ) handle instantiation and caching. Configuring Providers Providers are configured per-service in your deployment's configuration file. Each service can specify a default provider and model, plus provider-specific settings. Quick Start by Provider Use this flow if you want to start with a provider other than Ollama: Start from examples/deployments/basic-ollama/config.yaml and copy it to a new file. Change services.chat_app.default_provider and services.chat_app.default_model . If you are not using local , remove services.chat_app.providers.local . Add provider-specific services.chat_app.providers.<provider> settings only when needed (for example local mode/base URL). Put required secrets in your .env file. Run archi create --name my-archi --config <your-config>.yaml --podman --env-file .secrets.env --services chatbot . Minimal provider snippets for services.chat_app : OpenAI services: chat_app: default_provider: openai default_model: gpt-4o Required secret: OPENAI_API_KEY Anthropic services: chat_app: default_provider: anthropic default_model: claude-sonnet-4-20250514 Required secret: ANTHROPIC_API_KEY Google Gemini services: chat_app: default_provider: gemini default_model: gemini-2.0-flash providers: gemini: enabled: true Required secret: GOOGLE_API_KEY OpenRouter OpenRouter uses an OpenAI-compatible API to access models from multiple providers. services: chat_app: default_provider: openrouter default_model: anthropic/claude-3.5-sonnet Required secret: OPENROUTER_API_KEY Optional secrets: OPENROUTER_SITE_URL , OPENROUTER_APP_NAME Local Models (Ollama) services: chat_app: default_provider: local default_model: llama3.2 providers: local: base_url: http://localhost:11434 mode: ollama models: - llama3.2 Local OpenAI-compatible server (vLLM, LM Studio, etc.) services: chat_app: default_provider: local default_model: my-model providers: local: base_url: http://localhost:8000/v1 mode: openai_compat models: - my-model Secret usually not required unless your local server enforces API auth. The local provider supports two modes: ollama (default): Uses ChatOllama . Models are dynamically fetched from the Ollama server's /api/tags endpoint. openai_compat : Uses ChatOpenAI with a custom base URL. Suitable for vLLM, LM Studio, or other OpenAI-compatible servers. Note: For GPU setup with local models, see Advanced Setup & Deployment . Embedding Models Embeddings convert text into numerical vectors for semantic search. Configure these in the data_manager section: OpenAI Embeddings data_manager: embedding_name: OpenAIEmbeddings embedding_class_map: OpenAIEmbeddings: class: OpenAIEmbeddings kwargs: model: text-embedding-3-small similarity_score_reference: 10 Requires OPENAI_API_KEY in your secrets file. HuggingFace Embeddings data_manager: embedding_name: HuggingFaceEmbeddings embedding_class_map: HuggingFaceEmbeddings: class: HuggingFaceEmbeddings kwargs: model_name: sentence-transformers/all-MiniLM-L6-v2 model_kwargs: device: cpu encode_kwargs: normalize_embeddings: true similarity_score_reference: 10 Uses HuggingFace models locally. Optionally requires HUGGINGFACEHUB_API_TOKEN for private models. Bring Your Own Key (BYOK) BYOK allows users to provide their own API keys for LLM providers at runtime, enabling cost attribution, provider flexibility, and privacy. Supported Providers: BYOK session keys work with all configured provider types (OpenAI, Anthropic, OpenRouter, Gemini, etc.). The Settings UI shows status indicators for each provider. Key Hierarchy API keys are resolved in the following order (highest priority first): Session Storage : User-provided keys via the Settings UI (BYOK) Environment Variables / Docker Secrets : Admin-configured keys (e.g., OPENAI_API_KEY or keys mounted at /run/secrets/ ) Note: When a user provides a session key, it overrides any environment-level key for that user's requests. Environment keys serve as the default fallback for users who have not configured their own key. Using BYOK in the Chat Interface Open the Settings modal (gear icon in the header) Expand the API Keys section Enter your API key for each provider you want to use Click Save to store it in your session Select your preferred Provider and Model from the dropdowns Start chatting Status Indicators: Icon Meaning \u2713 Env Key configured via environment variable (cannot be changed) \u2713 Session Key configured via your session \u25cb No key configured BYOK API Endpoints Endpoint Method Description /api/providers/keys GET Get status of all provider keys /api/providers/keys/set POST Set a session API key (validates before storing) /api/providers/keys/clear POST Clear a session API key Security Considerations Keys are never logged or echoed Keys are session-scoped and cleared on logout or session expiry HTTPS is strongly recommended for production \u2014 see HTTPS Configuration","title":"Models & Providers"},{"location":"models_providers/#models-providers","text":"Archi uses a provider-based architecture for LLM access. Each provider wraps a specific LLM service and exposes a unified interface for model listing, connection validation, and chat model creation.","title":"Models &amp; Providers"},{"location":"models_providers/#provider-architecture","text":"All providers extend the BaseProvider abstract class and are registered in a global provider registry. The system supports five provider types: Provider Type API Key Env Var Default Model LangChain Backend OpenAI openai OPENAI_API_KEY gpt-4o ChatOpenAI Anthropic anthropic ANTHROPIC_API_KEY claude-sonnet-4-20250514 ChatAnthropic Google Gemini gemini GOOGLE_API_KEY gemini-2.0-flash ChatGoogleGenerativeAI OpenRouter openrouter OPENROUTER_API_KEY anthropic/claude-3.5-sonnet ChatOpenAI (custom base URL) Local (Ollama/vLLM) local N/A Dynamic (fetched from server) ChatOllama or ChatOpenAI","title":"Provider Architecture"},{"location":"models_providers/#key-concepts","text":"ProviderType : An enum of supported provider names ( OPENAI , ANTHROPIC , GEMINI , OPENROUTER , LOCAL ). ProviderConfig : A dataclass holding provider settings \u2014 type, API key, base URL, enabled state, models list, and extra kwargs. ModelInfo : Describes a model's capabilities \u2014 context window, tool support, streaming support, vision support, and max output tokens. Provider Registry : Providers are lazily registered at first use. Factory functions ( get_provider , get_model ) handle instantiation and caching.","title":"Key Concepts"},{"location":"models_providers/#configuring-providers","text":"Providers are configured per-service in your deployment's configuration file. Each service can specify a default provider and model, plus provider-specific settings.","title":"Configuring Providers"},{"location":"models_providers/#quick-start-by-provider","text":"Use this flow if you want to start with a provider other than Ollama: Start from examples/deployments/basic-ollama/config.yaml and copy it to a new file. Change services.chat_app.default_provider and services.chat_app.default_model . If you are not using local , remove services.chat_app.providers.local . Add provider-specific services.chat_app.providers.<provider> settings only when needed (for example local mode/base URL). Put required secrets in your .env file. Run archi create --name my-archi --config <your-config>.yaml --podman --env-file .secrets.env --services chatbot . Minimal provider snippets for services.chat_app :","title":"Quick Start by Provider"},{"location":"models_providers/#openai","text":"services: chat_app: default_provider: openai default_model: gpt-4o Required secret: OPENAI_API_KEY","title":"OpenAI"},{"location":"models_providers/#anthropic","text":"services: chat_app: default_provider: anthropic default_model: claude-sonnet-4-20250514 Required secret: ANTHROPIC_API_KEY","title":"Anthropic"},{"location":"models_providers/#google-gemini","text":"services: chat_app: default_provider: gemini default_model: gemini-2.0-flash providers: gemini: enabled: true Required secret: GOOGLE_API_KEY","title":"Google Gemini"},{"location":"models_providers/#openrouter","text":"OpenRouter uses an OpenAI-compatible API to access models from multiple providers. services: chat_app: default_provider: openrouter default_model: anthropic/claude-3.5-sonnet Required secret: OPENROUTER_API_KEY Optional secrets: OPENROUTER_SITE_URL , OPENROUTER_APP_NAME","title":"OpenRouter"},{"location":"models_providers/#local-models-ollama","text":"services: chat_app: default_provider: local default_model: llama3.2 providers: local: base_url: http://localhost:11434 mode: ollama models: - llama3.2","title":"Local Models (Ollama)"},{"location":"models_providers/#local-openai-compatible-server-vllm-lm-studio-etc","text":"services: chat_app: default_provider: local default_model: my-model providers: local: base_url: http://localhost:8000/v1 mode: openai_compat models: - my-model Secret usually not required unless your local server enforces API auth. The local provider supports two modes: ollama (default): Uses ChatOllama . Models are dynamically fetched from the Ollama server's /api/tags endpoint. openai_compat : Uses ChatOpenAI with a custom base URL. Suitable for vLLM, LM Studio, or other OpenAI-compatible servers. Note: For GPU setup with local models, see Advanced Setup & Deployment .","title":"Local OpenAI-compatible server (vLLM, LM Studio, etc.)"},{"location":"models_providers/#embedding-models","text":"Embeddings convert text into numerical vectors for semantic search. Configure these in the data_manager section:","title":"Embedding Models"},{"location":"models_providers/#openai-embeddings","text":"data_manager: embedding_name: OpenAIEmbeddings embedding_class_map: OpenAIEmbeddings: class: OpenAIEmbeddings kwargs: model: text-embedding-3-small similarity_score_reference: 10 Requires OPENAI_API_KEY in your secrets file.","title":"OpenAI Embeddings"},{"location":"models_providers/#huggingface-embeddings","text":"data_manager: embedding_name: HuggingFaceEmbeddings embedding_class_map: HuggingFaceEmbeddings: class: HuggingFaceEmbeddings kwargs: model_name: sentence-transformers/all-MiniLM-L6-v2 model_kwargs: device: cpu encode_kwargs: normalize_embeddings: true similarity_score_reference: 10 Uses HuggingFace models locally. Optionally requires HUGGINGFACEHUB_API_TOKEN for private models.","title":"HuggingFace Embeddings"},{"location":"models_providers/#bring-your-own-key-byok","text":"BYOK allows users to provide their own API keys for LLM providers at runtime, enabling cost attribution, provider flexibility, and privacy. Supported Providers: BYOK session keys work with all configured provider types (OpenAI, Anthropic, OpenRouter, Gemini, etc.). The Settings UI shows status indicators for each provider.","title":"Bring Your Own Key (BYOK)"},{"location":"models_providers/#key-hierarchy","text":"API keys are resolved in the following order (highest priority first): Session Storage : User-provided keys via the Settings UI (BYOK) Environment Variables / Docker Secrets : Admin-configured keys (e.g., OPENAI_API_KEY or keys mounted at /run/secrets/ ) Note: When a user provides a session key, it overrides any environment-level key for that user's requests. Environment keys serve as the default fallback for users who have not configured their own key.","title":"Key Hierarchy"},{"location":"models_providers/#using-byok-in-the-chat-interface","text":"Open the Settings modal (gear icon in the header) Expand the API Keys section Enter your API key for each provider you want to use Click Save to store it in your session Select your preferred Provider and Model from the dropdowns Start chatting Status Indicators: Icon Meaning \u2713 Env Key configured via environment variable (cannot be changed) \u2713 Session Key configured via your session \u25cb No key configured","title":"Using BYOK in the Chat Interface"},{"location":"models_providers/#byok-api-endpoints","text":"Endpoint Method Description /api/providers/keys GET Get status of all provider keys /api/providers/keys/set POST Set a session API key (validates before storing) /api/providers/keys/clear POST Clear a session API key","title":"BYOK API Endpoints"},{"location":"models_providers/#security-considerations","text":"Keys are never logged or echoed Keys are session-scoped and cleared on logout or session expiry HTTPS is strongly recommended for production \u2014 see HTTPS Configuration","title":"Security Considerations"},{"location":"quickstart/","text":"Quickstart Deploy your first instance of Archi and walk through the important concepts. Sources and Services Archi can ingest data from a variety of sources and supports several services . List them with the CLI command below and decide which ones you want to use so that we can configure them. archi list-services Example output: Available Archi services: Application Services: chatbot Interactive chat interface for users to communicate with the AI agent grafana Monitoring dashboard for system and LLM performance metrics uploader Admin interface for uploading and managing documents grader Automated grading service for assignments with web interface Integration Services: piazza Integration service for Piazza posts and Slack notifications mattermost Integration service for Mattermost channels redmine-mailer Email processing and Cleo/Redmine ticket management Data Sources: git Git repository scraping for MkDocs-based documentation jira Jira issue tracking integration redmine Redmine ticket integration sso SSO-backed web crawling See the User Guide for detailed information about each service and source. Pipelines Archi supports several pipelines (agentic and not). The active agent class is configured per service, and the agent prompt/tools are defined in agent markdown files. Example agent spec file ( examples/agents/default.md ): --- name: CMS CompOps Default tools: - search_local_files - search_metadata_index - list_metadata_schema - fetch_catalog_document - search_vectorstore_hybrid --- You are a CMS CompOps assistant. Use tools to gather evidence before answering, and keep responses concise. Configuration Once you have chosen the services, sources, and agent class you want to use, create a configuration file that specifies their settings. You can start from one of the example configuration files under examples/deployments/ , or create your own from scratch. This file sets parameters; the selected services and sources are determined at deployment time. Important: The configuration file follows the format of src/cli/templates/base-config.yaml . Any fields not specified in your configuration will be populated with the defaults from this template. Example configuration for the chatbot service using a local Ollama model and agent specs from services.chat_app.agents_dir : name: my_archi services: chat_app: agent_class: CMSCompOpsAgent agents_dir: examples/agents default_provider: local default_model: llama3.2 providers: local: base_url: http://localhost:11434 mode: ollama models: - llama3.2 trained_on: \"My data\" hostname: \"<your-hostname>\" vectorstore: backend: postgres # Uses PostgreSQL with pgvector (default) data_manager: sources: links: visible: true # include scraped pages in the chat citations input_lists: - examples/deployments/basic-gpu/miscellanea.list embedding_name: HuggingFaceEmbeddings chunk_size: 1000 Agent specs are Markdown files (see examples/agents/ ) with YAML frontmatter for name and tools , and the prompt in the Markdown body. Using OpenAI, Anthropic, Gemini, OpenRouter, or a non-Ollama local server? This quickstart uses Ollama for the first deployment path. For provider-specific startup snippets (including required secrets and config), see Models & Providers . Explanation of configuration parameters - `name`: Name of your Archi deployment. - `data_manager`: Settings related to data ingestion and the vector store. - `data_manager.sources.links.input_lists`: Lists of URLs to seed the deployment. - `data_manager.sources. .visible`: Controls whether content from a given source is surfaced to end users (defaults to `true`). - `data_manager.embedding_name`: Embedding model used for vectorization. - `data_manager.chunk_size`: Controls how documents are split prior to embedding. - `services`: Settings for individual services/interfaces. - `services.chat_app.agent_class`: Agent class to run (pipeline class name). - `services.chat_app.agents_dir`: Local path to agent markdown files (copied into the deployment). - `services.chat_app.default_provider` and `services.chat_app.default_model`: Default provider/model for chat when no UI override is set. - `services.chat_app.providers.local`: Ollama/local provider configuration. - `services.chat_app`: Chat interface configuration, including hostname and descriptive metadata. - `services.vectorstore.backend`: Vector store backend (`postgres` with pgvector). Secrets Secrets are sensitive values (passwords, API keys, etc.) that should not be stored directly in code or configuration files. Store them in a single .env file on your filesystem. Minimal deployments (chatbot with open-source LLM and embeddings) require: PG_PASSWORD : password used to secure the database. Create the secrets file with: echo \"PG_PASSWORD=my_strong_password\" > ~/.secrets.env If you are not using open-source models, supply the relevant API credentials: OPENAI_API_KEY : OpenAI API key. OPENROUTER_API_KEY : OpenRouter API key. OPENROUTER_SITE_URL : Optional site URL for OpenRouter attribution. OPENROUTER_APP_NAME : Optional app name for OpenRouter attribution. ANTHROPIC_API_KEY : Anthropic API key. GOOGLE_API_KEY : Google Gemini API key. HUGGINGFACEHUB_API_TOKEN : HuggingFace access token (for private models or embeddings). Other services may require additional secrets; see the User Guide for details. Creating an Archi Deployment with Ollama Create your deployment with the CLI. A deployment with a local Ollama model (make sure you specify in the config.yaml the URL of your Ollama instance): archi create --name my-archi \\ --config examples/deployments/basic-ollama/config.yaml \\ --podman \\ --env-file .secrets.env \\ --services chatbot Flag Description --name / -n Deployment name --config / -c Path to configuration file --env-file / -e Path to the secrets .env file --services / -s Comma-separated services to deploy --podman Use Podman instead of Docker Agent specs are loaded from services.chat_app.agents_dir in the config. Example output archi create --name my-archi --config examples/deployments/basic-ollama/config.yaml --podman --env-file .secrets.env --services chatbot Starting Archi deployment process... [archi] Creating deployment 'my-archi' with services: chatbot [archi] Auto-enabling dependencies: postgres [archi] Configuration validated successfully [archi] You are using an embedding model from HuggingFace; make sure to include a HuggingFace token if required for usage, it won't be explicitly enforced [archi] Required secrets validated: PG_PASSWORD [archi] Volume 'archi-pg-my-archi' already exists. No action needed. [archi] Volume 'archi-my-archi' already exists. No action needed. [archi] Starting compose deployment from /path/to/my/.archi/archi-my-archi [archi] Using compose file: /path/to/my/.archi/archi-my-archi/compose.yaml [archi] (This might take a minute...) [archi] Deployment started successfully Archi deployment 'my-archi' created successfully! Services running: chatbot, postgres [archi] Chatbot: http://localhost:7861 The first deployment builds the container images from scratch (which may take a few minutes). Subsequent deployments reuse the images and complete much faster (roughly a minute). Tip: Having issues? Run the command with -v 4 to enable DEBUG-level logging. Verifying a deployment Run these checks after archi create : Step 1: Confirm deployment registration archi list-deployments You should see your deployment name (for example my-archi ). Step 2: Confirm services are running with your container runtime podman ps # or: docker ps You should see containers for at least chatbot and postgres . Step 3: Open the chat app URL printed by the CLI (default http://localhost:7861 ) and verify the UI loads. Next Steps Once your deployment is running: Chat UI : Open http://localhost:7861 in your browser to start chatting. Data Viewer : Navigate to the /data page in the chat UI to browse ingested documents. Upload Documents : If you deployed the uploader service, access the upload interface at its configured port. From here, explore the rest of the documentation: User Guide \u2014 overview of all capabilities Agents & Tools \u2014 customize agent behavior and prompts Models & Providers \u2014 switch to cloud LLMs (OpenAI, Anthropic, Gemini) Configuration Reference \u2014 full YAML config schema CLI Reference \u2014 all CLI commands and options Troubleshooting \u2014 common issues and fixes","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"Deploy your first instance of Archi and walk through the important concepts.","title":"Quickstart"},{"location":"quickstart/#sources-and-services","text":"Archi can ingest data from a variety of sources and supports several services . List them with the CLI command below and decide which ones you want to use so that we can configure them. archi list-services Example output: Available Archi services: Application Services: chatbot Interactive chat interface for users to communicate with the AI agent grafana Monitoring dashboard for system and LLM performance metrics uploader Admin interface for uploading and managing documents grader Automated grading service for assignments with web interface Integration Services: piazza Integration service for Piazza posts and Slack notifications mattermost Integration service for Mattermost channels redmine-mailer Email processing and Cleo/Redmine ticket management Data Sources: git Git repository scraping for MkDocs-based documentation jira Jira issue tracking integration redmine Redmine ticket integration sso SSO-backed web crawling See the User Guide for detailed information about each service and source.","title":"Sources and Services"},{"location":"quickstart/#pipelines","text":"Archi supports several pipelines (agentic and not). The active agent class is configured per service, and the agent prompt/tools are defined in agent markdown files. Example agent spec file ( examples/agents/default.md ): --- name: CMS CompOps Default tools: - search_local_files - search_metadata_index - list_metadata_schema - fetch_catalog_document - search_vectorstore_hybrid --- You are a CMS CompOps assistant. Use tools to gather evidence before answering, and keep responses concise.","title":"Pipelines"},{"location":"quickstart/#configuration","text":"Once you have chosen the services, sources, and agent class you want to use, create a configuration file that specifies their settings. You can start from one of the example configuration files under examples/deployments/ , or create your own from scratch. This file sets parameters; the selected services and sources are determined at deployment time. Important: The configuration file follows the format of src/cli/templates/base-config.yaml . Any fields not specified in your configuration will be populated with the defaults from this template. Example configuration for the chatbot service using a local Ollama model and agent specs from services.chat_app.agents_dir : name: my_archi services: chat_app: agent_class: CMSCompOpsAgent agents_dir: examples/agents default_provider: local default_model: llama3.2 providers: local: base_url: http://localhost:11434 mode: ollama models: - llama3.2 trained_on: \"My data\" hostname: \"<your-hostname>\" vectorstore: backend: postgres # Uses PostgreSQL with pgvector (default) data_manager: sources: links: visible: true # include scraped pages in the chat citations input_lists: - examples/deployments/basic-gpu/miscellanea.list embedding_name: HuggingFaceEmbeddings chunk_size: 1000 Agent specs are Markdown files (see examples/agents/ ) with YAML frontmatter for name and tools , and the prompt in the Markdown body. Using OpenAI, Anthropic, Gemini, OpenRouter, or a non-Ollama local server? This quickstart uses Ollama for the first deployment path. For provider-specific startup snippets (including required secrets and config), see Models & Providers . Explanation of configuration parameters - `name`: Name of your Archi deployment. - `data_manager`: Settings related to data ingestion and the vector store. - `data_manager.sources.links.input_lists`: Lists of URLs to seed the deployment. - `data_manager.sources. .visible`: Controls whether content from a given source is surfaced to end users (defaults to `true`). - `data_manager.embedding_name`: Embedding model used for vectorization. - `data_manager.chunk_size`: Controls how documents are split prior to embedding. - `services`: Settings for individual services/interfaces. - `services.chat_app.agent_class`: Agent class to run (pipeline class name). - `services.chat_app.agents_dir`: Local path to agent markdown files (copied into the deployment). - `services.chat_app.default_provider` and `services.chat_app.default_model`: Default provider/model for chat when no UI override is set. - `services.chat_app.providers.local`: Ollama/local provider configuration. - `services.chat_app`: Chat interface configuration, including hostname and descriptive metadata. - `services.vectorstore.backend`: Vector store backend (`postgres` with pgvector).","title":"Configuration"},{"location":"quickstart/#secrets","text":"Secrets are sensitive values (passwords, API keys, etc.) that should not be stored directly in code or configuration files. Store them in a single .env file on your filesystem. Minimal deployments (chatbot with open-source LLM and embeddings) require: PG_PASSWORD : password used to secure the database. Create the secrets file with: echo \"PG_PASSWORD=my_strong_password\" > ~/.secrets.env If you are not using open-source models, supply the relevant API credentials: OPENAI_API_KEY : OpenAI API key. OPENROUTER_API_KEY : OpenRouter API key. OPENROUTER_SITE_URL : Optional site URL for OpenRouter attribution. OPENROUTER_APP_NAME : Optional app name for OpenRouter attribution. ANTHROPIC_API_KEY : Anthropic API key. GOOGLE_API_KEY : Google Gemini API key. HUGGINGFACEHUB_API_TOKEN : HuggingFace access token (for private models or embeddings). Other services may require additional secrets; see the User Guide for details.","title":"Secrets"},{"location":"quickstart/#creating-an-archi-deployment-with-ollama","text":"Create your deployment with the CLI. A deployment with a local Ollama model (make sure you specify in the config.yaml the URL of your Ollama instance): archi create --name my-archi \\ --config examples/deployments/basic-ollama/config.yaml \\ --podman \\ --env-file .secrets.env \\ --services chatbot Flag Description --name / -n Deployment name --config / -c Path to configuration file --env-file / -e Path to the secrets .env file --services / -s Comma-separated services to deploy --podman Use Podman instead of Docker Agent specs are loaded from services.chat_app.agents_dir in the config. Example output archi create --name my-archi --config examples/deployments/basic-ollama/config.yaml --podman --env-file .secrets.env --services chatbot Starting Archi deployment process... [archi] Creating deployment 'my-archi' with services: chatbot [archi] Auto-enabling dependencies: postgres [archi] Configuration validated successfully [archi] You are using an embedding model from HuggingFace; make sure to include a HuggingFace token if required for usage, it won't be explicitly enforced [archi] Required secrets validated: PG_PASSWORD [archi] Volume 'archi-pg-my-archi' already exists. No action needed. [archi] Volume 'archi-my-archi' already exists. No action needed. [archi] Starting compose deployment from /path/to/my/.archi/archi-my-archi [archi] Using compose file: /path/to/my/.archi/archi-my-archi/compose.yaml [archi] (This might take a minute...) [archi] Deployment started successfully Archi deployment 'my-archi' created successfully! Services running: chatbot, postgres [archi] Chatbot: http://localhost:7861 The first deployment builds the container images from scratch (which may take a few minutes). Subsequent deployments reuse the images and complete much faster (roughly a minute). Tip: Having issues? Run the command with -v 4 to enable DEBUG-level logging.","title":"Creating an Archi Deployment with Ollama"},{"location":"quickstart/#verifying-a-deployment","text":"Run these checks after archi create : Step 1: Confirm deployment registration archi list-deployments You should see your deployment name (for example my-archi ). Step 2: Confirm services are running with your container runtime podman ps # or: docker ps You should see containers for at least chatbot and postgres . Step 3: Open the chat app URL printed by the CLI (default http://localhost:7861 ) and verify the UI loads.","title":"Verifying a deployment"},{"location":"quickstart/#next-steps","text":"Once your deployment is running: Chat UI : Open http://localhost:7861 in your browser to start chatting. Data Viewer : Navigate to the /data page in the chat UI to browse ingested documents. Upload Documents : If you deployed the uploader service, access the upload interface at its configured port. From here, explore the rest of the documentation: User Guide \u2014 overview of all capabilities Agents & Tools \u2014 customize agent behavior and prompts Models & Providers \u2014 switch to cloud LLMs (OpenAI, Anthropic, Gemini) Configuration Reference \u2014 full YAML config schema CLI Reference \u2014 all CLI commands and options Troubleshooting \u2014 common issues and fixes","title":"Next Steps"},{"location":"services/","text":"Services Archi supports several services \u2014 containerized applications that interact with the AI pipelines. Services are enabled at deploy time with the --services flag. archi create [...] --services chatbot,uploader,grafana List all available services with: archi list-services Chat Interface The primary user-facing service. Provides a web-based chat application for interacting with Archi's AI agents. Default port: 7861 Key Features Streaming responses with tool-call visualization Agent selector dropdown for switching between agents Built-in Data Viewer at /data Settings panel for model/provider selection BYOK support Conversation history Configuration services: chat_app: agent_class: CMSCompOpsAgent agents_dir: examples/agents default_provider: local default_model: llama3.2 trained_on: \"Course documentation\" hostname: \"example.mit.edu\" port: 7861 external_port: 7861 Running archi create [...] --services chatbot Document Upload Document upload is exposed in the chat UI and backed by the Data Manager service. Documents can be uploaded via the web interface or by copying files directly into the data directory. See Data Sources \u2014 Adding Documents Manually for setup instructions. Data Manager A background service that handles data ingestion, vectorstore management, and scheduled re-scraping. It is automatically started with most deployments. Default port: 7871 Features Orchestrates all data collectors (links, git, JIRA, Redmine) Manages the vectorstore (chunking, embedding, indexing) Provides a scheduling system for periodic re-ingestion Exposes API endpoints for ingestion status and schedule management API Endpoints Endpoint Method Description /api/ingestion/status GET Current ingestion progress /api/reload-schedules POST Trigger schedule reload from database /api/schedules GET Current schedule status Configuration services: data_manager: port: 7871 external_port: 7871 Piazza Interface Reads posts from a Piazza forum and posts draft responses to a specified Slack channel. Setup Go to Slack Apps and sign in to your workspace. Click Create New App \u2192 From scratch . Name the app and select the workspace. Go to Incoming Webhooks under Features and toggle it on. Click Add New Webhook and select the target channel. Copy the Webhook URL to your secrets file. Configuration Get the Piazza network ID from the class homepage URL (e.g., https://piazza.com/class/m0g3v0ahsqm2lg \u2192 m0g3v0ahsqm2lg ). services: piazza: agent_class: QAPipeline provider: local model: llama3.2 network_id: <your Piazza network ID> chat_app: trained_on: \"Your class materials\" Secrets PIAZZA_EMAIL=... PIAZZA_PASSWORD=... SLACK_WEBHOOK=... Running archi create [...] --services chatbot,piazza Redmine / Mailbox Interface Reads new tickets in a Redmine project, drafts a response as a comment, and sends it as an email when the ticket is marked \"Resolved\" by an admin. Configuration services: redmine_mailbox: url: https://redmine.example.com project: my-project redmine_update_time: 10 mailbox_update_time: 10 answer_tag: \"-- Archi -- Resolving email was sent\" Secrets IMAP_USER=... IMAP_PW=... REDMINE_USER=... REDMINE_PW=... SENDER_SERVER=... SENDER_PORT=587 SENDER_REPLYTO=... SENDER_USER=... SENDER_PW=... Running archi create [...] --services chatbot,redmine-mailer Mattermost Interface Reads posts from a Mattermost forum and posts draft responses to a specified channel. Configuration services: mattermost: update_time: 60 Secrets MATTERMOST_WEBHOOK=... MATTERMOST_PAK=... MATTERMOST_CHANNEL_ID_READ=... MATTERMOST_CHANNEL_ID_WRITE=... Running archi create [...] --services chatbot,mattermost Grafana Monitoring Monitor system performance and LLM usage with a pre-configured Grafana dashboard. Default port: 3000 Note: If redeploying with an existing name (without removing volumes), the PostgreSQL Grafana user may not have been created. Deploy a fresh instance to avoid issues. Configuration services: grafana: external_port: 3000 Secrets PG_PASSWORD=<your_database_password> GRAFANA_PG_PASSWORD=<grafana_db_password> Running archi create [...] --services chatbot,grafana After deployment, access Grafana at your-hostname:3000 . The default login is admin / admin \u2014 you'll be prompted to change the password on first login. Navigate to Menu \u2192 Dashboards \u2192 Archi \u2192 Archi Usage for the main dashboard. Tip: For the \"Recent Conversation Messages\" panel, click the three dots \u2192 Edit \u2192 find \"Override 4\" \u2192 enable Cell value inspect to expand long text entries. Click Apply to save. Grader Interface An automated grading service for handwritten assignments with a web interface. Note: This service is experimental and not yet fully generalized. Requirements The following files are needed: users.csv : Two columns \u2014 MIT email and Unique code solution_with_rubric_*.txt : One file per problem, named with the problem number. Begins with the problem name and a line of dashes. admin_password.txt : Admin code for resetting student attempts (passed as a secret). Configuration services: grader_app: provider: local model: llama3.2 prompts: grading: final_grade_prompt: final_grade.prompt image_processing: image_processing_prompt: image_processing.prompt num_problems: 1 local_rubric_dir: ~/grading/my_rubrics local_users_csv_dir: ~/grading/logins chat_app: trained_on: \"rubrics, class info, etc.\" Secrets ADMIN_PASSWORD=your_password Running archi create [...] --services grader","title":"Services"},{"location":"services/#services","text":"Archi supports several services \u2014 containerized applications that interact with the AI pipelines. Services are enabled at deploy time with the --services flag. archi create [...] --services chatbot,uploader,grafana List all available services with: archi list-services","title":"Services"},{"location":"services/#chat-interface","text":"The primary user-facing service. Provides a web-based chat application for interacting with Archi's AI agents. Default port: 7861","title":"Chat Interface"},{"location":"services/#key-features","text":"Streaming responses with tool-call visualization Agent selector dropdown for switching between agents Built-in Data Viewer at /data Settings panel for model/provider selection BYOK support Conversation history","title":"Key Features"},{"location":"services/#configuration","text":"services: chat_app: agent_class: CMSCompOpsAgent agents_dir: examples/agents default_provider: local default_model: llama3.2 trained_on: \"Course documentation\" hostname: \"example.mit.edu\" port: 7861 external_port: 7861","title":"Configuration"},{"location":"services/#running","text":"archi create [...] --services chatbot","title":"Running"},{"location":"services/#document-upload","text":"Document upload is exposed in the chat UI and backed by the Data Manager service. Documents can be uploaded via the web interface or by copying files directly into the data directory. See Data Sources \u2014 Adding Documents Manually for setup instructions.","title":"Document Upload"},{"location":"services/#data-manager","text":"A background service that handles data ingestion, vectorstore management, and scheduled re-scraping. It is automatically started with most deployments. Default port: 7871","title":"Data Manager"},{"location":"services/#features","text":"Orchestrates all data collectors (links, git, JIRA, Redmine) Manages the vectorstore (chunking, embedding, indexing) Provides a scheduling system for periodic re-ingestion Exposes API endpoints for ingestion status and schedule management","title":"Features"},{"location":"services/#api-endpoints","text":"Endpoint Method Description /api/ingestion/status GET Current ingestion progress /api/reload-schedules POST Trigger schedule reload from database /api/schedules GET Current schedule status","title":"API Endpoints"},{"location":"services/#configuration_1","text":"services: data_manager: port: 7871 external_port: 7871","title":"Configuration"},{"location":"services/#piazza-interface","text":"Reads posts from a Piazza forum and posts draft responses to a specified Slack channel.","title":"Piazza Interface"},{"location":"services/#setup","text":"Go to Slack Apps and sign in to your workspace. Click Create New App \u2192 From scratch . Name the app and select the workspace. Go to Incoming Webhooks under Features and toggle it on. Click Add New Webhook and select the target channel. Copy the Webhook URL to your secrets file.","title":"Setup"},{"location":"services/#configuration_2","text":"Get the Piazza network ID from the class homepage URL (e.g., https://piazza.com/class/m0g3v0ahsqm2lg \u2192 m0g3v0ahsqm2lg ). services: piazza: agent_class: QAPipeline provider: local model: llama3.2 network_id: <your Piazza network ID> chat_app: trained_on: \"Your class materials\"","title":"Configuration"},{"location":"services/#secrets","text":"PIAZZA_EMAIL=... PIAZZA_PASSWORD=... SLACK_WEBHOOK=...","title":"Secrets"},{"location":"services/#running_1","text":"archi create [...] --services chatbot,piazza","title":"Running"},{"location":"services/#redmine-mailbox-interface","text":"Reads new tickets in a Redmine project, drafts a response as a comment, and sends it as an email when the ticket is marked \"Resolved\" by an admin.","title":"Redmine / Mailbox Interface"},{"location":"services/#configuration_3","text":"services: redmine_mailbox: url: https://redmine.example.com project: my-project redmine_update_time: 10 mailbox_update_time: 10 answer_tag: \"-- Archi -- Resolving email was sent\"","title":"Configuration"},{"location":"services/#secrets_1","text":"IMAP_USER=... IMAP_PW=... REDMINE_USER=... REDMINE_PW=... SENDER_SERVER=... SENDER_PORT=587 SENDER_REPLYTO=... SENDER_USER=... SENDER_PW=...","title":"Secrets"},{"location":"services/#running_2","text":"archi create [...] --services chatbot,redmine-mailer","title":"Running"},{"location":"services/#mattermost-interface","text":"Reads posts from a Mattermost forum and posts draft responses to a specified channel.","title":"Mattermost Interface"},{"location":"services/#configuration_4","text":"services: mattermost: update_time: 60","title":"Configuration"},{"location":"services/#secrets_2","text":"MATTERMOST_WEBHOOK=... MATTERMOST_PAK=... MATTERMOST_CHANNEL_ID_READ=... MATTERMOST_CHANNEL_ID_WRITE=...","title":"Secrets"},{"location":"services/#running_3","text":"archi create [...] --services chatbot,mattermost","title":"Running"},{"location":"services/#grafana-monitoring","text":"Monitor system performance and LLM usage with a pre-configured Grafana dashboard. Default port: 3000 Note: If redeploying with an existing name (without removing volumes), the PostgreSQL Grafana user may not have been created. Deploy a fresh instance to avoid issues.","title":"Grafana Monitoring"},{"location":"services/#configuration_5","text":"services: grafana: external_port: 3000","title":"Configuration"},{"location":"services/#secrets_3","text":"PG_PASSWORD=<your_database_password> GRAFANA_PG_PASSWORD=<grafana_db_password>","title":"Secrets"},{"location":"services/#running_4","text":"archi create [...] --services chatbot,grafana After deployment, access Grafana at your-hostname:3000 . The default login is admin / admin \u2014 you'll be prompted to change the password on first login. Navigate to Menu \u2192 Dashboards \u2192 Archi \u2192 Archi Usage for the main dashboard. Tip: For the \"Recent Conversation Messages\" panel, click the three dots \u2192 Edit \u2192 find \"Override 4\" \u2192 enable Cell value inspect to expand long text entries. Click Apply to save.","title":"Running"},{"location":"services/#grader-interface","text":"An automated grading service for handwritten assignments with a web interface. Note: This service is experimental and not yet fully generalized.","title":"Grader Interface"},{"location":"services/#requirements","text":"The following files are needed: users.csv : Two columns \u2014 MIT email and Unique code solution_with_rubric_*.txt : One file per problem, named with the problem number. Begins with the problem name and a line of dashes. admin_password.txt : Admin code for resetting student attempts (passed as a secret).","title":"Requirements"},{"location":"services/#configuration_6","text":"services: grader_app: provider: local model: llama3.2 prompts: grading: final_grade_prompt: final_grade.prompt image_processing: image_processing_prompt: image_processing.prompt num_problems: 1 local_rubric_dir: ~/grading/my_rubrics local_users_csv_dir: ~/grading/logins chat_app: trained_on: \"rubrics, class info, etc.\"","title":"Configuration"},{"location":"services/#secrets_4","text":"ADMIN_PASSWORD=your_password","title":"Secrets"},{"location":"services/#running_5","text":"archi create [...] --services grader","title":"Running"},{"location":"troubleshooting/","text":"Troubleshooting Common issues and their resolutions when running Archi. Port Conflicts Symptom : Container fails to start with \"port already in use\" or \"address already allocated.\" Fix : Change the conflicting port in your config.yaml : services: chat_app: port: 7862 # default: 7861 postgres: port: 5433 # default: 5432 data_manager: port: 7872 # default: 7871 Check what's using a port: lsof -i :7861 # or ss -tlnp | grep 7861 GPU / CUDA Errors Symptom : RuntimeError: CUDA out of memory or model fails to load on GPU. Fixes : Use --gpu-ids to restrict which GPUs are used: bash archi create -n myapp -c config.yaml --gpu-ids 0 Switch to a smaller model or use CPU-only mode (omit --gpu-ids ). Check GPU memory: bash nvidia-smi Container Debugging View logs for a specific service: docker logs archi-<deployment>-<service> # Example: docker logs archi-myapp-chat docker logs archi-myapp-data-manager Enter a running container: docker exec -it archi-myapp-chat /bin/bash Check all containers for a deployment: docker ps --filter \"name=archi-myapp\" Data Manager Not Ingesting Symptom : Data sources aren't being indexed or the data viewer shows no documents. Checks : Verify the data manager container is running: bash docker ps --filter \"name=data-manager\" Check data manager logs: bash docker logs archi-myapp-data-manager Verify your data source configuration is correct in config.yaml under data_manager.sources . Check the ingestion status endpoint: bash curl http://localhost:7871/api/ingestion/status Chat Returns Empty or Generic Responses Possible causes : No data ingested : Check the data viewer to verify documents exist. Wrong provider config : Verify your API key is set and the provider/model names are correct. Retrieval issues : Check data_manager.retrievers.hybrid_retriever settings \u2014 num_documents_to_retrieve may be too low, or bm25_weight / semantic_weight may need tuning. Enable verbose logging by checking container logs: docker logs -f archi-myapp-chat Authentication Issues Symptom : Login fails or \"unauthorized\" errors. Checks : Ensure FLASK_UPLOADER_APP_SECRET_KEY is set in your .secrets.env for stable session behavior. Verify postgres is running and accessible. Check that auth tables were initialized \u2014 the chat service creates them on first startup. Docker Build Failures Symptom : archi create fails during image build. Fixes : Ensure Docker is running and accessible: bash docker info Check disk space \u2014 Docker builds can require significant space: bash docker system df docker system prune # clean up unused images/containers For network issues during build, check your Docker daemon's DNS and proxy settings. Multiple Deployments You can run multiple Archi deployments simultaneously as long as ports don't conflict. Each deployment is isolated under ~/.archi/archi-<name>/ . List all deployments: archi list-deployments Check services for a specific deployment: archi list-services Getting Help GitHub Issues : archi-physics/archi Verbose Logging : Check container logs with docker logs -f <container-name> Configuration Reference : See the Configuration page for all available settings","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"Common issues and their resolutions when running Archi.","title":"Troubleshooting"},{"location":"troubleshooting/#port-conflicts","text":"Symptom : Container fails to start with \"port already in use\" or \"address already allocated.\" Fix : Change the conflicting port in your config.yaml : services: chat_app: port: 7862 # default: 7861 postgres: port: 5433 # default: 5432 data_manager: port: 7872 # default: 7871 Check what's using a port: lsof -i :7861 # or ss -tlnp | grep 7861","title":"Port Conflicts"},{"location":"troubleshooting/#gpu-cuda-errors","text":"Symptom : RuntimeError: CUDA out of memory or model fails to load on GPU. Fixes : Use --gpu-ids to restrict which GPUs are used: bash archi create -n myapp -c config.yaml --gpu-ids 0 Switch to a smaller model or use CPU-only mode (omit --gpu-ids ). Check GPU memory: bash nvidia-smi","title":"GPU / CUDA Errors"},{"location":"troubleshooting/#container-debugging","text":"View logs for a specific service: docker logs archi-<deployment>-<service> # Example: docker logs archi-myapp-chat docker logs archi-myapp-data-manager Enter a running container: docker exec -it archi-myapp-chat /bin/bash Check all containers for a deployment: docker ps --filter \"name=archi-myapp\"","title":"Container Debugging"},{"location":"troubleshooting/#data-manager-not-ingesting","text":"Symptom : Data sources aren't being indexed or the data viewer shows no documents. Checks : Verify the data manager container is running: bash docker ps --filter \"name=data-manager\" Check data manager logs: bash docker logs archi-myapp-data-manager Verify your data source configuration is correct in config.yaml under data_manager.sources . Check the ingestion status endpoint: bash curl http://localhost:7871/api/ingestion/status","title":"Data Manager Not Ingesting"},{"location":"troubleshooting/#chat-returns-empty-or-generic-responses","text":"Possible causes : No data ingested : Check the data viewer to verify documents exist. Wrong provider config : Verify your API key is set and the provider/model names are correct. Retrieval issues : Check data_manager.retrievers.hybrid_retriever settings \u2014 num_documents_to_retrieve may be too low, or bm25_weight / semantic_weight may need tuning. Enable verbose logging by checking container logs: docker logs -f archi-myapp-chat","title":"Chat Returns Empty or Generic Responses"},{"location":"troubleshooting/#authentication-issues","text":"Symptom : Login fails or \"unauthorized\" errors. Checks : Ensure FLASK_UPLOADER_APP_SECRET_KEY is set in your .secrets.env for stable session behavior. Verify postgres is running and accessible. Check that auth tables were initialized \u2014 the chat service creates them on first startup.","title":"Authentication Issues"},{"location":"troubleshooting/#docker-build-failures","text":"Symptom : archi create fails during image build. Fixes : Ensure Docker is running and accessible: bash docker info Check disk space \u2014 Docker builds can require significant space: bash docker system df docker system prune # clean up unused images/containers For network issues during build, check your Docker daemon's DNS and proxy settings.","title":"Docker Build Failures"},{"location":"troubleshooting/#multiple-deployments","text":"You can run multiple Archi deployments simultaneously as long as ports don't conflict. Each deployment is isolated under ~/.archi/archi-<name>/ . List all deployments: archi list-deployments Check services for a specific deployment: archi list-services","title":"Multiple Deployments"},{"location":"troubleshooting/#getting-help","text":"GitHub Issues : archi-physics/archi Verbose Logging : Check container logs with docker logs -f <container-name> Configuration Reference : See the Configuration page for all available settings","title":"Getting Help"},{"location":"user_guide/","text":"User Guide This guide covers the core concepts and features of Archi. Each topic has its own dedicated page for detailed reference. Overview Archi is a retrieval-based assistant framework with four core parts: Data sources : Where knowledge comes from (links, git repos, JIRA/Redmine, uploaded files) Vector store + retrievers : Where ingested content is indexed and searched semantically/lexically Agents + tools : The reasoning layer that decides what to do and can call tools (search, fetch, MCP, etc.) Services : The apps users interact with ( chatbot , data_manager , integrations, dashboards) Why both a vector store and tools? The vector store is best for relevance-based retrieval across the indexed knowledge base. Tools let the agent do targeted operations (metadata lookup, full-document fetch, external system calls) that pure embedding search cannot do reliably. Services are enabled at deployment via flags to archi create : archi create [...] --services chatbot Pipelines (agent classes) define runtime behavior. Agent specs define prompt + enabled tool subset. Models, embeddings, and retriever settings are configured in YAML. Data Sources Data sources define what gets ingested into Archi's knowledge base for retrieval. Archi supports several data ingestion methods: Web link lists (including SSO-protected pages) Git scraping for MkDocs-based repositories JIRA and Redmine ticketing systems Manual document upload via the Uploader service or direct file copy Local documents Sources are configured under data_manager.sources in your config file. Read more \u2192 Services Archi provides these deployable services: Service Description Default Port chatbot Web-based chat interface 7861 data_manager Data ingestion and vectorstore management 7871 piazza Piazza forum integration with Slack \u2014 redmine-mailer Redmine ticket responses via email \u2014 mattermost Mattermost channel integration \u2014 grafana Monitoring dashboard 3000 grader Automated grading service 7862 Read more \u2192 Agents & Tools Agents are defined by agent specs \u2014 Markdown files with YAML frontmatter specifying name, tools, and system prompt. The agent specs directory is configured via services.chat_app.agents_dir . Read more \u2192 Models & Providers Archi supports five LLM provider types: Provider Models OpenAI GPT-4o, GPT-4, etc. Anthropic Claude 4, Claude 3.5 Sonnet, etc. Google Gemini Gemini 2.0 Flash, Gemini 1.5 Pro, etc. OpenRouter Access to 100+ models via a unified API Local (Ollama/vLLM) Any open-source model Users can also provide their own API keys at runtime via Bring Your Own Key (BYOK) . Read more \u2192 Configuration Management Archi uses a three-tier configuration system: Static Configuration (deploy-time, immutable): deployment name, embedding model, available pipelines Dynamic Configuration (admin-controlled, runtime-modifiable): default model, temperature, retrieval parameters User Preferences (per-user overrides): preferred model, temperature, prompt selections Settings are resolved as: User Preference \u2192 Dynamic Config \u2192 Static Default. See the Configuration Reference for the full YAML schema and the API Reference for the configuration API. Secrets Secrets are stored in a .env file passed via --env-file . Required secrets depend on your deployment: Secret Required For PG_PASSWORD All deployments OPENAI_API_KEY OpenAI provider ANTHROPIC_API_KEY Anthropic provider GOOGLE_API_KEY Google Gemini provider OPENROUTER_API_KEY OpenRouter provider HUGGINGFACEHUB_API_TOKEN Private HuggingFace models GIT_USERNAME / GIT_TOKEN Git source JIRA_PAT JIRA source REDMINE_USER / REDMINE_PW Redmine source See Data Sources and Services for service-specific secrets. Benchmarking Archi has benchmarking functionality via the archi evaluate CLI command: SOURCES mode : Checks if retrieved documents contain the correct sources RAGAS mode : Uses the Ragas evaluator for answer relevancy, faithfulness, context precision, and context relevancy Read more \u2192 Admin Guide Becoming an Admin Set admin status in PostgreSQL: UPDATE users SET is_admin = true WHERE email = 'admin@example.com'; Admin Capabilities Set deployment-wide defaults via the dynamic configuration API Manage prompts (add, edit, reload via API) View the configuration audit log Grant admin privileges to other users Audit Logging All admin configuration changes are logged and queryable: GET /api/config/audit?limit=50 See the API Reference for full endpoint documentation.","title":"User Guide"},{"location":"user_guide/#user-guide","text":"This guide covers the core concepts and features of Archi. Each topic has its own dedicated page for detailed reference.","title":"User Guide"},{"location":"user_guide/#overview","text":"Archi is a retrieval-based assistant framework with four core parts: Data sources : Where knowledge comes from (links, git repos, JIRA/Redmine, uploaded files) Vector store + retrievers : Where ingested content is indexed and searched semantically/lexically Agents + tools : The reasoning layer that decides what to do and can call tools (search, fetch, MCP, etc.) Services : The apps users interact with ( chatbot , data_manager , integrations, dashboards) Why both a vector store and tools? The vector store is best for relevance-based retrieval across the indexed knowledge base. Tools let the agent do targeted operations (metadata lookup, full-document fetch, external system calls) that pure embedding search cannot do reliably. Services are enabled at deployment via flags to archi create : archi create [...] --services chatbot Pipelines (agent classes) define runtime behavior. Agent specs define prompt + enabled tool subset. Models, embeddings, and retriever settings are configured in YAML.","title":"Overview"},{"location":"user_guide/#data-sources","text":"Data sources define what gets ingested into Archi's knowledge base for retrieval. Archi supports several data ingestion methods: Web link lists (including SSO-protected pages) Git scraping for MkDocs-based repositories JIRA and Redmine ticketing systems Manual document upload via the Uploader service or direct file copy Local documents Sources are configured under data_manager.sources in your config file. Read more \u2192","title":"Data Sources"},{"location":"user_guide/#services","text":"Archi provides these deployable services: Service Description Default Port chatbot Web-based chat interface 7861 data_manager Data ingestion and vectorstore management 7871 piazza Piazza forum integration with Slack \u2014 redmine-mailer Redmine ticket responses via email \u2014 mattermost Mattermost channel integration \u2014 grafana Monitoring dashboard 3000 grader Automated grading service 7862 Read more \u2192","title":"Services"},{"location":"user_guide/#agents-tools","text":"Agents are defined by agent specs \u2014 Markdown files with YAML frontmatter specifying name, tools, and system prompt. The agent specs directory is configured via services.chat_app.agents_dir . Read more \u2192","title":"Agents &amp; Tools"},{"location":"user_guide/#models-providers","text":"Archi supports five LLM provider types: Provider Models OpenAI GPT-4o, GPT-4, etc. Anthropic Claude 4, Claude 3.5 Sonnet, etc. Google Gemini Gemini 2.0 Flash, Gemini 1.5 Pro, etc. OpenRouter Access to 100+ models via a unified API Local (Ollama/vLLM) Any open-source model Users can also provide their own API keys at runtime via Bring Your Own Key (BYOK) . Read more \u2192","title":"Models &amp; Providers"},{"location":"user_guide/#configuration-management","text":"Archi uses a three-tier configuration system: Static Configuration (deploy-time, immutable): deployment name, embedding model, available pipelines Dynamic Configuration (admin-controlled, runtime-modifiable): default model, temperature, retrieval parameters User Preferences (per-user overrides): preferred model, temperature, prompt selections Settings are resolved as: User Preference \u2192 Dynamic Config \u2192 Static Default. See the Configuration Reference for the full YAML schema and the API Reference for the configuration API.","title":"Configuration Management"},{"location":"user_guide/#secrets","text":"Secrets are stored in a .env file passed via --env-file . Required secrets depend on your deployment: Secret Required For PG_PASSWORD All deployments OPENAI_API_KEY OpenAI provider ANTHROPIC_API_KEY Anthropic provider GOOGLE_API_KEY Google Gemini provider OPENROUTER_API_KEY OpenRouter provider HUGGINGFACEHUB_API_TOKEN Private HuggingFace models GIT_USERNAME / GIT_TOKEN Git source JIRA_PAT JIRA source REDMINE_USER / REDMINE_PW Redmine source See Data Sources and Services for service-specific secrets.","title":"Secrets"},{"location":"user_guide/#benchmarking","text":"Archi has benchmarking functionality via the archi evaluate CLI command: SOURCES mode : Checks if retrieved documents contain the correct sources RAGAS mode : Uses the Ragas evaluator for answer relevancy, faithfulness, context precision, and context relevancy Read more \u2192","title":"Benchmarking"},{"location":"user_guide/#admin-guide","text":"","title":"Admin Guide"},{"location":"user_guide/#becoming-an-admin","text":"Set admin status in PostgreSQL: UPDATE users SET is_admin = true WHERE email = 'admin@example.com';","title":"Becoming an Admin"},{"location":"user_guide/#admin-capabilities","text":"Set deployment-wide defaults via the dynamic configuration API Manage prompts (add, edit, reload via API) View the configuration audit log Grant admin privileges to other users","title":"Admin Capabilities"},{"location":"user_guide/#audit-logging","text":"All admin configuration changes are logged and queryable: GET /api/config/audit?limit=50 See the API Reference for full endpoint documentation.","title":"Audit Logging"}]}